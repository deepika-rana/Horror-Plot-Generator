{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Horror Plot Generator - Halloween.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepika-rana/Horror-Plot-Generator/blob/main/Horror_Plot_Generator_Halloween.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VF3MczVP94x"
      },
      "source": [
        "# Horror Plot Generator\n",
        "\n",
        "A character-level LSTM is contructed with PyTorch to generate a horror plot on the ocassion of Halloween. The network will train character by character on some text, then generate new text character by character. \n",
        "\n",
        "The input used to train this model was from a public dataset on kaggle that has more than 15k movie plots.\n",
        "**This model will be able to generate new text based on the text from the lyrics!**\n",
        "\n",
        "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FHmKOjfP941"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV9iBXXhP946"
      },
      "source": [
        "## Load in Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DZfM0ejxaJ0",
        "outputId": "cfcec807-60ea-4021-c535-fa86ec25d845",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlUBlk4exqcl"
      },
      "source": [
        "path = \"/content/drive/My Drive/mpst_full_data.csv\"\n",
        "data = pd.read_csv(path)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UV16UIF5pQy",
        "outputId": "bcb666b8-01c1-4a3f-a59f-92bcb00cb9d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        }
      },
      "source": [
        "genre_based = data[data['tags'].str.contains('horror','dark') == True].copy()\n",
        "plot_based = data[data['plot_synopsis'].str.contains('violence','murder') == True].copy()\n",
        "suspense_based = data[data['tags'].str.contains('suspenseful','story telling') == True].copy()\n",
        "story_based = data[data['tags'].str.contains('mystery','cruelty') == True].copy()\n",
        "fear_based = data[data['tags'].str.contains('paranormal','haunting') == True].copy()\n",
        "data = pd.concat([genre_based, plot_based,suspense_based,story_based,fear_based]).drop_duplicates().reset_index(drop=True)\n",
        "data = plot_based.dropna(subset=['tags'])\n",
        "data[:20]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>imdb_id</th>\n",
              "      <th>title</th>\n",
              "      <th>plot_synopsis</th>\n",
              "      <th>tags</th>\n",
              "      <th>split</th>\n",
              "      <th>synopsis_source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tt0057603</td>\n",
              "      <td>I tre volti della paura</td>\n",
              "      <td>Note: this synopsis is for the orginal Italian...</td>\n",
              "      <td>cult, horror, gothic, murder, atmospheric</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tt0086250</td>\n",
              "      <td>Scarface</td>\n",
              "      <td>In May 1980, a Cuban man named Tony Montana (A...</td>\n",
              "      <td>cruelty, murder, dramatic, cult, violence, atm...</td>\n",
              "      <td>val</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>tt1232776</td>\n",
              "      <td>Fish Tank</td>\n",
              "      <td>We open with Mia (Katie Jarvis), a 15 year old...</td>\n",
              "      <td>suspenseful, depressing, realism</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>tt2937898</td>\n",
              "      <td>A Most Violent Year</td>\n",
              "      <td>In 1981 New York, Abel Morales (Oscar Isaac) i...</td>\n",
              "      <td>violence</td>\n",
              "      <td>train</td>\n",
              "      <td>wikipedia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>tt2216240</td>\n",
              "      <td>Kapringen</td>\n",
              "      <td>The film is about ancient business model, alre...</td>\n",
              "      <td>dramatic, suspenseful, mystery</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>tt0310793</td>\n",
              "      <td>Bowling for Columbine</td>\n",
              "      <td>In Michael Moore's discussions with various fi...</td>\n",
              "      <td>brainwashing, violence, satire, murder, stupid</td>\n",
              "      <td>test</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>tt0088757</td>\n",
              "      <td>Avenging Angel</td>\n",
              "      <td>Set a few years after the first 'Angel' film, ...</td>\n",
              "      <td>cult</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>tt0486946</td>\n",
              "      <td>Wild Hogs</td>\n",
              "      <td>Doug Madsen (Tim Allen), Woody Stevens (John T...</td>\n",
              "      <td>humor</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>tt0429493</td>\n",
              "      <td>The A-Team</td>\n",
              "      <td>The A-Team is a naturally episodic show, with ...</td>\n",
              "      <td>comedy, murder, violence, flashback, clever, p...</td>\n",
              "      <td>train</td>\n",
              "      <td>wikipedia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>tt0074084</td>\n",
              "      <td>Novecento</td>\n",
              "      <td>Born the same day at the turn of the 20th cent...</td>\n",
              "      <td>cruelty, murder, violence, flashback, insanity...</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>tt4258698</td>\n",
              "      <td>Southside with You</td>\n",
              "      <td>It is the summer of 1989 in Chicago, Illinois....</td>\n",
              "      <td>romantic, historical</td>\n",
              "      <td>test</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>tt0107808</td>\n",
              "      <td>A Perfect World</td>\n",
              "      <td>\"A Perfect World,\" ostensibly about the escape...</td>\n",
              "      <td>cult, violence</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213</th>\n",
              "      <td>tt0094625</td>\n",
              "      <td>Akira</td>\n",
              "      <td>On July 16, 1988 an atom bomb vaporizes Tokyo....</td>\n",
              "      <td>murder, stupid, paranormal, anti war, cult, vi...</td>\n",
              "      <td>val</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217</th>\n",
              "      <td>tt0104797</td>\n",
              "      <td>Malcolm X</td>\n",
              "      <td>As the opening credits roll, we hear Malcolm X...</td>\n",
              "      <td>murder, cult, violence, flashback, historical,...</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>218</th>\n",
              "      <td>tt1289406</td>\n",
              "      <td>Harry Brown</td>\n",
              "      <td>Shot with a cellphone camera, the film opens w...</td>\n",
              "      <td>neo noir, murder, thought-provoking, cult, rev...</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>219</th>\n",
              "      <td>tt1536044</td>\n",
              "      <td>Paranormal Activity 2</td>\n",
              "      <td>The film opens with home video footage of Kris...</td>\n",
              "      <td>cult, boring, murder</td>\n",
              "      <td>val</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220</th>\n",
              "      <td>tt0090329</td>\n",
              "      <td>Witness</td>\n",
              "      <td>Rachel Lapp (Kelly McGillis), a young Amish wi...</td>\n",
              "      <td>mystery, neo noir, murder, realism, dramatic, ...</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>tt0039152</td>\n",
              "      <td>Angel and the Badman</td>\n",
              "      <td>Wounded and on the run, notorious gunman Quirt...</td>\n",
              "      <td>revenge, comedy, violence</td>\n",
              "      <td>train</td>\n",
              "      <td>wikipedia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>304</th>\n",
              "      <td>tt0114863</td>\n",
              "      <td>To vlemma tou Odyssea</td>\n",
              "      <td>In \"Ulysses' Gaze\" (\"To Vlemma Tou Odyssea,\" 1...</td>\n",
              "      <td>murder</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>tt0119488</td>\n",
              "      <td>L.A. Confidential</td>\n",
              "      <td>An opening montage, narrated by Sid Hudgens (D...</td>\n",
              "      <td>dark, suspenseful, neo noir, murder, boring, m...</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       imdb_id                    title  ...  split synopsis_source\n",
              "0    tt0057603  I tre volti della paura  ...  train            imdb\n",
              "4    tt0086250                 Scarface  ...    val            imdb\n",
              "16   tt1232776                Fish Tank  ...  train            imdb\n",
              "57   tt2937898      A Most Violent Year  ...  train       wikipedia\n",
              "73   tt2216240                Kapringen  ...  train            imdb\n",
              "86   tt0310793    Bowling for Columbine  ...   test            imdb\n",
              "104  tt0088757           Avenging Angel  ...  train            imdb\n",
              "136  tt0486946                Wild Hogs  ...  train            imdb\n",
              "168  tt0429493               The A-Team  ...  train       wikipedia\n",
              "172  tt0074084                Novecento  ...  train            imdb\n",
              "188  tt4258698       Southside with You  ...   test            imdb\n",
              "191  tt0107808          A Perfect World  ...  train            imdb\n",
              "213  tt0094625                    Akira  ...    val            imdb\n",
              "217  tt0104797                Malcolm X  ...  train            imdb\n",
              "218  tt1289406              Harry Brown  ...  train            imdb\n",
              "219  tt1536044    Paranormal Activity 2  ...    val            imdb\n",
              "220  tt0090329                  Witness  ...  train            imdb\n",
              "269  tt0039152     Angel and the Badman  ...  train       wikipedia\n",
              "304  tt0114863    To vlemma tou Odyssea  ...  train            imdb\n",
              "314  tt0119488        L.A. Confidential  ...  train            imdb\n",
              "\n",
              "[20 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Zc9DCwK59vy",
        "outputId": "d1ce920e-d269-4109-c729-6c2ca671fcd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "plot = ''.join(data.plot_synopsis)\n",
        "plot = plot.lower()\n",
        "plot_based = ''.join([c for c in plot if c not in \"#$%&()*+,-./:;<=>@[\\\\]^_`{|}~\"])\n",
        "plot[:300]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"note: this synopsis is for the orginal italian release with the segments in this certain order.boris karloff introduces three horror tales of the macabre and the supernatural known as the 'three faces of fear'.the telephonerosy (michele mercier) is an attractive, high-priced parisian call-girl who r\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHsd3Mk_P95C"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "In the cells, below, I'm creating a couple **dictionaries** to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ciGq1_QP95D"
      },
      "source": [
        "# Creating two dictionaries:\n",
        "# 1. int2char, which maps integers to characters\n",
        "# 2. char2int, which maps characters to unique integers\n",
        "chars = tuple(set(plot))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encoding the text\n",
        "encoded = np.array([char2int[ch] for ch in plot])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHwpVTRwP95G"
      },
      "source": [
        "And those same characters from above, are encoded as integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5jCICfLP95J",
        "outputId": "12244db6-0fe9-42a2-be9e-e3ff6f740382",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "encoded[:100]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 74, 113,  85,   1,  91,  34,  85,  23,  95,  40,  34,  40,  72,\n",
              "        74, 113, 108,  40,  95,  40,  34,  95,  40,  34,  43, 113,  17,\n",
              "        34,  85,  23,   1,  34, 113,  17,  78,  95,  74,  46,  38,  34,\n",
              "        95,  85,  46,  38,  95,  46,  74,  34,  17,   1,  38,   1,  46,\n",
              "        40,   1,  34, 102,  95,  85,  23,  34,  85,  23,   1,  34,  40,\n",
              "         1,  78,  80,   1,  74,  85,  40,  34,  95,  74,  34,  85,  23,\n",
              "        95,  40,  34,  41,   1,  17,  85,  46,  95,  74,  34, 113,  17,\n",
              "        81,   1,  17,  47,  54, 113,  17,  95,  40])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0h75OgaP95M"
      },
      "source": [
        "## Pre-processing the data\n",
        "\n",
        "The LSTM expects an input that is **one-hot encoded** meaning that each character is converted into an integer (via our created dictionary) and *then* converted into a column vector where only it's corresponding integer index will have the value of 1 and the rest of the vector will be filled with 0's. One-hot encoding the data using a function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icEEMrosP95N"
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOBVj5ZCP95P",
        "outputId": "80838c96-e638-4dbd-bca9-b1d7e491a7d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Check that the function works as expected\n",
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_MRM8INP95T"
      },
      "source": [
        "## Making training mini-batches\n",
        "\n",
        "\n",
        "To train on this data, I created mini-batches for training. \n",
        "\n",
        "In this example, taking the encoded characters (passed in as the `arr` parameter) and split them into multiple sequences, given by `batch_size`. Each of the sequences will be `seq_length` long.\n",
        "\n",
        "### Creating Batches\n",
        "\n",
        "**1. The first thing is to discard some of the text so to obtain completely full mini-batches.**\n",
        "\n",
        "**2. After that, split `arr` into $N$ batches.** \n",
        "\n",
        "**3. Now that we have this array, we can iterate through it to get our mini-batches.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss46q46iP95U"
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''A generator that returns batches of size\n",
        "       batch_size x seq_length from arr.\n",
        "       \n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence\n",
        "    '''\n",
        "    \n",
        "    batch_size_total = batch_size * seq_length\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q48pKMhbP95X"
      },
      "source": [
        "### Testing the Implementation\n",
        "\n",
        "Now I'll make some data sets and we can check out what's going on as we batch data. Here, as an example, I'm going to use a batch size of 8 and 50 sequence steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0MjPHLfP95Z"
      },
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_41dpMq7P95b",
        "outputId": "5ade8fca-42f1-457b-8973-531d9947d6f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "# printing out the first 10 items in a sequence\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[ 74 113  85   1  91  34  85  23  95  40]\n",
            " [ 40   1  34  43  95  74  81  40  34  41]\n",
            " [  1   1  21  34  46  40  40 110  80   1]\n",
            " [ 34  46  43  43  46  95  17  34  46 108]\n",
            " [ 46  80   1  47  85   1  81  81  72  34]\n",
            " [ 95 108 108  38   1  23 113  17  74  75]\n",
            " [ 74  41  38 110  81  95  74  78  34  17]\n",
            " [ 95  41  34  85  17  95  54   1  75  34]]\n",
            "\n",
            "y\n",
            " [[113  85   1  91  34  85  23  95  40  34]\n",
            " [  1  34  43  95  74  81  40  34  41  23]\n",
            " [  1  21  34  46  40  40 110  80   1  40]\n",
            " [ 46  43  43  46  95  17  34  46 108 108]\n",
            " [ 80   1  47  85   1  81  81  72  34  78]\n",
            " [108 108  38   1  23 113  17  74  75  34]\n",
            " [ 41  38 110  81  95  74  78  34  17  46]\n",
            " [ 41  34  85  17  95  54   1  75  34  54]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIukVIXjP95e"
      },
      "source": [
        "---\n",
        "## Defining the network with PyTorch\n",
        "\n",
        "Next, I'll use PyTorch to define the architecture of the network. Started by defining the layers and operations. Then, defined a method for the forward pass. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP5k8CfjP95i",
        "outputId": "b430c648-c784-452f-c788-e65d3b4daaf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else: \n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogFmtSx_P95m"
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2,  \n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        ## define the layers of the model\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "                \n",
        "        ## Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        # pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        # Stack up LSTM outputs using view\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden\n",
        "        "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNMiIgz8P95q"
      },
      "source": [
        "## Time to train\n",
        "\n",
        "The train function gives the ability to set the number of epochs, the learning rate, and other parameters.\n",
        "\n",
        "Below using an Adam optimizer and cross entropy loss since as looking at character class scores as output. Calculate the loss and perform backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLanJBSsU_2C"
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXiZCqs3P95t"
      },
      "source": [
        "## Instantiating the model\n",
        "\n",
        "Actually training the network. First creating the network itself, with some given hyperparameters. Then, defined the mini-batches sizes, and started training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRkBkzcxP95u",
        "outputId": "01fbd023-6ba8-43c0-e7d8-5dbfc9bea38e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "#Set model hyperparameters\n",
        "# define and print the net\n",
        "n_hidden=512\n",
        "n_layers=2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(116, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=116, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzLRrSIHP95w"
      },
      "source": [
        "### Set training hyperparameters!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6Egd98q5P95x",
        "outputId": "3e617120-fd61-4b4d-ab70-583a26a22fc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "batch_size = 128\n",
        "seq_length = 10\n",
        "n_epochs = 20 \n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=2000)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/20... Step: 2000... Loss: 1.2554... Val Loss: 1.3060\n",
            "Epoch: 2/20... Step: 4000... Loss: 1.2178... Val Loss: 1.3046\n",
            "Epoch: 3/20... Step: 6000... Loss: 1.1500... Val Loss: 1.3023\n",
            "Epoch: 4/20... Step: 8000... Loss: 1.2750... Val Loss: 1.3022\n",
            "Epoch: 5/20... Step: 10000... Loss: 1.2173... Val Loss: 1.3050\n",
            "Epoch: 5/20... Step: 12000... Loss: 1.2239... Val Loss: 1.2987\n",
            "Epoch: 6/20... Step: 14000... Loss: 1.2387... Val Loss: 1.3047\n",
            "Epoch: 7/20... Step: 16000... Loss: 1.2706... Val Loss: 1.3039\n",
            "Epoch: 8/20... Step: 18000... Loss: 1.1886... Val Loss: 1.2998\n",
            "Epoch: 9/20... Step: 20000... Loss: 1.1817... Val Loss: 1.3030\n",
            "Epoch: 10/20... Step: 22000... Loss: 1.2225... Val Loss: 1.3014\n",
            "Epoch: 10/20... Step: 24000... Loss: 1.2396... Val Loss: 1.2997\n",
            "Epoch: 11/20... Step: 26000... Loss: 1.2855... Val Loss: 1.3045\n",
            "Epoch: 12/20... Step: 28000... Loss: 1.1731... Val Loss: 1.2992\n",
            "Epoch: 13/20... Step: 30000... Loss: 1.2165... Val Loss: 1.2960\n",
            "Epoch: 14/20... Step: 32000... Loss: 1.2301... Val Loss: 1.2977\n",
            "Epoch: 15/20... Step: 34000... Loss: 1.2078... Val Loss: 1.3002\n",
            "Epoch: 15/20... Step: 36000... Loss: 1.1961... Val Loss: 1.2983\n",
            "Epoch: 16/20... Step: 38000... Loss: 1.2054... Val Loss: 1.3034\n",
            "Epoch: 17/20... Step: 40000... Loss: 1.2540... Val Loss: 1.2982\n",
            "Epoch: 18/20... Step: 42000... Loss: 1.2113... Val Loss: 1.3000\n",
            "Epoch: 19/20... Step: 44000... Loss: 1.2233... Val Loss: 1.3016\n",
            "Epoch: 20/20... Step: 46000... Loss: 1.2008... Val Loss: 1.2960\n",
            "Epoch: 20/20... Step: 48000... Loss: 1.2426... Val Loss: 1.2971\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3aH8UF3P951"
      },
      "source": [
        "## Checkpoint\n",
        "\n",
        "After training, saved the model to load it again later if we need too. Here I'm saving the parameters needed to create the same architecture, the hidden layer hyperparameters and the text characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "884oUk09P952"
      },
      "source": [
        "# change the name, for saving multiple files\n",
        "model_name = 'rnn_x_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9o-o_8g5P956"
      },
      "source": [
        "---\n",
        "## Making Predictions\n",
        "\n",
        "Now that the model is trained, I'll want to sample from it and make predictions about next characters! To sample, pass in a character and have the network predict the next character. Then take that character, pass it back in, and get another predicted character. Just keep doing this and generate a bunch of text!\n",
        "\n",
        "\n",
        "The output of RNN is from a fully-connected layer and it outputs a **distribution of next-character scores**.\n",
        "\n",
        "> To actually get the next character, applied a softmax function, which gives us a *probability* distribution that can then sample to predict the next character.\n",
        "\n",
        "### Top K sampling\n",
        "\n",
        "The predictions come from a categorical probability distribution over all the possible characters. The sample text can be made more reasonable to handle (with less variables) by only considering some $K$ most probable characters. This will prevent the network from giving completely absurd characters while allowing it to introduce some noise and randomness into the sampled text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVfWFYgpP957"
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgrW2ZbBP95-"
      },
      "source": [
        "### Priming and generating the plot\n",
        "\n",
        "Typically I'll want to prime the network to build up a hidden state. Otherwise the network will start out generating characters at random which will not generate a decent horror plot. In general the first bunch of characters will be a little rough since it hasn't built up a long history of characters to predict from."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0YOTdKgP95_"
      },
      "source": [
        "def PlotGenerate(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw8DDZaZl32M"
      },
      "source": [
        "##Plot Generation by the Neural Network\n",
        "\n",
        "Outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIeoYiCKP96E",
        "outputId": "01189c9e-8c45-4be0-c53c-22bc22fc0130",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(PlotGenerate(net, 1000, prime='a girl hanging from a tree upside down', top_k=5))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a girl hanging from a tree upside down on an ambush when the precinct in a masked body taken out with the townspeople when carter confirms that the contend of the cops are trying to see those who has arrested while something is a picture when the man tries to shoot the characters and she has been shooting her and she and jane attends the military business and has a steal that truman's story, alan throws him out on the shudgers of highway, and the three became oblivious princess interraped by an indian also, but then comforts moore as well. he also surprises the police over the battle and sees a second thing to stay with the branch actions to a painting. all of the moroi. she says she will be told and he's at first so the criminal contestants are attacked by the shop. though the train is supposed to destroy having been shot to a boy being a boys. while a sound of her people warning to the counters of a man the news trapped as well as a cop, sarah, a brothel talking about her and contrasts of the most, but the two begin her b\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p02SLWsDqrK",
        "outputId": "9460d1da-c367-4c22-8bc9-2f4f94aa2805",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(PlotGenerate(net, 500, prime='crippled person possessed by haunted spirits', top_k=10))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "crippled person possessed by haunted spirits by the pink passing militant day outlaw). however, they sign himself in her by the handkerchief, dressed again. after reterrablizing the grievers that can spiral. in his supervisor actions with because of them. miss giddens drafts at elizabeth, but the police are lights into the correct man as he calls, surrounded by a tango aspirin and promised to tamalia that a social terminal suicide is a decision to save the friends, he is in that's computer, who is the sound. and he regrets friend of his ma\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8IUF2oirFSg",
        "outputId": "8ea419a7-11de-4807-b1bf-9ccce31e3f9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(PlotGenerate(net, 1200, prime='cawing sounds echo the mountains', top_k=5))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cawing sounds echo the mountains on the crime with his friend's first person. a setting service, and threatens to see the thought of his arm tony, and that she'll have to see that they are all of the passenger or handing when he tells her to get on a car and choke out with a concert about his survivors, sanjuro is suscecially angered in the shadows into a final but children. however, they are coming around to her. he arrives there and convince her help, she comes back to his friend watch the novel in the case against the conversation with them to the pool attending to throw out an allie story about that means that he doesn't seem the butcher. she is called alone with a poor children introduced tim, the police tell her him and that he sees them. he then then all of his strogoi with the baby, and her problems. he is a chamber over the tang. human still wearing a bottle of the body. as she tries to calm the bomb with an entire characters. the three cross is a caped and an old men are a man, taking a gun to seater who is strengthened a first town to the tree with her trees and shoots, he and jane watches them as well. the marriage were the one that hardin's head thrown out of the same continuous side, are changed in t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IbHHsCmF_bD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}