{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Horror Plot Generator - Halloween.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepika-rana/Horror-Plot-Generator/blob/main/Horror_Plot_Generator_Halloween.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VF3MczVP94x"
      },
      "source": [
        "# Horror Plot Generator\n",
        "\n",
        "A character-level LSTM is contructed with PyTorch to generate a horror plot on the ocassion of Halloween. The network will train character by character on some text, then generate new text character by character. \n",
        "\n",
        "The input used to train this model was from a public dataset on kaggle that has more than 15k movie plots.\n",
        "**This model will be able to generate new text based on the text from the lyrics!**\n",
        "\n",
        "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FHmKOjfP941"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV9iBXXhP946"
      },
      "source": [
        "## Load in Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DZfM0ejxaJ0",
        "outputId": "cfcec807-60ea-4021-c535-fa86ec25d845",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlUBlk4exqcl"
      },
      "source": [
        "path = \"/content/drive/My Drive/mpst_full_data.csv\"\n",
        "data = pd.read_csv(path)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UV16UIF5pQy",
        "outputId": "bcb666b8-01c1-4a3f-a59f-92bcb00cb9d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        }
      },
      "source": [
        "genre_based = data[data['tags'].str.contains('horror','dark') == True].copy()\n",
        "plot_based = data[data['plot_synopsis'].str.contains('violence','murder') == True].copy()\n",
        "suspense_based = data[data['tags'].str.contains('suspenseful','story telling') == True].copy()\n",
        "story_based = data[data['tags'].str.contains('mystery','cruelty') == True].copy()\n",
        "fear_based = data[data['tags'].str.contains('paranormal','haunting') == True].copy()\n",
        "data = pd.concat([genre_based, plot_based,suspense_based,story_based,fear_based]).drop_duplicates().reset_index(drop=True)\n",
        "data = plot_based.dropna(subset=['tags'])\n",
        "data[:20]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>imdb_id</th>\n",
              "      <th>title</th>\n",
              "      <th>plot_synopsis</th>\n",
              "      <th>tags</th>\n",
              "      <th>split</th>\n",
              "      <th>synopsis_source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tt0057603</td>\n",
              "      <td>I tre volti della paura</td>\n",
              "      <td>Note: this synopsis is for the orginal Italian...</td>\n",
              "      <td>cult, horror, gothic, murder, atmospheric</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tt0086250</td>\n",
              "      <td>Scarface</td>\n",
              "      <td>In May 1980, a Cuban man named Tony Montana (A...</td>\n",
              "      <td>cruelty, murder, dramatic, cult, violence, atm...</td>\n",
              "      <td>val</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>tt1232776</td>\n",
              "      <td>Fish Tank</td>\n",
              "      <td>We open with Mia (Katie Jarvis), a 15 year old...</td>\n",
              "      <td>suspenseful, depressing, realism</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>tt2937898</td>\n",
              "      <td>A Most Violent Year</td>\n",
              "      <td>In 1981 New York, Abel Morales (Oscar Isaac) i...</td>\n",
              "      <td>violence</td>\n",
              "      <td>train</td>\n",
              "      <td>wikipedia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>tt2216240</td>\n",
              "      <td>Kapringen</td>\n",
              "      <td>The film is about ancient business model, alre...</td>\n",
              "      <td>dramatic, suspenseful, mystery</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>tt0310793</td>\n",
              "      <td>Bowling for Columbine</td>\n",
              "      <td>In Michael Moore's discussions with various fi...</td>\n",
              "      <td>brainwashing, violence, satire, murder, stupid</td>\n",
              "      <td>test</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>tt0088757</td>\n",
              "      <td>Avenging Angel</td>\n",
              "      <td>Set a few years after the first 'Angel' film, ...</td>\n",
              "      <td>cult</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>tt0486946</td>\n",
              "      <td>Wild Hogs</td>\n",
              "      <td>Doug Madsen (Tim Allen), Woody Stevens (John T...</td>\n",
              "      <td>humor</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>tt0429493</td>\n",
              "      <td>The A-Team</td>\n",
              "      <td>The A-Team is a naturally episodic show, with ...</td>\n",
              "      <td>comedy, murder, violence, flashback, clever, p...</td>\n",
              "      <td>train</td>\n",
              "      <td>wikipedia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>tt0074084</td>\n",
              "      <td>Novecento</td>\n",
              "      <td>Born the same day at the turn of the 20th cent...</td>\n",
              "      <td>cruelty, murder, violence, flashback, insanity...</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>tt4258698</td>\n",
              "      <td>Southside with You</td>\n",
              "      <td>It is the summer of 1989 in Chicago, Illinois....</td>\n",
              "      <td>romantic, historical</td>\n",
              "      <td>test</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>tt0107808</td>\n",
              "      <td>A Perfect World</td>\n",
              "      <td>\"A Perfect World,\" ostensibly about the escape...</td>\n",
              "      <td>cult, violence</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213</th>\n",
              "      <td>tt0094625</td>\n",
              "      <td>Akira</td>\n",
              "      <td>On July 16, 1988 an atom bomb vaporizes Tokyo....</td>\n",
              "      <td>murder, stupid, paranormal, anti war, cult, vi...</td>\n",
              "      <td>val</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217</th>\n",
              "      <td>tt0104797</td>\n",
              "      <td>Malcolm X</td>\n",
              "      <td>As the opening credits roll, we hear Malcolm X...</td>\n",
              "      <td>murder, cult, violence, flashback, historical,...</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>218</th>\n",
              "      <td>tt1289406</td>\n",
              "      <td>Harry Brown</td>\n",
              "      <td>Shot with a cellphone camera, the film opens w...</td>\n",
              "      <td>neo noir, murder, thought-provoking, cult, rev...</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>219</th>\n",
              "      <td>tt1536044</td>\n",
              "      <td>Paranormal Activity 2</td>\n",
              "      <td>The film opens with home video footage of Kris...</td>\n",
              "      <td>cult, boring, murder</td>\n",
              "      <td>val</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220</th>\n",
              "      <td>tt0090329</td>\n",
              "      <td>Witness</td>\n",
              "      <td>Rachel Lapp (Kelly McGillis), a young Amish wi...</td>\n",
              "      <td>mystery, neo noir, murder, realism, dramatic, ...</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>tt0039152</td>\n",
              "      <td>Angel and the Badman</td>\n",
              "      <td>Wounded and on the run, notorious gunman Quirt...</td>\n",
              "      <td>revenge, comedy, violence</td>\n",
              "      <td>train</td>\n",
              "      <td>wikipedia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>304</th>\n",
              "      <td>tt0114863</td>\n",
              "      <td>To vlemma tou Odyssea</td>\n",
              "      <td>In \"Ulysses' Gaze\" (\"To Vlemma Tou Odyssea,\" 1...</td>\n",
              "      <td>murder</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>tt0119488</td>\n",
              "      <td>L.A. Confidential</td>\n",
              "      <td>An opening montage, narrated by Sid Hudgens (D...</td>\n",
              "      <td>dark, suspenseful, neo noir, murder, boring, m...</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       imdb_id                    title  ...  split synopsis_source\n",
              "0    tt0057603  I tre volti della paura  ...  train            imdb\n",
              "4    tt0086250                 Scarface  ...    val            imdb\n",
              "16   tt1232776                Fish Tank  ...  train            imdb\n",
              "57   tt2937898      A Most Violent Year  ...  train       wikipedia\n",
              "73   tt2216240                Kapringen  ...  train            imdb\n",
              "86   tt0310793    Bowling for Columbine  ...   test            imdb\n",
              "104  tt0088757           Avenging Angel  ...  train            imdb\n",
              "136  tt0486946                Wild Hogs  ...  train            imdb\n",
              "168  tt0429493               The A-Team  ...  train       wikipedia\n",
              "172  tt0074084                Novecento  ...  train            imdb\n",
              "188  tt4258698       Southside with You  ...   test            imdb\n",
              "191  tt0107808          A Perfect World  ...  train            imdb\n",
              "213  tt0094625                    Akira  ...    val            imdb\n",
              "217  tt0104797                Malcolm X  ...  train            imdb\n",
              "218  tt1289406              Harry Brown  ...  train            imdb\n",
              "219  tt1536044    Paranormal Activity 2  ...    val            imdb\n",
              "220  tt0090329                  Witness  ...  train            imdb\n",
              "269  tt0039152     Angel and the Badman  ...  train       wikipedia\n",
              "304  tt0114863    To vlemma tou Odyssea  ...  train            imdb\n",
              "314  tt0119488        L.A. Confidential  ...  train            imdb\n",
              "\n",
              "[20 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Zc9DCwK59vy",
        "outputId": "d1ce920e-d269-4109-c729-6c2ca671fcd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "plot = ''.join(data.plot_synopsis)\n",
        "plot = plot.lower()\n",
        "plot_based = ''.join([c for c in plot if c not in \"#$%&()*+,-./:;<=>@[\\\\]^_`{|}~\"])\n",
        "plot[:300]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"note: this synopsis is for the orginal italian release with the segments in this certain order.boris karloff introduces three horror tales of the macabre and the supernatural known as the 'three faces of fear'.the telephonerosy (michele mercier) is an attractive, high-priced parisian call-girl who r\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHsd3Mk_P95C"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "In the cells, below, I'm creating a couple **dictionaries** to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ciGq1_QP95D"
      },
      "source": [
        "# Creating two dictionaries:\n",
        "# 1. int2char, which maps integers to characters\n",
        "# 2. char2int, which maps characters to unique integers\n",
        "chars = tuple(set(plot))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encoding the text\n",
        "encoded = np.array([char2int[ch] for ch in plot])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHwpVTRwP95G"
      },
      "source": [
        "And those same characters from above, are encoded as integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5jCICfLP95J",
        "outputId": "12244db6-0fe9-42a2-be9e-e3ff6f740382",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "encoded[:100]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 74, 113,  85,   1,  91,  34,  85,  23,  95,  40,  34,  40,  72,\n",
              "        74, 113, 108,  40,  95,  40,  34,  95,  40,  34,  43, 113,  17,\n",
              "        34,  85,  23,   1,  34, 113,  17,  78,  95,  74,  46,  38,  34,\n",
              "        95,  85,  46,  38,  95,  46,  74,  34,  17,   1,  38,   1,  46,\n",
              "        40,   1,  34, 102,  95,  85,  23,  34,  85,  23,   1,  34,  40,\n",
              "         1,  78,  80,   1,  74,  85,  40,  34,  95,  74,  34,  85,  23,\n",
              "        95,  40,  34,  41,   1,  17,  85,  46,  95,  74,  34, 113,  17,\n",
              "        81,   1,  17,  47,  54, 113,  17,  95,  40])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0h75OgaP95M"
      },
      "source": [
        "## Pre-processing the data\n",
        "\n",
        "The LSTM expects an input that is **one-hot encoded** meaning that each character is converted into an integer (via our created dictionary) and *then* converted into a column vector where only it's corresponding integer index will have the value of 1 and the rest of the vector will be filled with 0's. One-hot encoding the data using a function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icEEMrosP95N"
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOBVj5ZCP95P",
        "outputId": "80838c96-e638-4dbd-bca9-b1d7e491a7d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Check that the function works as expected\n",
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_MRM8INP95T"
      },
      "source": [
        "## Making training mini-batches\n",
        "\n",
        "\n",
        "To train on this data, I created mini-batches for training. \n",
        "\n",
        "In this example, taking the encoded characters (passed in as the `arr` parameter) and split them into multiple sequences, given by `batch_size`. Each of the sequences will be `seq_length` long.\n",
        "\n",
        "### Creating Batches\n",
        "\n",
        "**1. The first thing is to discard some of the text so to obtain completely full mini-batches.**\n",
        "\n",
        "**2. After that, split `arr` into $N$ batches.** \n",
        "\n",
        "**3. Now that we have this array, we can iterate through it to get our mini-batches.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss46q46iP95U"
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''A generator that returns batches of size\n",
        "       batch_size x seq_length from arr.\n",
        "       \n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence\n",
        "    '''\n",
        "    \n",
        "    batch_size_total = batch_size * seq_length\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q48pKMhbP95X"
      },
      "source": [
        "### Testing the Implementation\n",
        "\n",
        "Now I'll make some data sets and we can check out what's going on as we batch data. Here, as an example, I'm going to use a batch size of 8 and 50 sequence steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0MjPHLfP95Z"
      },
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_41dpMq7P95b",
        "outputId": "5ade8fca-42f1-457b-8973-531d9947d6f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "# printing out the first 10 items in a sequence\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[ 74 113  85   1  91  34  85  23  95  40]\n",
            " [ 40   1  34  43  95  74  81  40  34  41]\n",
            " [  1   1  21  34  46  40  40 110  80   1]\n",
            " [ 34  46  43  43  46  95  17  34  46 108]\n",
            " [ 46  80   1  47  85   1  81  81  72  34]\n",
            " [ 95 108 108  38   1  23 113  17  74  75]\n",
            " [ 74  41  38 110  81  95  74  78  34  17]\n",
            " [ 95  41  34  85  17  95  54   1  75  34]]\n",
            "\n",
            "y\n",
            " [[113  85   1  91  34  85  23  95  40  34]\n",
            " [  1  34  43  95  74  81  40  34  41  23]\n",
            " [  1  21  34  46  40  40 110  80   1  40]\n",
            " [ 46  43  43  46  95  17  34  46 108 108]\n",
            " [ 80   1  47  85   1  81  81  72  34  78]\n",
            " [108 108  38   1  23 113  17  74  75  34]\n",
            " [ 41  38 110  81  95  74  78  34  17  46]\n",
            " [ 41  34  85  17  95  54   1  75  34  54]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIukVIXjP95e"
      },
      "source": [
        "---\n",
        "## Defining the network with PyTorch\n",
        "\n",
        "Next, I'll use PyTorch to define the architecture of the network. Started by defining the layers and operations. Then, defined a method for the forward pass. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP5k8CfjP95i",
        "outputId": "b430c648-c784-452f-c788-e65d3b4daaf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else: \n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogFmtSx_P95m"
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2,  \n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        ## define the layers of the model\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "                \n",
        "        ## Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        # pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        # Stack up LSTM outputs using view\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden\n",
        "        "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNMiIgz8P95q"
      },
      "source": [
        "## Time to train\n",
        "\n",
        "The train function gives the ability to set the number of epochs, the learning rate, and other parameters.\n",
        "\n",
        "Below using an Adam optimizer and cross entropy loss since as looking at character class scores as output. Calculate the loss and perform backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLanJBSsU_2C"
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXiZCqs3P95t"
      },
      "source": [
        "## Instantiating the model\n",
        "\n",
        "Actually training the network. First creating the network itself, with some given hyperparameters. Then, defined the mini-batches sizes, and started training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRkBkzcxP95u",
        "outputId": "01fbd023-6ba8-43c0-e7d8-5dbfc9bea38e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "#Set model hyperparameters\n",
        "# define and print the net\n",
        "n_hidden=512\n",
        "n_layers=2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(116, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=116, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzLRrSIHP95w"
      },
      "source": [
        "### Set training hyperparameters!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6Egd98q5P95x",
        "outputId": "7176b384-2b93-4d96-9436-9eaa064a8e26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 128\n",
        "seq_length = 10\n",
        "n_epochs = 50 \n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/50... Step: 10... Loss: 3.1239... Val Loss: 3.0856\n",
            "Epoch: 1/50... Step: 20... Loss: 3.0397... Val Loss: 3.0064\n",
            "Epoch: 1/50... Step: 30... Loss: 3.0106... Val Loss: 2.9890\n",
            "Epoch: 1/50... Step: 40... Loss: 2.9870... Val Loss: 2.9832\n",
            "Epoch: 1/50... Step: 50... Loss: 2.9959... Val Loss: 2.9788\n",
            "Epoch: 1/50... Step: 60... Loss: 3.0008... Val Loss: 2.9778\n",
            "Epoch: 1/50... Step: 70... Loss: 2.9274... Val Loss: 2.9781\n",
            "Epoch: 1/50... Step: 80... Loss: 2.9622... Val Loss: 2.9750\n",
            "Epoch: 1/50... Step: 90... Loss: 3.0204... Val Loss: 2.9727\n",
            "Epoch: 1/50... Step: 100... Loss: 2.9649... Val Loss: 2.9659\n",
            "Epoch: 1/50... Step: 110... Loss: 3.0139... Val Loss: 2.9509\n",
            "Epoch: 1/50... Step: 120... Loss: 2.9469... Val Loss: 2.9199\n",
            "Epoch: 1/50... Step: 130... Loss: 2.9111... Val Loss: 2.8900\n",
            "Epoch: 1/50... Step: 140... Loss: 2.8482... Val Loss: 2.8130\n",
            "Epoch: 1/50... Step: 150... Loss: 2.7813... Val Loss: 2.7574\n",
            "Epoch: 1/50... Step: 160... Loss: 2.7046... Val Loss: 2.6815\n",
            "Epoch: 1/50... Step: 170... Loss: 2.6250... Val Loss: 2.6209\n",
            "Epoch: 1/50... Step: 180... Loss: 2.6304... Val Loss: 2.5549\n",
            "Epoch: 1/50... Step: 190... Loss: 2.5383... Val Loss: 2.5204\n",
            "Epoch: 1/50... Step: 200... Loss: 2.4056... Val Loss: 2.4927\n",
            "Epoch: 1/50... Step: 210... Loss: 2.4647... Val Loss: 2.4596\n",
            "Epoch: 1/50... Step: 220... Loss: 2.4879... Val Loss: 2.4413\n",
            "Epoch: 1/50... Step: 230... Loss: 2.3763... Val Loss: 2.4226\n",
            "Epoch: 1/50... Step: 240... Loss: 2.4766... Val Loss: 2.4079\n",
            "Epoch: 1/50... Step: 250... Loss: 2.4352... Val Loss: 2.3895\n",
            "Epoch: 1/50... Step: 260... Loss: 2.3405... Val Loss: 2.3798\n",
            "Epoch: 1/50... Step: 270... Loss: 2.4112... Val Loss: 2.3660\n",
            "Epoch: 1/50... Step: 280... Loss: 2.4028... Val Loss: 2.3477\n",
            "Epoch: 1/50... Step: 290... Loss: 2.2994... Val Loss: 2.3387\n",
            "Epoch: 1/50... Step: 300... Loss: 2.3494... Val Loss: 2.3240\n",
            "Epoch: 1/50... Step: 310... Loss: 2.2584... Val Loss: 2.3268\n",
            "Epoch: 1/50... Step: 320... Loss: 2.2792... Val Loss: 2.3109\n",
            "Epoch: 1/50... Step: 330... Loss: 2.3857... Val Loss: 2.3073\n",
            "Epoch: 1/50... Step: 340... Loss: 2.3001... Val Loss: 2.2916\n",
            "Epoch: 1/50... Step: 350... Loss: 2.2863... Val Loss: 2.2733\n",
            "Epoch: 1/50... Step: 360... Loss: 2.2361... Val Loss: 2.2616\n",
            "Epoch: 1/50... Step: 370... Loss: 2.2884... Val Loss: 2.2531\n",
            "Epoch: 1/50... Step: 380... Loss: 2.2993... Val Loss: 2.2458\n",
            "Epoch: 1/50... Step: 390... Loss: 2.2511... Val Loss: 2.2338\n",
            "Epoch: 1/50... Step: 400... Loss: 2.2268... Val Loss: 2.2298\n",
            "Epoch: 1/50... Step: 410... Loss: 2.2015... Val Loss: 2.2170\n",
            "Epoch: 1/50... Step: 420... Loss: 2.2405... Val Loss: 2.2115\n",
            "Epoch: 1/50... Step: 430... Loss: 2.2354... Val Loss: 2.2042\n",
            "Epoch: 1/50... Step: 440... Loss: 2.2059... Val Loss: 2.1973\n",
            "Epoch: 1/50... Step: 450... Loss: 2.2863... Val Loss: 2.1807\n",
            "Epoch: 1/50... Step: 460... Loss: 2.1844... Val Loss: 2.1745\n",
            "Epoch: 1/50... Step: 470... Loss: 2.2056... Val Loss: 2.1660\n",
            "Epoch: 1/50... Step: 480... Loss: 2.1303... Val Loss: 2.1619\n",
            "Epoch: 1/50... Step: 490... Loss: 2.2064... Val Loss: 2.1516\n",
            "Epoch: 1/50... Step: 500... Loss: 2.2183... Val Loss: 2.1444\n",
            "Epoch: 1/50... Step: 510... Loss: 2.2039... Val Loss: 2.1340\n",
            "Epoch: 1/50... Step: 520... Loss: 2.1830... Val Loss: 2.1331\n",
            "Epoch: 1/50... Step: 530... Loss: 2.1565... Val Loss: 2.1346\n",
            "Epoch: 1/50... Step: 540... Loss: 2.1341... Val Loss: 2.1295\n",
            "Epoch: 1/50... Step: 550... Loss: 2.2069... Val Loss: 2.1120\n",
            "Epoch: 1/50... Step: 560... Loss: 2.1731... Val Loss: 2.1065\n",
            "Epoch: 1/50... Step: 570... Loss: 2.1290... Val Loss: 2.0992\n",
            "Epoch: 1/50... Step: 580... Loss: 2.1390... Val Loss: 2.0923\n",
            "Epoch: 1/50... Step: 590... Loss: 2.1536... Val Loss: 2.0831\n",
            "Epoch: 1/50... Step: 600... Loss: 2.1116... Val Loss: 2.0883\n",
            "Epoch: 1/50... Step: 610... Loss: 2.0389... Val Loss: 2.0768\n",
            "Epoch: 1/50... Step: 620... Loss: 2.1204... Val Loss: 2.0631\n",
            "Epoch: 1/50... Step: 630... Loss: 2.0130... Val Loss: 2.0579\n",
            "Epoch: 1/50... Step: 640... Loss: 2.1627... Val Loss: 2.0540\n",
            "Epoch: 1/50... Step: 650... Loss: 2.0554... Val Loss: 2.0535\n",
            "Epoch: 1/50... Step: 660... Loss: 2.0278... Val Loss: 2.0505\n",
            "Epoch: 1/50... Step: 670... Loss: 2.0428... Val Loss: 2.0399\n",
            "Epoch: 1/50... Step: 680... Loss: 2.1041... Val Loss: 2.0338\n",
            "Epoch: 1/50... Step: 690... Loss: 2.0435... Val Loss: 2.0283\n",
            "Epoch: 1/50... Step: 700... Loss: 2.0708... Val Loss: 2.0189\n",
            "Epoch: 1/50... Step: 710... Loss: 2.0975... Val Loss: 2.0102\n",
            "Epoch: 1/50... Step: 720... Loss: 2.0497... Val Loss: 2.0095\n",
            "Epoch: 1/50... Step: 730... Loss: 2.0697... Val Loss: 2.0079\n",
            "Epoch: 1/50... Step: 740... Loss: 1.9511... Val Loss: 1.9993\n",
            "Epoch: 1/50... Step: 750... Loss: 2.1018... Val Loss: 1.9980\n",
            "Epoch: 1/50... Step: 760... Loss: 2.0437... Val Loss: 1.9912\n",
            "Epoch: 1/50... Step: 770... Loss: 2.0385... Val Loss: 1.9874\n",
            "Epoch: 1/50... Step: 780... Loss: 2.0064... Val Loss: 1.9872\n",
            "Epoch: 1/50... Step: 790... Loss: 2.0143... Val Loss: 1.9789\n",
            "Epoch: 1/50... Step: 800... Loss: 2.0332... Val Loss: 1.9725\n",
            "Epoch: 1/50... Step: 810... Loss: 1.9362... Val Loss: 1.9674\n",
            "Epoch: 1/50... Step: 820... Loss: 1.9795... Val Loss: 1.9662\n",
            "Epoch: 1/50... Step: 830... Loss: 1.9955... Val Loss: 1.9611\n",
            "Epoch: 1/50... Step: 840... Loss: 1.9658... Val Loss: 1.9568\n",
            "Epoch: 1/50... Step: 850... Loss: 1.9904... Val Loss: 1.9552\n",
            "Epoch: 1/50... Step: 860... Loss: 1.9813... Val Loss: 1.9490\n",
            "Epoch: 1/50... Step: 870... Loss: 1.9703... Val Loss: 1.9461\n",
            "Epoch: 1/50... Step: 880... Loss: 1.9882... Val Loss: 1.9396\n",
            "Epoch: 1/50... Step: 890... Loss: 2.0472... Val Loss: 1.9387\n",
            "Epoch: 1/50... Step: 900... Loss: 2.0247... Val Loss: 1.9343\n",
            "Epoch: 1/50... Step: 910... Loss: 1.9444... Val Loss: 1.9293\n",
            "Epoch: 1/50... Step: 920... Loss: 1.9401... Val Loss: 1.9264\n",
            "Epoch: 1/50... Step: 930... Loss: 1.9843... Val Loss: 1.9220\n",
            "Epoch: 1/50... Step: 940... Loss: 2.0509... Val Loss: 1.9153\n",
            "Epoch: 1/50... Step: 950... Loss: 1.9397... Val Loss: 1.9097\n",
            "Epoch: 1/50... Step: 960... Loss: 1.9637... Val Loss: 1.9101\n",
            "Epoch: 1/50... Step: 970... Loss: 1.9540... Val Loss: 1.9045\n",
            "Epoch: 1/50... Step: 980... Loss: 2.0388... Val Loss: 1.9032\n",
            "Epoch: 1/50... Step: 990... Loss: 2.0430... Val Loss: 1.9012\n",
            "Epoch: 1/50... Step: 1000... Loss: 1.9606... Val Loss: 1.8968\n",
            "Epoch: 1/50... Step: 1010... Loss: 1.9054... Val Loss: 1.8918\n",
            "Epoch: 1/50... Step: 1020... Loss: 1.9831... Val Loss: 1.8904\n",
            "Epoch: 1/50... Step: 1030... Loss: 1.8836... Val Loss: 1.8819\n",
            "Epoch: 1/50... Step: 1040... Loss: 1.9147... Val Loss: 1.8778\n",
            "Epoch: 1/50... Step: 1050... Loss: 1.8921... Val Loss: 1.8765\n",
            "Epoch: 1/50... Step: 1060... Loss: 1.9665... Val Loss: 1.8754\n",
            "Epoch: 1/50... Step: 1070... Loss: 2.0005... Val Loss: 1.8822\n",
            "Epoch: 1/50... Step: 1080... Loss: 1.8319... Val Loss: 1.8648\n",
            "Epoch: 1/50... Step: 1090... Loss: 1.9347... Val Loss: 1.8638\n",
            "Epoch: 1/50... Step: 1100... Loss: 1.9820... Val Loss: 1.8650\n",
            "Epoch: 1/50... Step: 1110... Loss: 1.9069... Val Loss: 1.8595\n",
            "Epoch: 1/50... Step: 1120... Loss: 1.8957... Val Loss: 1.8542\n",
            "Epoch: 1/50... Step: 1130... Loss: 1.8436... Val Loss: 1.8592\n",
            "Epoch: 1/50... Step: 1140... Loss: 1.8523... Val Loss: 1.8492\n",
            "Epoch: 1/50... Step: 1150... Loss: 1.8412... Val Loss: 1.8416\n",
            "Epoch: 1/50... Step: 1160... Loss: 1.8843... Val Loss: 1.8408\n",
            "Epoch: 1/50... Step: 1170... Loss: 1.8200... Val Loss: 1.8382\n",
            "Epoch: 1/50... Step: 1180... Loss: 1.9220... Val Loss: 1.8364\n",
            "Epoch: 1/50... Step: 1190... Loss: 1.8839... Val Loss: 1.8338\n",
            "Epoch: 1/50... Step: 1200... Loss: 1.9412... Val Loss: 1.8311\n",
            "Epoch: 1/50... Step: 1210... Loss: 1.9286... Val Loss: 1.8306\n",
            "Epoch: 1/50... Step: 1220... Loss: 1.8118... Val Loss: 1.8316\n",
            "Epoch: 1/50... Step: 1230... Loss: 1.9107... Val Loss: 1.8222\n",
            "Epoch: 1/50... Step: 1240... Loss: 1.8676... Val Loss: 1.8215\n",
            "Epoch: 1/50... Step: 1250... Loss: 1.8615... Val Loss: 1.8165\n",
            "Epoch: 1/50... Step: 1260... Loss: 1.8416... Val Loss: 1.8155\n",
            "Epoch: 1/50... Step: 1270... Loss: 1.8136... Val Loss: 1.8155\n",
            "Epoch: 1/50... Step: 1280... Loss: 1.8262... Val Loss: 1.8110\n",
            "Epoch: 1/50... Step: 1290... Loss: 1.8676... Val Loss: 1.8089\n",
            "Epoch: 1/50... Step: 1300... Loss: 1.8481... Val Loss: 1.8020\n",
            "Epoch: 1/50... Step: 1310... Loss: 1.8564... Val Loss: 1.8018\n",
            "Epoch: 1/50... Step: 1320... Loss: 1.8666... Val Loss: 1.8005\n",
            "Epoch: 1/50... Step: 1330... Loss: 1.8453... Val Loss: 1.7993\n",
            "Epoch: 1/50... Step: 1340... Loss: 1.8231... Val Loss: 1.7979\n",
            "Epoch: 1/50... Step: 1350... Loss: 1.8238... Val Loss: 1.8016\n",
            "Epoch: 1/50... Step: 1360... Loss: 1.8381... Val Loss: 1.7958\n",
            "Epoch: 1/50... Step: 1370... Loss: 1.7807... Val Loss: 1.7919\n",
            "Epoch: 1/50... Step: 1380... Loss: 1.7688... Val Loss: 1.7856\n",
            "Epoch: 1/50... Step: 1390... Loss: 1.8691... Val Loss: 1.7928\n",
            "Epoch: 1/50... Step: 1400... Loss: 1.8352... Val Loss: 1.7837\n",
            "Epoch: 1/50... Step: 1410... Loss: 1.8665... Val Loss: 1.7775\n",
            "Epoch: 1/50... Step: 1420... Loss: 1.8430... Val Loss: 1.7791\n",
            "Epoch: 1/50... Step: 1430... Loss: 1.7447... Val Loss: 1.7778\n",
            "Epoch: 1/50... Step: 1440... Loss: 1.8480... Val Loss: 1.7723\n",
            "Epoch: 1/50... Step: 1450... Loss: 1.8303... Val Loss: 1.7715\n",
            "Epoch: 1/50... Step: 1460... Loss: 1.8677... Val Loss: 1.7721\n",
            "Epoch: 1/50... Step: 1470... Loss: 1.7386... Val Loss: 1.7675\n",
            "Epoch: 1/50... Step: 1480... Loss: 1.7706... Val Loss: 1.7640\n",
            "Epoch: 1/50... Step: 1490... Loss: 1.7868... Val Loss: 1.7629\n",
            "Epoch: 1/50... Step: 1500... Loss: 1.7796... Val Loss: 1.7561\n",
            "Epoch: 1/50... Step: 1510... Loss: 1.7843... Val Loss: 1.7575\n",
            "Epoch: 1/50... Step: 1520... Loss: 1.7656... Val Loss: 1.7555\n",
            "Epoch: 1/50... Step: 1530... Loss: 1.7686... Val Loss: 1.7533\n",
            "Epoch: 1/50... Step: 1540... Loss: 1.7160... Val Loss: 1.7522\n",
            "Epoch: 1/50... Step: 1550... Loss: 1.8094... Val Loss: 1.7587\n",
            "Epoch: 1/50... Step: 1560... Loss: 1.7733... Val Loss: 1.7490\n",
            "Epoch: 1/50... Step: 1570... Loss: 1.8107... Val Loss: 1.7445\n",
            "Epoch: 1/50... Step: 1580... Loss: 1.7909... Val Loss: 1.7422\n",
            "Epoch: 1/50... Step: 1590... Loss: 1.7824... Val Loss: 1.7417\n",
            "Epoch: 1/50... Step: 1600... Loss: 1.7867... Val Loss: 1.7370\n",
            "Epoch: 1/50... Step: 1610... Loss: 1.8370... Val Loss: 1.7394\n",
            "Epoch: 1/50... Step: 1620... Loss: 1.7965... Val Loss: 1.7368\n",
            "Epoch: 1/50... Step: 1630... Loss: 1.7782... Val Loss: 1.7345\n",
            "Epoch: 1/50... Step: 1640... Loss: 1.7763... Val Loss: 1.7305\n",
            "Epoch: 1/50... Step: 1650... Loss: 1.8278... Val Loss: 1.7311\n",
            "Epoch: 1/50... Step: 1660... Loss: 1.8612... Val Loss: 1.7257\n",
            "Epoch: 1/50... Step: 1670... Loss: 1.6877... Val Loss: 1.7250\n",
            "Epoch: 1/50... Step: 1680... Loss: 1.7954... Val Loss: 1.7242\n",
            "Epoch: 1/50... Step: 1690... Loss: 1.7529... Val Loss: 1.7215\n",
            "Epoch: 1/50... Step: 1700... Loss: 1.7482... Val Loss: 1.7183\n",
            "Epoch: 1/50... Step: 1710... Loss: 1.8362... Val Loss: 1.7135\n",
            "Epoch: 1/50... Step: 1720... Loss: 1.7666... Val Loss: 1.7130\n",
            "Epoch: 1/50... Step: 1730... Loss: 1.7225... Val Loss: 1.7141\n",
            "Epoch: 1/50... Step: 1740... Loss: 1.7516... Val Loss: 1.7104\n",
            "Epoch: 1/50... Step: 1750... Loss: 1.6927... Val Loss: 1.7111\n",
            "Epoch: 1/50... Step: 1760... Loss: 1.6936... Val Loss: 1.7128\n",
            "Epoch: 1/50... Step: 1770... Loss: 1.6602... Val Loss: 1.7053\n",
            "Epoch: 1/50... Step: 1780... Loss: 1.6508... Val Loss: 1.7034\n",
            "Epoch: 1/50... Step: 1790... Loss: 1.7241... Val Loss: 1.7054\n",
            "Epoch: 1/50... Step: 1800... Loss: 1.6838... Val Loss: 1.7023\n",
            "Epoch: 1/50... Step: 1810... Loss: 1.6980... Val Loss: 1.6957\n",
            "Epoch: 1/50... Step: 1820... Loss: 1.7736... Val Loss: 1.6969\n",
            "Epoch: 1/50... Step: 1830... Loss: 1.7220... Val Loss: 1.7041\n",
            "Epoch: 1/50... Step: 1840... Loss: 1.8322... Val Loss: 1.6955\n",
            "Epoch: 1/50... Step: 1850... Loss: 1.8741... Val Loss: 1.6955\n",
            "Epoch: 1/50... Step: 1860... Loss: 1.7096... Val Loss: 1.6943\n",
            "Epoch: 1/50... Step: 1870... Loss: 1.5648... Val Loss: 1.6894\n",
            "Epoch: 1/50... Step: 1880... Loss: 1.6747... Val Loss: 1.6934\n",
            "Epoch: 1/50... Step: 1890... Loss: 1.7547... Val Loss: 1.6896\n",
            "Epoch: 1/50... Step: 1900... Loss: 1.6775... Val Loss: 1.6891\n",
            "Epoch: 1/50... Step: 1910... Loss: 1.7184... Val Loss: 1.6859\n",
            "Epoch: 1/50... Step: 1920... Loss: 1.7443... Val Loss: 1.6873\n",
            "Epoch: 1/50... Step: 1930... Loss: 1.6932... Val Loss: 1.6904\n",
            "Epoch: 1/50... Step: 1940... Loss: 1.7144... Val Loss: 1.6843\n",
            "Epoch: 1/50... Step: 1950... Loss: 1.7324... Val Loss: 1.6799\n",
            "Epoch: 1/50... Step: 1960... Loss: 1.6822... Val Loss: 1.6794\n",
            "Epoch: 1/50... Step: 1970... Loss: 1.7513... Val Loss: 1.6798\n",
            "Epoch: 1/50... Step: 1980... Loss: 1.6217... Val Loss: 1.6732\n",
            "Epoch: 1/50... Step: 1990... Loss: 1.7091... Val Loss: 1.6747\n",
            "Epoch: 1/50... Step: 2000... Loss: 1.7010... Val Loss: 1.6769\n",
            "Epoch: 1/50... Step: 2010... Loss: 1.7875... Val Loss: 1.6722\n",
            "Epoch: 1/50... Step: 2020... Loss: 1.6240... Val Loss: 1.6722\n",
            "Epoch: 1/50... Step: 2030... Loss: 1.6828... Val Loss: 1.6708\n",
            "Epoch: 1/50... Step: 2040... Loss: 1.6573... Val Loss: 1.6742\n",
            "Epoch: 1/50... Step: 2050... Loss: 1.7646... Val Loss: 1.6685\n",
            "Epoch: 1/50... Step: 2060... Loss: 1.6605... Val Loss: 1.6676\n",
            "Epoch: 1/50... Step: 2070... Loss: 1.6946... Val Loss: 1.6613\n",
            "Epoch: 1/50... Step: 2080... Loss: 1.6618... Val Loss: 1.6627\n",
            "Epoch: 1/50... Step: 2090... Loss: 1.6623... Val Loss: 1.6594\n",
            "Epoch: 1/50... Step: 2100... Loss: 1.7142... Val Loss: 1.6572\n",
            "Epoch: 1/50... Step: 2110... Loss: 1.7002... Val Loss: 1.6576\n",
            "Epoch: 1/50... Step: 2120... Loss: 1.6630... Val Loss: 1.6576\n",
            "Epoch: 1/50... Step: 2130... Loss: 1.7160... Val Loss: 1.6616\n",
            "Epoch: 1/50... Step: 2140... Loss: 1.7240... Val Loss: 1.6549\n",
            "Epoch: 1/50... Step: 2150... Loss: 1.7089... Val Loss: 1.6525\n",
            "Epoch: 1/50... Step: 2160... Loss: 1.6843... Val Loss: 1.6494\n",
            "Epoch: 1/50... Step: 2170... Loss: 1.6717... Val Loss: 1.6490\n",
            "Epoch: 1/50... Step: 2180... Loss: 1.6859... Val Loss: 1.6502\n",
            "Epoch: 1/50... Step: 2190... Loss: 1.6941... Val Loss: 1.6487\n",
            "Epoch: 1/50... Step: 2200... Loss: 1.7819... Val Loss: 1.6467\n",
            "Epoch: 1/50... Step: 2210... Loss: 1.6871... Val Loss: 1.6460\n",
            "Epoch: 1/50... Step: 2220... Loss: 1.6422... Val Loss: 1.6465\n",
            "Epoch: 1/50... Step: 2230... Loss: 1.6827... Val Loss: 1.6475\n",
            "Epoch: 1/50... Step: 2240... Loss: 1.7131... Val Loss: 1.6420\n",
            "Epoch: 1/50... Step: 2250... Loss: 1.7126... Val Loss: 1.6404\n",
            "Epoch: 1/50... Step: 2260... Loss: 1.7219... Val Loss: 1.6426\n",
            "Epoch: 1/50... Step: 2270... Loss: 1.6631... Val Loss: 1.6357\n",
            "Epoch: 1/50... Step: 2280... Loss: 1.6680... Val Loss: 1.6365\n",
            "Epoch: 1/50... Step: 2290... Loss: 1.6914... Val Loss: 1.6367\n",
            "Epoch: 1/50... Step: 2300... Loss: 1.6853... Val Loss: 1.6344\n",
            "Epoch: 1/50... Step: 2310... Loss: 1.6767... Val Loss: 1.6314\n",
            "Epoch: 1/50... Step: 2320... Loss: 1.7281... Val Loss: 1.6312\n",
            "Epoch: 1/50... Step: 2330... Loss: 1.7637... Val Loss: 1.6324\n",
            "Epoch: 1/50... Step: 2340... Loss: 1.6392... Val Loss: 1.6309\n",
            "Epoch: 1/50... Step: 2350... Loss: 1.7552... Val Loss: 1.6303\n",
            "Epoch: 1/50... Step: 2360... Loss: 1.6391... Val Loss: 1.6281\n",
            "Epoch: 1/50... Step: 2370... Loss: 1.6678... Val Loss: 1.6279\n",
            "Epoch: 1/50... Step: 2380... Loss: 1.6676... Val Loss: 1.6257\n",
            "Epoch: 1/50... Step: 2390... Loss: 1.6448... Val Loss: 1.6257\n",
            "Epoch: 1/50... Step: 2400... Loss: 1.6534... Val Loss: 1.6243\n",
            "Epoch: 1/50... Step: 2410... Loss: 1.6861... Val Loss: 1.6218\n",
            "Epoch: 2/50... Step: 2420... Loss: 1.6604... Val Loss: 1.6221\n",
            "Epoch: 2/50... Step: 2430... Loss: 1.6540... Val Loss: 1.6184\n",
            "Epoch: 2/50... Step: 2440... Loss: 1.6848... Val Loss: 1.6195\n",
            "Epoch: 2/50... Step: 2450... Loss: 1.6771... Val Loss: 1.6184\n",
            "Epoch: 2/50... Step: 2460... Loss: 1.6630... Val Loss: 1.6187\n",
            "Epoch: 2/50... Step: 2470... Loss: 1.6250... Val Loss: 1.6189\n",
            "Epoch: 2/50... Step: 2480... Loss: 1.6649... Val Loss: 1.6170\n",
            "Epoch: 2/50... Step: 2490... Loss: 1.6369... Val Loss: 1.6152\n",
            "Epoch: 2/50... Step: 2500... Loss: 1.6349... Val Loss: 1.6120\n",
            "Epoch: 2/50... Step: 2510... Loss: 1.6521... Val Loss: 1.6087\n",
            "Epoch: 2/50... Step: 2520... Loss: 1.6639... Val Loss: 1.6133\n",
            "Epoch: 2/50... Step: 2530... Loss: 1.6379... Val Loss: 1.6103\n",
            "Epoch: 2/50... Step: 2540... Loss: 1.6712... Val Loss: 1.6125\n",
            "Epoch: 2/50... Step: 2550... Loss: 1.6461... Val Loss: 1.6133\n",
            "Epoch: 2/50... Step: 2560... Loss: 1.7957... Val Loss: 1.6102\n",
            "Epoch: 2/50... Step: 2570... Loss: 1.5672... Val Loss: 1.6096\n",
            "Epoch: 2/50... Step: 2580... Loss: 1.6027... Val Loss: 1.6069\n",
            "Epoch: 2/50... Step: 2590... Loss: 1.6209... Val Loss: 1.6089\n",
            "Epoch: 2/50... Step: 2600... Loss: 1.5474... Val Loss: 1.6067\n",
            "Epoch: 2/50... Step: 2610... Loss: 1.6742... Val Loss: 1.6059\n",
            "Epoch: 2/50... Step: 2620... Loss: 1.6665... Val Loss: 1.6065\n",
            "Epoch: 2/50... Step: 2630... Loss: 1.6350... Val Loss: 1.6024\n",
            "Epoch: 2/50... Step: 2640... Loss: 1.5639... Val Loss: 1.6009\n",
            "Epoch: 2/50... Step: 2650... Loss: 1.5828... Val Loss: 1.5988\n",
            "Epoch: 2/50... Step: 2660... Loss: 1.6223... Val Loss: 1.5961\n",
            "Epoch: 2/50... Step: 2670... Loss: 1.6719... Val Loss: 1.5980\n",
            "Epoch: 2/50... Step: 2680... Loss: 1.7292... Val Loss: 1.5972\n",
            "Epoch: 2/50... Step: 2690... Loss: 1.5493... Val Loss: 1.5962\n",
            "Epoch: 2/50... Step: 2700... Loss: 1.6356... Val Loss: 1.5950\n",
            "Epoch: 2/50... Step: 2710... Loss: 1.6545... Val Loss: 1.5955\n",
            "Epoch: 2/50... Step: 2720... Loss: 1.6265... Val Loss: 1.5937\n",
            "Epoch: 2/50... Step: 2730... Loss: 1.6445... Val Loss: 1.5935\n",
            "Epoch: 2/50... Step: 2740... Loss: 1.6731... Val Loss: 1.5928\n",
            "Epoch: 2/50... Step: 2750... Loss: 1.6367... Val Loss: 1.5865\n",
            "Epoch: 2/50... Step: 2760... Loss: 1.5933... Val Loss: 1.5886\n",
            "Epoch: 2/50... Step: 2770... Loss: 1.6478... Val Loss: 1.5849\n",
            "Epoch: 2/50... Step: 2780... Loss: 1.5779... Val Loss: 1.5878\n",
            "Epoch: 2/50... Step: 2790... Loss: 1.7796... Val Loss: 1.5884\n",
            "Epoch: 2/50... Step: 2800... Loss: 1.6008... Val Loss: 1.5926\n",
            "Epoch: 2/50... Step: 2810... Loss: 1.6835... Val Loss: 1.5836\n",
            "Epoch: 2/50... Step: 2820... Loss: 1.6165... Val Loss: 1.5863\n",
            "Epoch: 2/50... Step: 2830... Loss: 1.6222... Val Loss: 1.5838\n",
            "Epoch: 2/50... Step: 2840... Loss: 1.5931... Val Loss: 1.5826\n",
            "Epoch: 2/50... Step: 2850... Loss: 1.6000... Val Loss: 1.5838\n",
            "Epoch: 2/50... Step: 2860... Loss: 1.6263... Val Loss: 1.5817\n",
            "Epoch: 2/50... Step: 2870... Loss: 1.5478... Val Loss: 1.5808\n",
            "Epoch: 2/50... Step: 2880... Loss: 1.6508... Val Loss: 1.5816\n",
            "Epoch: 2/50... Step: 2890... Loss: 1.6416... Val Loss: 1.5812\n",
            "Epoch: 2/50... Step: 2900... Loss: 1.6260... Val Loss: 1.5803\n",
            "Epoch: 2/50... Step: 2910... Loss: 1.6164... Val Loss: 1.5829\n",
            "Epoch: 2/50... Step: 2920... Loss: 1.5586... Val Loss: 1.5825\n",
            "Epoch: 2/50... Step: 2930... Loss: 1.6316... Val Loss: 1.5777\n",
            "Epoch: 2/50... Step: 2940... Loss: 1.6179... Val Loss: 1.5782\n",
            "Epoch: 2/50... Step: 2950... Loss: 1.6296... Val Loss: 1.5801\n",
            "Epoch: 2/50... Step: 2960... Loss: 1.5978... Val Loss: 1.5789\n",
            "Epoch: 2/50... Step: 2970... Loss: 1.6467... Val Loss: 1.5745\n",
            "Epoch: 2/50... Step: 2980... Loss: 1.6398... Val Loss: 1.5741\n",
            "Epoch: 2/50... Step: 2990... Loss: 1.6465... Val Loss: 1.5732\n",
            "Epoch: 2/50... Step: 3000... Loss: 1.6686... Val Loss: 1.5733\n",
            "Epoch: 2/50... Step: 3010... Loss: 1.6554... Val Loss: 1.5739\n",
            "Epoch: 2/50... Step: 3020... Loss: 1.5879... Val Loss: 1.5705\n",
            "Epoch: 2/50... Step: 3030... Loss: 1.6741... Val Loss: 1.5725\n",
            "Epoch: 2/50... Step: 3040... Loss: 1.5347... Val Loss: 1.5709\n",
            "Epoch: 2/50... Step: 3050... Loss: 1.5794... Val Loss: 1.5674\n",
            "Epoch: 2/50... Step: 3060... Loss: 1.6921... Val Loss: 1.5675\n",
            "Epoch: 2/50... Step: 3070... Loss: 1.5873... Val Loss: 1.5681\n",
            "Epoch: 2/50... Step: 3080... Loss: 1.6144... Val Loss: 1.5653\n",
            "Epoch: 2/50... Step: 3090... Loss: 1.5803... Val Loss: 1.5657\n",
            "Epoch: 2/50... Step: 3100... Loss: 1.6240... Val Loss: 1.5660\n",
            "Epoch: 2/50... Step: 3110... Loss: 1.4826... Val Loss: 1.5651\n",
            "Epoch: 2/50... Step: 3120... Loss: 1.6809... Val Loss: 1.5633\n",
            "Epoch: 2/50... Step: 3130... Loss: 1.7271... Val Loss: 1.5618\n",
            "Epoch: 2/50... Step: 3140... Loss: 1.7094... Val Loss: 1.5587\n",
            "Epoch: 2/50... Step: 3150... Loss: 1.6209... Val Loss: 1.5641\n",
            "Epoch: 2/50... Step: 3160... Loss: 1.6022... Val Loss: 1.5610\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3aH8UF3P951"
      },
      "source": [
        "## Checkpoint\n",
        "\n",
        "After training, saved the model to load it again later if we need too. Here I'm saving the parameters needed to create the same architecture, the hidden layer hyperparameters and the text characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "884oUk09P952"
      },
      "source": [
        "# change the name, for saving multiple files\n",
        "model_name = 'rnn_x_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9o-o_8g5P956"
      },
      "source": [
        "---\n",
        "## Making Predictions\n",
        "\n",
        "Now that the model is trained, I'll want to sample from it and make predictions about next characters! To sample, pass in a character and have the network predict the next character. Then take that character, pass it back in, and get another predicted character. Just keep doing this and generate a bunch of text!\n",
        "\n",
        "\n",
        "The output of RNN is from a fully-connected layer and it outputs a **distribution of next-character scores**.\n",
        "\n",
        "> To actually get the next character, applied a softmax function, which gives us a *probability* distribution that can then sample to predict the next character.\n",
        "\n",
        "### Top K sampling\n",
        "\n",
        "The predictions come from a categorical probability distribution over all the possible characters. The sample text can be made more reasonable to handle (with less variables) by only considering some $K$ most probable characters. This will prevent the network from giving completely absurd characters while allowing it to introduce some noise and randomness into the sampled text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVfWFYgpP957"
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgrW2ZbBP95-"
      },
      "source": [
        "### Priming and generating the plot\n",
        "\n",
        "Typically I'll want to prime the network to build up a hidden state. Otherwise the network will start out generating characters at random which will not generate a decent horror plot. In general the first bunch of characters will be a little rough since it hasn't built up a long history of characters to predict from."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0YOTdKgP95_"
      },
      "source": [
        "def PlotGenerate(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw8DDZaZl32M"
      },
      "source": [
        "##Plot Generation by the Neural Network\n",
        "\n",
        "Outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIeoYiCKP96E",
        "outputId": "c2559ec7-0c93-47c1-ba80-c7ab2843a5c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(PlotGenerate(net, 1000, prime='A girl hanging from a tree upside down', top_k=5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scenes to stand them they have been a mother that, but she should take him to the tribe to heal both mare, and this is being restoning their apartment and he has no interview on thinking that the season two sent the screen as he could step for too clate to the car. she says they are standing on tom and tells him that he's an american for the tournament, and says she has no interesting the police decision in a metaphor.is talking as his wife soon of the same time. the scortion thinks suspicious. the second to gathing her featured, but tarek says that he had a marriage, still hugh alerand (a fearing of closer and starts to talk about how to get a marine and still thinks that this is bringing target in the team about leging the prisoners. they start to get help off at his still, as the two of them drives off the courter that is freelan from the first manish that he has burned her for the county and son it is being striggering her time he sees her way for a child. a man named seems to bang the bottle. solomon is completely explaining to say that his behavior it is brought away where the strogoi would be the trunk. they give up back, but he is coming to the fourth season and so she will be talking to the same entire children, well helps the bomb on the first could his his house with supervisor. the cop tarred her fire standing a prop-6 and he speaks with the phone. after teddy and that she will be reminded of the battle sintered by the father and a previous church, which he and collapses the something; after the protecting castle, but they have been human. she asks where he was blood in the time and starts about her from behind the scene that is soon that they can't send her and a continue to get on his, and he is crushing the biters every car with the corleone patients behind the spirit magic to the science car the tournament counter and starts to be the first and the streets of the back of the front gathering and hopes of the telephone to the back of the side, the sexual activist is able to tells her to be chasing the seaton and, the two of them alice. he takes the girl was to the same time. john only a performance is stranged in tarning.tom also all the tournament.talking about the comic book by string into his control. as the character warns to help his, she doesn't know why the same time later had a peacekeepers, she starts to bases a train and how to share his campaign story at the building and he staleek is attracted by salameh and a marine is aware that she will be seen in their, looking along a critain and another flats and says that as the those and their decision that the scent is stopped by a milk a small station and hit the partnership. the cullens says the scortes his chopper is comes as an also talk to secure the cullens and the prisoners are crichton the people of the circumstance, but at the present; he can get a brief story. the there and his secret realizes it will be completely a bomb to the prisoners. she has sacet its told by the pain of the battlefield and seemed moves tries to see that sometimes it's all become a small descent tho winch back to the chicches, but he cut hit him and his previous through the series, and a party is carried having to be able to company. he says that he had a friend, sawa, and he walks away from her. and so they are shown as them and in the confrict of the time.the sexual activist still sees he will be seen an imputs with the problem to buy something that he will have sex with her world war ii and her home with support. he tells him that someone talks to the time of a then technique to the story, as he sees the moment and told him she doesn't become his own scent, and the men is closing the cullens have a commanding his car from her brutality and says, \"and she takes a moment of the target that shows malcolm that she has a spy who can be an alarm and his points if she doesn't know when he's shitched in a surrounding, she will be. seligman realizes that she is stopped by and he's surprised, at the bank figures of a pointing of the series, a stone for a stolen communication as well as the confrontation, she slips her through a sharo so that he is a good point of the chair, but the senset to get to him. he tells him about how to straightening the cullens and his story tournament cross through the process of sex and his frataries, and she has breek a short battle, setting the priest that the monologue. tells the temple as they go to the floor. this is shown that he human roables. she also adds that they are the story in the battlefield on him, but they should see if she doesn't believe hes that they completely eret as he is in the crowd is intercup tracking and raises the protest that another draws her body straded of the constilution that he is any going towards. he starts to climb the street into the program how actian attending to find her show or that his plantation to attack their days at the bathroom before the character search for simon to be able to thinks: the commander says that the stranger think in that means they are carrying a protecting and a sexual events of tong po's contents. malcolm is three secretary sexual in shocking steve clear that anna to be coming against the scene. he tells the card is in the car, as a character informs him what this meeting. joe survives and terrifying him a stage of her family about the series' at all of them, but he is, telling the tracking device and has the corleone computer and the same railway talks if they help him. she tells him he would like to be a friends watch. the correcting crossword that she hadn't want to get her to the scene of the fact that scar and that scorpius was to be capturing susan's head and is that they alert him back into the passon. and anna is stabbed. she's been at she asked her to see it off and an elderly marriage is a progony of an ilanted arm, she shoots her with a small bottle of the police station. mia's last car at the contents to him and says that they wrent himself for her to the first person. she says that there was some conflicting for the beginning of the first person. at this time that he can go to a parking room. seth takes a great series are chosing her, they have to leave a might and seem a second time, all adostimately to the corleone second and shoots him to be a matter of the top sam to the tribe and he sent tony, so they want, the statested from tournament would be. malcolm tells supervisor and takes him into bella. and she says he has a man are able to rulling himself in sexual and says; the means that senies, and that milk has seen in the castro to take off her the opening docks. india and the same street is satisfied when she is in the city's could higher on the front door. all, in ambert of the crossward characters; hand and her castle. he that she wasn't about to be a restaurant that shaw second life when she is an assumption once of the secret she will never return about a mindrapersional story and the second room fox someone else that he still has to be as a surgical strength, and he calls his hostage that assembers an ally to the police. holmes sees the time against pooh-bear and the town was mentally being help to the street, and selving shot on a booby and still, solomon and the captured his from the family well down. at that moment was a point of the book as he rishes from the chirace breaks. bella is and his probably a far who she would leave.\" telling him where they atear the truth, and she comes home, we see that she has solved him in the best. he sees a muscular door, and the cop sees the street for the police. he has the bed he cut order a meat in. shaw is shot and that the compromision at the techies to go to the forehol and assumes he is some start in the front of him, but it survives his highest computer who are the best first, a meat of season two are somewhat and solomon start at a series of confrontation, and he will have to steal to start on the shadows and attempts to call him that the tracker is shot in order to speak to them. the tracking dimprish start to break them worried to the car for the car. after a police. john solving off a motivated but the camera search signature, he sees that he's a place before and say why hes allowed the party's release and sees his sight and stops and she had been talking. after her feat, she sees that some time is dranked to the fight shot and talked on the ground. the street is surpressing that some of the same this mini-seation is ablusted in the castro.milk's considers and suggests, however, they can become the train out, so he can remove an unarrunation. the two men shaves they start to go to how teddy's best. he comes home to him. she shows him his corsciouster tracking dot and the police shie chases her to throw, and much his family at the corse, and as the scene spirs a strength, because it's now held both and that his father, helled him with a stranger station, but he had been stating about the first time, they continue terrifying him at all and her sentences are the ordining story to a surviving about her being about a brother bringing her same things that the contestants would like a computer change but its consequented the but tells him to shoot on the room; she says that shes building to the street.the scene still had a call a bloody of the same side of the corleone concentration, and a prisoners look out. she then allows malcolm how that there was only the chemical remaining stage and his one that he work. they get a moss stathouse, tony to take on her crap of the film. he tells her that the most day, with a bar wighing have been traced in one work that the same time was the couple and tells the guards to the scene of the films inside, and shortly then shots the bomb to the school, and makes his machine is decided from the soldiers in the time singer that she's been seen talking about what she would be clean that he stands away. she tells her that the top, he tries to stay and sees his corpsite, and tony asks him what to trape him before leaving the people and tells him the brother was telling the same room before and the men to get at the street their confrontations. she sees a film plays in on the bottom of the second as well. he approaches. he has alro knows that the corleone said is the only private to try and has harder, and this is being help of the boy where they say now something with someone come in the terrorist. malcolm and she adds that a small party is also allowing the tational mind, but she's been conversation, and they are all be about to have television. she's too more than that she would have a plan on the stage to the car. jerme asks how after her. the two spots his friend who seems on the teachers and all with her at him in the trust. johnson has taken the car. there will grab accorsing them. a man is awred to sandwii again and starts to state on, and he has beated his chase to see if he will be happy to the tournament. as a second and says she said. he is the one who could have no set one time. the concert was somewhat and hurt a boobs of sexual actions and the three dm's that he was dated by the fireplowed and says that hes have been head to his father, allowing the bottom on the street, taken into his first stage of the first cylon district. at the soldier, so milk's soon, she would be stayed in of a couple of tournament contestants all of their friends in the back, before she starts to break together. she received the protegning after a back of the body and starts a few supervisor' because she who stopped him and tells him that the couch were assassinated with aside, who have been seen attacked by her first.in a flashback, too comes to his character was a shot and she alice sought that there's a small drawn.tony causes the street at the some of the car at the stop carraining to the trees, he tells him. he's numbers and it had a motel concest of a shot off. she tries to reach the problem that they're comen back to the stage and the support holds a river arm still broke to survive. as the track is seen any months. after solomon, and that milk trained him with a based all battle-a cell in have found security assassination with the still and they are not all alan for their shadow with an extent trace. the scientisted would let. then meets her before he tells them that he is a memoryal strong that would be a male think and seems to help her assistance and their business says, \"taking him off on the fire. a commander that all as the meeting we see them is seen held and they're start to hear an intercourters and so secure of the truck, the cullen crime to get a metal doctors to cannot support, but something to say that his body tortured by the battlefield. a farther season trained, says that she would have seen them out of the campaign station; a monitors wants to be the series' party the bus is training his son's death, but after his for her. he says her senses. she then then resules he doesn't think he will be the same exactly is stopped by the carriage father macavoy to a factory to help his son. the modiscrowlarlus astended the battle that he's the south footage of several team, and starts to be a major and collection with the convenience to be forced to recaute this is a club.as a sudden convicts want to go to the table.the temperation armed is droming the county and they see his wife and daughter are told that they want to go but and miles says that the machine is telling her to stay their feutce, and she sees he's to to talk to be a sex at thought of her story to try to say anything in the field, he shering at the car and starts to brother since he hoters cant committed by the terrified, and they are trief to control the foandary. the second man talks that he has all a past the bus. she shows his high station, while how managed to take off the pathetic display after his hair by crichton shooting their trip were named he will cause other grand crash and some one of the train, tony appears only a place in the face and him. the three dm's are a series of attacks, when john shoots the beginning of the story and to the three dm's. after say, he is seen a flashback of a far handly said and she had been said at a things who then goes out of the truth; it was done wouldn't take an incompoter. he also talks that he was the one who suggests she can get a police wonderland and the computer tracker is told that alan has always seen a fartary through the tournament. he sees it to be deal. the conversation to the stairs, holmes stops the speech. he came out of his peece. she says that if he does not know why he has the two monitors the beard and he wants. he then realizes that he wants to believe the second respect for the production as a sherisapeap, since tony's face, but she is introductibled, but she was the one from the time of having been to the present man to help see his complete, steve the series, states that they want to stay for his children in the fourth season, to the campsite at a teen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p02SLWsDqrK"
      },
      "source": [
        "print(PlotGenerate(net, 500, prime='Crippled person possessed by haunted spirits', top_k=10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8IUF2oirFSg"
      },
      "source": [
        "print(PlotGenerate(net, 1200, prime='Cawing sounds echo the mountains', top_k=5))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}