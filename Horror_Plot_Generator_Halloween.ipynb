{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Horror Plot Generator - Halloween.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepika-rana/Horror-Plot-Generator/blob/main/Horror_Plot_Generator_Halloween.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VF3MczVP94x"
      },
      "source": [
        "# Horror Plot Generator\n",
        "\n",
        "A character-level LSTM is contructed with PyTorch to generate a horror plot on the ocassion of Halloween. The network will train character by character on some text, then generate new text character by character. \n",
        "\n",
        "The input used to train this model was from a public dataset on kaggle that has more than 15k movie plots.\n",
        "**This model will be able to generate new text based on the text from the lyrics!**\n",
        "\n",
        "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FHmKOjfP941"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV9iBXXhP946"
      },
      "source": [
        "## Load in Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DZfM0ejxaJ0",
        "outputId": "c53ccb05-25bb-4a82-c34f-bdb8f075a9a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlUBlk4exqcl"
      },
      "source": [
        "path = \"/content/drive/My Drive/mpst_full_data.csv\"\n",
        "data = pd.read_csv(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UV16UIF5pQy",
        "outputId": "76cacd92-4275-4ebf-810d-d16906ce5eba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        }
      },
      "source": [
        "genre_based = data[data['tags'].str.contains('horror','dark') == True].copy()\n",
        "plot_based = data[data['plot_synopsis'].str.contains('violence','murder') == True].copy()\n",
        "suspense_based = data[data['tags'].str.contains('suspenseful','story telling') == True].copy()\n",
        "story_based = data[data['tags'].str.contains('mystery','cruelty') == True].copy()\n",
        "fear_based = data[data['tags'].str.contains('paranormal','haunting') == True].copy()\n",
        "data = pd.concat([genre_based, plot_based,suspense_based,story_based,fear_based]).drop_duplicates().reset_index(drop=True)\n",
        "data = plot_based.dropna(subset=['tags'])\n",
        "data[:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>imdb_id</th>\n",
              "      <th>title</th>\n",
              "      <th>plot_synopsis</th>\n",
              "      <th>tags</th>\n",
              "      <th>split</th>\n",
              "      <th>synopsis_source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>217</th>\n",
              "      <td>tt0104797</td>\n",
              "      <td>Malcolm X</td>\n",
              "      <td>As the opening credits roll, we hear Malcolm X...</td>\n",
              "      <td>murder, cult, violence, flashback, historical,...</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1391</th>\n",
              "      <td>tt1686821</td>\n",
              "      <td>Vampire Academy</td>\n",
              "      <td>Two years ago. A dark, forest lined road:Rose ...</td>\n",
              "      <td>comedy, murder, violence, plot twist, flashbac...</td>\n",
              "      <td>val</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1433</th>\n",
              "      <td>tt0408306</td>\n",
              "      <td>Munich</td>\n",
              "      <td>The film begins with a depiction of the events...</td>\n",
              "      <td>avant garde, mystery, murder, realism, dramati...</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1676</th>\n",
              "      <td>tt0471041</td>\n",
              "      <td>The Tournament</td>\n",
              "      <td>A series of TV news reports are shown about ac...</td>\n",
              "      <td>comedy, murder, violence, flashback, revenge, ...</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1739</th>\n",
              "      <td>tt1322302</td>\n",
              "      <td>La hora cero</td>\n",
              "      <td>The film begins at night, at the Venezuelan sl...</td>\n",
              "      <td>murder, flashback</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1877</th>\n",
              "      <td>tt2084970</td>\n",
              "      <td>The Imitation Game</td>\n",
              "      <td>BASED ON A TRUE STORY.We hear Alan Turing sayi...</td>\n",
              "      <td>tragedy, humor, historical, mystery, flashback</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020</th>\n",
              "      <td>tt0099674</td>\n",
              "      <td>The Godfather: Part III</td>\n",
              "      <td>The movie begins in 1979, with a brief flashba...</td>\n",
              "      <td>murder, dramatic, cult, flashback, good versus...</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2101</th>\n",
              "      <td>tt0376541</td>\n",
              "      <td>Closer</td>\n",
              "      <td>Dan (Jude Law) is a young writer walking down ...</td>\n",
              "      <td>dark, psychological, stupid, bleak, dramatic, ...</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2108</th>\n",
              "      <td>tt0335563</td>\n",
              "      <td>Wonderland</td>\n",
              "      <td>John Holmes and Dawn SchillerLos Angeles, June...</td>\n",
              "      <td>revenge, cruelty, murder, violence, flashback</td>\n",
              "      <td>test</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2434</th>\n",
              "      <td>tt0493464</td>\n",
              "      <td>Wanted</td>\n",
              "      <td>A young man named Wesley Gibson (James McAvoy)...</td>\n",
              "      <td>murder, cult, violence, plot twist, flashback,...</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2456</th>\n",
              "      <td>tt0082782</td>\n",
              "      <td>My Bloody Valentine</td>\n",
              "      <td>Two miners in full gear, including face masks,...</td>\n",
              "      <td>cult, murder, violence, flashback</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2467</th>\n",
              "      <td>tt0387736</td>\n",
              "      <td>Farscape: The Peacekeeper Wars</td>\n",
              "      <td>The story itself begins with a flashback, and ...</td>\n",
              "      <td>romantic</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2484</th>\n",
              "      <td>tt0070332</td>\n",
              "      <td>Lolly-Madonna XXX</td>\n",
              "      <td>This isn't perfect - I'm doing this from memor...</td>\n",
              "      <td>revenge, prank, murder, violence, flashback</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2549</th>\n",
              "      <td>tt0099637</td>\n",
              "      <td>Un gatto nel cervello</td>\n",
              "      <td>Dr. Lucio Fulci (more or less playing himself)...</td>\n",
              "      <td>violence</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2752</th>\n",
              "      <td>tt2382009</td>\n",
              "      <td>Nymphomaniac: Vol. II</td>\n",
              "      <td>This film is a continuation of Nymphomaniac: V...</td>\n",
              "      <td>flashback</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2944</th>\n",
              "      <td>tt1034032</td>\n",
              "      <td>Gamer</td>\n",
              "      <td>The film begins some years in the future from ...</td>\n",
              "      <td>comedy, dark, murder, stupid, cult, alternate ...</td>\n",
              "      <td>test</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2950</th>\n",
              "      <td>tt0801526</td>\n",
              "      <td>The Tracey Fragments</td>\n",
              "      <td>Because this film is literally a spliced colla...</td>\n",
              "      <td>flashback</td>\n",
              "      <td>test</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3056</th>\n",
              "      <td>tt1129442</td>\n",
              "      <td>Transporter 3</td>\n",
              "      <td>The film begins with a ship in the ocean. Two ...</td>\n",
              "      <td>murder, stupid, violence, flashback, psychedel...</td>\n",
              "      <td>train</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3071</th>\n",
              "      <td>tt2024544</td>\n",
              "      <td>12 Years a Slave</td>\n",
              "      <td>The movie opens with a group of slaves receivi...</td>\n",
              "      <td>dramatic, boring, historical</td>\n",
              "      <td>test</td>\n",
              "      <td>imdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3298</th>\n",
              "      <td>tt0250258</td>\n",
              "      <td>Das Experiment</td>\n",
              "      <td>While reading a newspaper advertisement, taxi ...</td>\n",
              "      <td>psychological, murder, thought-provoking, viol...</td>\n",
              "      <td>test</td>\n",
              "      <td>wikipedia</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        imdb_id                           title  ...  split synopsis_source\n",
              "217   tt0104797                       Malcolm X  ...  train            imdb\n",
              "1391  tt1686821                 Vampire Academy  ...    val            imdb\n",
              "1433  tt0408306                          Munich  ...  train            imdb\n",
              "1676  tt0471041                  The Tournament  ...  train            imdb\n",
              "1739  tt1322302                    La hora cero  ...  train            imdb\n",
              "1877  tt2084970              The Imitation Game  ...  train            imdb\n",
              "2020  tt0099674         The Godfather: Part III  ...  train            imdb\n",
              "2101  tt0376541                          Closer  ...  train            imdb\n",
              "2108  tt0335563                      Wonderland  ...   test            imdb\n",
              "2434  tt0493464                          Wanted  ...  train            imdb\n",
              "2456  tt0082782             My Bloody Valentine  ...  train            imdb\n",
              "2467  tt0387736  Farscape: The Peacekeeper Wars  ...  train            imdb\n",
              "2484  tt0070332               Lolly-Madonna XXX  ...  train            imdb\n",
              "2549  tt0099637           Un gatto nel cervello  ...  train            imdb\n",
              "2752  tt2382009           Nymphomaniac: Vol. II  ...  train            imdb\n",
              "2944  tt1034032                           Gamer  ...   test            imdb\n",
              "2950  tt0801526            The Tracey Fragments  ...   test            imdb\n",
              "3056  tt1129442                   Transporter 3  ...  train            imdb\n",
              "3071  tt2024544                12 Years a Slave  ...   test            imdb\n",
              "3298  tt0250258                  Das Experiment  ...   test       wikipedia\n",
              "\n",
              "[20 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Zc9DCwK59vy",
        "outputId": "11615343-c8f8-4c21-ebf5-9bb1873e43b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "plot = ''.join(data.plot_synopsis)\n",
        "plot = plot.lower()\n",
        "plot_based = ''.join([c for c in plot if c not in \"#$%&()*+,-./:;<=>@[\\\\]^_`{|}~\"])\n",
        "plot[:100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'as the opening credits roll, we hear malcolm x (denzel washington) delivering a speech about the opp'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHsd3Mk_P95C"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "In the cells, below, I'm creating a couple **dictionaries** to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ciGq1_QP95D"
      },
      "source": [
        "# Creating two dictionaries:\n",
        "# 1. int2char, which maps integers to characters\n",
        "# 2. char2int, which maps characters to unique integers\n",
        "chars = tuple(set(plot))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encoding the text\n",
        "encoded = np.array([char2int[ch] for ch in plot])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHwpVTRwP95G"
      },
      "source": [
        "And those same characters from above, are encoded as integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5jCICfLP95J",
        "outputId": "656bd1ad-1834-43a2-933f-076760ee583d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "encoded[:100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([68, 23, 14, 21, 18, 15, 14, 58, 79, 15, 54, 66, 54, 19, 14, 13, 70,\n",
              "       15, 67, 66, 21, 23, 14, 70, 58,  6,  6, 75, 14,  9, 15, 14, 18, 15,\n",
              "       68, 70, 14,  7, 68,  6, 13, 58,  6,  7, 14, 69, 14,  4, 67, 15, 54,\n",
              "       39, 15,  6, 14,  9, 68, 23, 18, 66, 54, 19, 21, 58, 54, 62, 14, 67,\n",
              "       15,  6, 66, 56, 15, 70, 66, 54, 19, 14, 68, 14, 23, 79, 15, 15, 13,\n",
              "       18, 14, 68, 22, 58, 10, 21, 14, 21, 18, 15, 14, 58, 79, 79])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0h75OgaP95M"
      },
      "source": [
        "## Pre-processing the data\n",
        "\n",
        "The LSTM expects an input that is **one-hot encoded** meaning that each character is converted into an integer (via our created dictionary) and *then* converted into a column vector where only it's corresponding integer index will have the value of 1 and the rest of the vector will be filled with 0's. One-hot encoding the data using a function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icEEMrosP95N"
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOBVj5ZCP95P",
        "outputId": "bbcd2af9-fff7-4f39-f647-e3a4152e0491",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Check that the function works as expected\n",
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_MRM8INP95T"
      },
      "source": [
        "## Making training mini-batches\n",
        "\n",
        "\n",
        "To train on this data, I created mini-batches for training. \n",
        "\n",
        "In this example, taking the encoded characters (passed in as the `arr` parameter) and split them into multiple sequences, given by `batch_size`. Each of the sequences will be `seq_length` long.\n",
        "\n",
        "### Creating Batches\n",
        "\n",
        "**1. The first thing is to discard some of the text so to obtain completely full mini-batches.**\n",
        "\n",
        "**2. After that, split `arr` into $N$ batches.** \n",
        "\n",
        "**3. Now that we have this array, we can iterate through it to get our mini-batches.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss46q46iP95U"
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''A generator that returns batches of size\n",
        "       batch_size x seq_length from arr.\n",
        "       \n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence\n",
        "    '''\n",
        "    \n",
        "    batch_size_total = batch_size * seq_length\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q48pKMhbP95X"
      },
      "source": [
        "### Testing the Implementation\n",
        "\n",
        "Now I'll make some data sets and we can check out what's going on as we batch data. Here, as an example, I'm going to use a batch size of 8 and 50 sequence steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0MjPHLfP95Z"
      },
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_41dpMq7P95b",
        "outputId": "8c50acb4-3170-44b3-f4f6-c030aca5740e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "# printing out the first 10 items in a sequence\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[68 23 14 21 18 15 14 58 79 15]\n",
            " [43 66 70 15 67 75 14 21 18 66]\n",
            " [14 13 58 13 68 66 54 15 14 68]\n",
            " [14 58 10 21 14 58 43 14 18 15]\n",
            " [18 15  7 14 58 22 15 67 66 15]\n",
            " [ 6 58  9 23 14  9 66  6  6 14]\n",
            " [70 14 70 15 23 79 15 13 21 14]\n",
            " [10 19 23 14 22 15 19 66 54 23]]\n",
            "\n",
            "y\n",
            " [[23 14 21 18 15 14 58 79 15 54]\n",
            " [66 70 15 67 75 14 21 18 66 23]\n",
            " [13 58 13 68 66 54 15 14 68 67]\n",
            " [58 10 21 14 58 43 14 18 15 70]\n",
            " [15  7 14 58 22 15 67 66 15 54]\n",
            " [58  9 23 14  9 66  6  6 14 22]\n",
            " [14 70 15 23 79 15 13 21 14 68]\n",
            " [19 23 14 22 15 19 66 54 23 14]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIukVIXjP95e"
      },
      "source": [
        "---\n",
        "## Defining the network with PyTorch\n",
        "\n",
        "Next, I'll use PyTorch to define the architecture of the network. Started by defining the layers and operations. Then, define a method for the forward pass. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP5k8CfjP95i",
        "outputId": "1500595b-acca-4d2b-c573-51a518be5faf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else: \n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogFmtSx_P95m"
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2,  \n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        ## define the layers of the model\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "                \n",
        "        ## Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        # pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        # Stack up LSTM outputs using view\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNMiIgz8P95q"
      },
      "source": [
        "## Time to train\n",
        "\n",
        "The train function gives us the ability to set the number of epochs, the learning rate, and other parameters.\n",
        "\n",
        "Below using an Adam optimizer and cross entropy loss since as looking at character class scores as output. Calculate the loss and perform backpropagation.\n",
        "\n",
        "A couple of details about training: \n",
        ">* Within the batch loop, detach the hidden state from its history; this time setting it equal to a new *tuple* variable because an LSTM has a hidden state that is a tuple of the hidden and cell states.\n",
        "* We use [`clip_grad_norm_`](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html) to help prevent exploding gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLanJBSsU_2C"
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXiZCqs3P95t"
      },
      "source": [
        "## Instantiating the model\n",
        "\n",
        "Actually training the network. First creating the network itself, with some given hyperparameters. Then, define the mini-batches sizes, and start training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRkBkzcxP95u",
        "outputId": "c92d570a-7e4a-40d3-e479-dc0e6a55233b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "#Set model hyperparameters\n",
        "# define and print the net\n",
        "n_hidden=512\n",
        "n_layers=2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(80, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=80, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzLRrSIHP95w"
      },
      "source": [
        "### Set training hyperparameters!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6Egd98q5P95x",
        "outputId": "1836860a-6ef1-4285-87e8-b5ac7fda3449",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 128\n",
        "seq_length = 10\n",
        "n_epochs = 50 \n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/50... Step: 10... Loss: 1.3502... Val Loss: 1.5869\n",
            "Epoch: 1/50... Step: 20... Loss: 1.4028... Val Loss: 1.5971\n",
            "Epoch: 1/50... Step: 30... Loss: 1.3971... Val Loss: 1.5950\n",
            "Epoch: 1/50... Step: 40... Loss: 1.3744... Val Loss: 1.5963\n",
            "Epoch: 1/50... Step: 50... Loss: 1.3402... Val Loss: 1.5910\n",
            "Epoch: 1/50... Step: 60... Loss: 1.4194... Val Loss: 1.6001\n",
            "Epoch: 1/50... Step: 70... Loss: 1.4217... Val Loss: 1.5914\n",
            "Epoch: 1/50... Step: 80... Loss: 1.3131... Val Loss: 1.5922\n",
            "Epoch: 1/50... Step: 90... Loss: 1.3567... Val Loss: 1.5925\n",
            "Epoch: 1/50... Step: 100... Loss: 1.3382... Val Loss: 1.5969\n",
            "Epoch: 1/50... Step: 110... Loss: 1.4342... Val Loss: 1.5907\n",
            "Epoch: 1/50... Step: 120... Loss: 1.3653... Val Loss: 1.5826\n",
            "Epoch: 1/50... Step: 130... Loss: 1.4344... Val Loss: 1.5955\n",
            "Epoch: 1/50... Step: 140... Loss: 1.4031... Val Loss: 1.5915\n",
            "Epoch: 1/50... Step: 150... Loss: 1.3863... Val Loss: 1.5858\n",
            "Epoch: 1/50... Step: 160... Loss: 1.4199... Val Loss: 1.5872\n",
            "Epoch: 1/50... Step: 170... Loss: 1.4317... Val Loss: 1.5888\n",
            "Epoch: 1/50... Step: 180... Loss: 1.3794... Val Loss: 1.5814\n",
            "Epoch: 1/50... Step: 190... Loss: 1.3895... Val Loss: 1.5977\n",
            "Epoch: 1/50... Step: 200... Loss: 1.2981... Val Loss: 1.5876\n",
            "Epoch: 1/50... Step: 210... Loss: 1.4342... Val Loss: 1.5836\n",
            "Epoch: 1/50... Step: 220... Loss: 1.3410... Val Loss: 1.5937\n",
            "Epoch: 1/50... Step: 230... Loss: 1.3972... Val Loss: 1.5806\n",
            "Epoch: 1/50... Step: 240... Loss: 1.4902... Val Loss: 1.5899\n",
            "Epoch: 1/50... Step: 250... Loss: 1.3927... Val Loss: 1.5831\n",
            "Epoch: 1/50... Step: 260... Loss: 1.4050... Val Loss: 1.5859\n",
            "Epoch: 1/50... Step: 270... Loss: 1.4121... Val Loss: 1.5799\n",
            "Epoch: 1/50... Step: 280... Loss: 1.3937... Val Loss: 1.5829\n",
            "Epoch: 1/50... Step: 290... Loss: 1.3683... Val Loss: 1.5900\n",
            "Epoch: 1/50... Step: 300... Loss: 1.3545... Val Loss: 1.5882\n",
            "Epoch: 1/50... Step: 310... Loss: 1.4163... Val Loss: 1.5798\n",
            "Epoch: 1/50... Step: 320... Loss: 1.3274... Val Loss: 1.5885\n",
            "Epoch: 1/50... Step: 330... Loss: 1.4394... Val Loss: 1.5827\n",
            "Epoch: 1/50... Step: 340... Loss: 1.4851... Val Loss: 1.5911\n",
            "Epoch: 1/50... Step: 350... Loss: 1.3381... Val Loss: 1.5851\n",
            "Epoch: 1/50... Step: 360... Loss: 1.3540... Val Loss: 1.5946\n",
            "Epoch: 1/50... Step: 370... Loss: 1.4272... Val Loss: 1.5843\n",
            "Epoch: 1/50... Step: 380... Loss: 1.4608... Val Loss: 1.5839\n",
            "Epoch: 1/50... Step: 390... Loss: 1.3356... Val Loss: 1.5860\n",
            "Epoch: 1/50... Step: 400... Loss: 1.4143... Val Loss: 1.5808\n",
            "Epoch: 1/50... Step: 410... Loss: 1.3514... Val Loss: 1.5859\n",
            "Epoch: 1/50... Step: 420... Loss: 1.3932... Val Loss: 1.5812\n",
            "Epoch: 1/50... Step: 430... Loss: 1.3820... Val Loss: 1.5829\n",
            "Epoch: 1/50... Step: 440... Loss: 1.4422... Val Loss: 1.5817\n",
            "Epoch: 1/50... Step: 450... Loss: 1.3889... Val Loss: 1.5836\n",
            "Epoch: 1/50... Step: 460... Loss: 1.4088... Val Loss: 1.5810\n",
            "Epoch: 1/50... Step: 470... Loss: 1.3452... Val Loss: 1.5761\n",
            "Epoch: 1/50... Step: 480... Loss: 1.4009... Val Loss: 1.5822\n",
            "Epoch: 1/50... Step: 490... Loss: 1.4463... Val Loss: 1.5789\n",
            "Epoch: 1/50... Step: 500... Loss: 1.4145... Val Loss: 1.5900\n",
            "Epoch: 1/50... Step: 510... Loss: 1.3574... Val Loss: 1.5796\n",
            "Epoch: 1/50... Step: 520... Loss: 1.9155... Val Loss: 1.5856\n",
            "Epoch: 2/50... Step: 530... Loss: 1.3941... Val Loss: 1.5879\n",
            "Epoch: 2/50... Step: 540... Loss: 1.3834... Val Loss: 1.5879\n",
            "Epoch: 2/50... Step: 550... Loss: 1.3807... Val Loss: 1.5935\n",
            "Epoch: 2/50... Step: 560... Loss: 1.3827... Val Loss: 1.5928\n",
            "Epoch: 2/50... Step: 570... Loss: 1.3710... Val Loss: 1.5946\n",
            "Epoch: 2/50... Step: 580... Loss: 1.4236... Val Loss: 1.5936\n",
            "Epoch: 2/50... Step: 590... Loss: 1.3739... Val Loss: 1.5857\n",
            "Epoch: 2/50... Step: 600... Loss: 1.3047... Val Loss: 1.5924\n",
            "Epoch: 2/50... Step: 610... Loss: 1.3778... Val Loss: 1.5833\n",
            "Epoch: 2/50... Step: 620... Loss: 1.3202... Val Loss: 1.5902\n",
            "Epoch: 2/50... Step: 630... Loss: 1.4458... Val Loss: 1.5869\n",
            "Epoch: 2/50... Step: 640... Loss: 1.4017... Val Loss: 1.5814\n",
            "Epoch: 2/50... Step: 650... Loss: 1.4683... Val Loss: 1.5918\n",
            "Epoch: 2/50... Step: 660... Loss: 1.3393... Val Loss: 1.5887\n",
            "Epoch: 2/50... Step: 670... Loss: 1.3895... Val Loss: 1.5793\n",
            "Epoch: 2/50... Step: 680... Loss: 1.4069... Val Loss: 1.5924\n",
            "Epoch: 2/50... Step: 690... Loss: 1.4690... Val Loss: 1.5794\n",
            "Epoch: 2/50... Step: 700... Loss: 1.3559... Val Loss: 1.5774\n",
            "Epoch: 2/50... Step: 710... Loss: 1.3815... Val Loss: 1.5879\n",
            "Epoch: 2/50... Step: 720... Loss: 1.3086... Val Loss: 1.5785\n",
            "Epoch: 2/50... Step: 730... Loss: 1.4273... Val Loss: 1.5884\n",
            "Epoch: 2/50... Step: 740... Loss: 1.3289... Val Loss: 1.5793\n",
            "Epoch: 2/50... Step: 750... Loss: 1.3839... Val Loss: 1.5869\n",
            "Epoch: 2/50... Step: 760... Loss: 1.4944... Val Loss: 1.5799\n",
            "Epoch: 2/50... Step: 770... Loss: 1.3563... Val Loss: 1.5869\n",
            "Epoch: 2/50... Step: 780... Loss: 1.3861... Val Loss: 1.5853\n",
            "Epoch: 2/50... Step: 790... Loss: 1.3853... Val Loss: 1.5764\n",
            "Epoch: 2/50... Step: 800... Loss: 1.4078... Val Loss: 1.5824\n",
            "Epoch: 2/50... Step: 810... Loss: 1.3822... Val Loss: 1.5883\n",
            "Epoch: 2/50... Step: 820... Loss: 1.3489... Val Loss: 1.5809\n",
            "Epoch: 2/50... Step: 830... Loss: 1.3994... Val Loss: 1.5840\n",
            "Epoch: 2/50... Step: 840... Loss: 1.3452... Val Loss: 1.5828\n",
            "Epoch: 2/50... Step: 850... Loss: 1.4094... Val Loss: 1.5781\n",
            "Epoch: 2/50... Step: 860... Loss: 1.5086... Val Loss: 1.5898\n",
            "Epoch: 2/50... Step: 870... Loss: 1.3564... Val Loss: 1.5810\n",
            "Epoch: 2/50... Step: 880... Loss: 1.3624... Val Loss: 1.5921\n",
            "Epoch: 2/50... Step: 890... Loss: 1.3764... Val Loss: 1.5790\n",
            "Epoch: 2/50... Step: 900... Loss: 1.4249... Val Loss: 1.5892\n",
            "Epoch: 2/50... Step: 910... Loss: 1.3220... Val Loss: 1.5800\n",
            "Epoch: 2/50... Step: 920... Loss: 1.3789... Val Loss: 1.5813\n",
            "Epoch: 2/50... Step: 930... Loss: 1.3221... Val Loss: 1.5834\n",
            "Epoch: 2/50... Step: 940... Loss: 1.3875... Val Loss: 1.5824\n",
            "Epoch: 2/50... Step: 950... Loss: 1.3877... Val Loss: 1.5826\n",
            "Epoch: 2/50... Step: 960... Loss: 1.4460... Val Loss: 1.5826\n",
            "Epoch: 2/50... Step: 970... Loss: 1.3506... Val Loss: 1.5780\n",
            "Epoch: 2/50... Step: 980... Loss: 1.3760... Val Loss: 1.5773\n",
            "Epoch: 2/50... Step: 990... Loss: 1.3595... Val Loss: 1.5755\n",
            "Epoch: 2/50... Step: 1000... Loss: 1.3922... Val Loss: 1.5847\n",
            "Epoch: 2/50... Step: 1010... Loss: 1.4496... Val Loss: 1.5787\n",
            "Epoch: 2/50... Step: 1020... Loss: 1.3810... Val Loss: 1.5839\n",
            "Epoch: 2/50... Step: 1030... Loss: 1.3377... Val Loss: 1.5876\n",
            "Epoch: 2/50... Step: 1040... Loss: 1.8778... Val Loss: 1.5816\n",
            "Epoch: 3/50... Step: 1050... Loss: 1.3787... Val Loss: 1.5875\n",
            "Epoch: 3/50... Step: 1060... Loss: 1.3877... Val Loss: 1.5869\n",
            "Epoch: 3/50... Step: 1070... Loss: 1.3354... Val Loss: 1.5881\n",
            "Epoch: 3/50... Step: 1080... Loss: 1.3715... Val Loss: 1.5907\n",
            "Epoch: 3/50... Step: 1090... Loss: 1.3356... Val Loss: 1.5840\n",
            "Epoch: 3/50... Step: 1100... Loss: 1.3491... Val Loss: 1.5866\n",
            "Epoch: 3/50... Step: 1110... Loss: 1.3755... Val Loss: 1.5865\n",
            "Epoch: 3/50... Step: 1120... Loss: 1.2461... Val Loss: 1.5822\n",
            "Epoch: 3/50... Step: 1130... Loss: 1.3887... Val Loss: 1.5856\n",
            "Epoch: 3/50... Step: 1140... Loss: 1.3328... Val Loss: 1.5844\n",
            "Epoch: 3/50... Step: 1150... Loss: 1.4181... Val Loss: 1.5810\n",
            "Epoch: 3/50... Step: 1160... Loss: 1.3648... Val Loss: 1.5808\n",
            "Epoch: 3/50... Step: 1170... Loss: 1.4288... Val Loss: 1.5822\n",
            "Epoch: 3/50... Step: 1180... Loss: 1.3748... Val Loss: 1.5858\n",
            "Epoch: 3/50... Step: 1190... Loss: 1.3675... Val Loss: 1.5789\n",
            "Epoch: 3/50... Step: 1200... Loss: 1.4105... Val Loss: 1.5841\n",
            "Epoch: 3/50... Step: 1210... Loss: 1.4436... Val Loss: 1.5770\n",
            "Epoch: 3/50... Step: 1220... Loss: 1.3743... Val Loss: 1.5734\n",
            "Epoch: 3/50... Step: 1230... Loss: 1.3513... Val Loss: 1.5792\n",
            "Epoch: 3/50... Step: 1240... Loss: 1.3146... Val Loss: 1.5761\n",
            "Epoch: 3/50... Step: 1250... Loss: 1.4183... Val Loss: 1.5790\n",
            "Epoch: 3/50... Step: 1260... Loss: 1.3546... Val Loss: 1.5749\n",
            "Epoch: 3/50... Step: 1270... Loss: 1.3773... Val Loss: 1.5779\n",
            "Epoch: 3/50... Step: 1280... Loss: 1.4745... Val Loss: 1.5797\n",
            "Epoch: 3/50... Step: 1290... Loss: 1.3615... Val Loss: 1.5770\n",
            "Epoch: 3/50... Step: 1300... Loss: 1.3687... Val Loss: 1.5827\n",
            "Epoch: 3/50... Step: 1310... Loss: 1.4003... Val Loss: 1.5701\n",
            "Epoch: 3/50... Step: 1320... Loss: 1.3763... Val Loss: 1.5807\n",
            "Epoch: 3/50... Step: 1330... Loss: 1.3769... Val Loss: 1.5808\n",
            "Epoch: 3/50... Step: 1340... Loss: 1.3489... Val Loss: 1.5792\n",
            "Epoch: 3/50... Step: 1350... Loss: 1.3995... Val Loss: 1.5772\n",
            "Epoch: 3/50... Step: 1360... Loss: 1.3162... Val Loss: 1.5867\n",
            "Epoch: 3/50... Step: 1370... Loss: 1.4053... Val Loss: 1.5777\n",
            "Epoch: 3/50... Step: 1380... Loss: 1.4790... Val Loss: 1.5809\n",
            "Epoch: 3/50... Step: 1390... Loss: 1.3072... Val Loss: 1.5811\n",
            "Epoch: 3/50... Step: 1400... Loss: 1.3593... Val Loss: 1.5899\n",
            "Epoch: 3/50... Step: 1410... Loss: 1.3926... Val Loss: 1.5837\n",
            "Epoch: 3/50... Step: 1420... Loss: 1.3905... Val Loss: 1.5846\n",
            "Epoch: 3/50... Step: 1430... Loss: 1.3207... Val Loss: 1.5834\n",
            "Epoch: 3/50... Step: 1440... Loss: 1.3871... Val Loss: 1.5819\n",
            "Epoch: 3/50... Step: 1450... Loss: 1.3157... Val Loss: 1.5851\n",
            "Epoch: 3/50... Step: 1460... Loss: 1.3998... Val Loss: 1.5789\n",
            "Epoch: 3/50... Step: 1470... Loss: 1.3609... Val Loss: 1.5806\n",
            "Epoch: 3/50... Step: 1480... Loss: 1.4410... Val Loss: 1.5814\n",
            "Epoch: 3/50... Step: 1490... Loss: 1.3574... Val Loss: 1.5777\n",
            "Epoch: 3/50... Step: 1500... Loss: 1.3591... Val Loss: 1.5828\n",
            "Epoch: 3/50... Step: 1510... Loss: 1.3071... Val Loss: 1.5766\n",
            "Epoch: 3/50... Step: 1520... Loss: 1.3813... Val Loss: 1.5876\n",
            "Epoch: 3/50... Step: 1530... Loss: 1.4380... Val Loss: 1.5746\n",
            "Epoch: 3/50... Step: 1540... Loss: 1.4082... Val Loss: 1.5880\n",
            "Epoch: 3/50... Step: 1550... Loss: 1.3570... Val Loss: 1.5768\n",
            "Epoch: 3/50... Step: 1560... Loss: 1.8558... Val Loss: 1.5817\n",
            "Epoch: 4/50... Step: 1570... Loss: 1.3485... Val Loss: 1.5722\n",
            "Epoch: 4/50... Step: 1580... Loss: 1.3561... Val Loss: 1.5919\n",
            "Epoch: 4/50... Step: 1590... Loss: 1.3358... Val Loss: 1.5844\n",
            "Epoch: 4/50... Step: 1600... Loss: 1.3412... Val Loss: 1.6014\n",
            "Epoch: 4/50... Step: 1610... Loss: 1.3408... Val Loss: 1.5812\n",
            "Epoch: 4/50... Step: 1620... Loss: 1.3961... Val Loss: 1.5923\n",
            "Epoch: 4/50... Step: 1630... Loss: 1.3463... Val Loss: 1.5814\n",
            "Epoch: 4/50... Step: 1640... Loss: 1.2437... Val Loss: 1.5834\n",
            "Epoch: 4/50... Step: 1650... Loss: 1.3461... Val Loss: 1.5836\n",
            "Epoch: 4/50... Step: 1660... Loss: 1.3064... Val Loss: 1.5831\n",
            "Epoch: 4/50... Step: 1670... Loss: 1.3916... Val Loss: 1.5897\n",
            "Epoch: 4/50... Step: 1680... Loss: 1.3704... Val Loss: 1.5793\n",
            "Epoch: 4/50... Step: 1690... Loss: 1.3991... Val Loss: 1.5783\n",
            "Epoch: 4/50... Step: 1700... Loss: 1.3736... Val Loss: 1.5865\n",
            "Epoch: 4/50... Step: 1710... Loss: 1.3871... Val Loss: 1.5818\n",
            "Epoch: 4/50... Step: 1720... Loss: 1.3888... Val Loss: 1.5837\n",
            "Epoch: 4/50... Step: 1730... Loss: 1.4050... Val Loss: 1.5842\n",
            "Epoch: 4/50... Step: 1740... Loss: 1.3637... Val Loss: 1.5774\n",
            "Epoch: 4/50... Step: 1750... Loss: 1.3288... Val Loss: 1.5809\n",
            "Epoch: 4/50... Step: 1760... Loss: 1.2912... Val Loss: 1.5755\n",
            "Epoch: 4/50... Step: 1770... Loss: 1.4174... Val Loss: 1.5819\n",
            "Epoch: 4/50... Step: 1780... Loss: 1.3111... Val Loss: 1.5760\n",
            "Epoch: 4/50... Step: 1790... Loss: 1.3977... Val Loss: 1.5798\n",
            "Epoch: 4/50... Step: 1800... Loss: 1.4546... Val Loss: 1.5742\n",
            "Epoch: 4/50... Step: 1810... Loss: 1.3372... Val Loss: 1.5771\n",
            "Epoch: 4/50... Step: 1820... Loss: 1.3671... Val Loss: 1.5857\n",
            "Epoch: 4/50... Step: 1830... Loss: 1.3411... Val Loss: 1.5747\n",
            "Epoch: 4/50... Step: 1840... Loss: 1.3461... Val Loss: 1.5770\n",
            "Epoch: 4/50... Step: 1850... Loss: 1.3914... Val Loss: 1.5799\n",
            "Epoch: 4/50... Step: 1860... Loss: 1.3215... Val Loss: 1.5831\n",
            "Epoch: 4/50... Step: 1870... Loss: 1.3954... Val Loss: 1.5754\n",
            "Epoch: 4/50... Step: 1880... Loss: 1.2986... Val Loss: 1.5753\n",
            "Epoch: 4/50... Step: 1890... Loss: 1.3709... Val Loss: 1.5779\n",
            "Epoch: 4/50... Step: 1900... Loss: 1.4965... Val Loss: 1.5788\n",
            "Epoch: 4/50... Step: 1910... Loss: 1.2791... Val Loss: 1.5816\n",
            "Epoch: 4/50... Step: 1920... Loss: 1.3334... Val Loss: 1.5808\n",
            "Epoch: 4/50... Step: 1930... Loss: 1.3697... Val Loss: 1.5819\n",
            "Epoch: 4/50... Step: 1940... Loss: 1.3953... Val Loss: 1.5801\n",
            "Epoch: 4/50... Step: 1950... Loss: 1.3134... Val Loss: 1.5801\n",
            "Epoch: 4/50... Step: 1960... Loss: 1.4117... Val Loss: 1.5803\n",
            "Epoch: 4/50... Step: 1970... Loss: 1.3192... Val Loss: 1.5791\n",
            "Epoch: 4/50... Step: 1980... Loss: 1.3672... Val Loss: 1.5806\n",
            "Epoch: 4/50... Step: 1990... Loss: 1.3408... Val Loss: 1.5742\n",
            "Epoch: 4/50... Step: 2000... Loss: 1.4186... Val Loss: 1.5827\n",
            "Epoch: 4/50... Step: 2010... Loss: 1.3546... Val Loss: 1.5829\n",
            "Epoch: 4/50... Step: 2020... Loss: 1.3773... Val Loss: 1.5752\n",
            "Epoch: 4/50... Step: 2030... Loss: 1.3056... Val Loss: 1.5730\n",
            "Epoch: 4/50... Step: 2040... Loss: 1.3475... Val Loss: 1.5808\n",
            "Epoch: 4/50... Step: 2050... Loss: 1.3812... Val Loss: 1.5693\n",
            "Epoch: 4/50... Step: 2060... Loss: 1.3701... Val Loss: 1.5727\n",
            "Epoch: 4/50... Step: 2070... Loss: 1.3210... Val Loss: 1.5787\n",
            "Epoch: 4/50... Step: 2080... Loss: 1.8935... Val Loss: 1.5773\n",
            "Epoch: 5/50... Step: 2090... Loss: 1.3385... Val Loss: 1.5724\n",
            "Epoch: 5/50... Step: 2100... Loss: 1.3787... Val Loss: 1.5841\n",
            "Epoch: 5/50... Step: 2110... Loss: 1.3412... Val Loss: 1.5805\n",
            "Epoch: 5/50... Step: 2120... Loss: 1.3436... Val Loss: 1.5873\n",
            "Epoch: 5/50... Step: 2130... Loss: 1.3204... Val Loss: 1.5831\n",
            "Epoch: 5/50... Step: 2140... Loss: 1.3376... Val Loss: 1.5849\n",
            "Epoch: 5/50... Step: 2150... Loss: 1.3568... Val Loss: 1.5862\n",
            "Epoch: 5/50... Step: 2160... Loss: 1.2725... Val Loss: 1.5765\n",
            "Epoch: 5/50... Step: 2170... Loss: 1.3475... Val Loss: 1.5872\n",
            "Epoch: 5/50... Step: 2180... Loss: 1.2952... Val Loss: 1.5802\n",
            "Epoch: 5/50... Step: 2190... Loss: 1.3879... Val Loss: 1.5783\n",
            "Epoch: 5/50... Step: 2200... Loss: 1.3600... Val Loss: 1.5747\n",
            "Epoch: 5/50... Step: 2210... Loss: 1.4225... Val Loss: 1.5801\n",
            "Epoch: 5/50... Step: 2220... Loss: 1.3482... Val Loss: 1.5851\n",
            "Epoch: 5/50... Step: 2230... Loss: 1.3559... Val Loss: 1.5820\n",
            "Epoch: 5/50... Step: 2240... Loss: 1.3716... Val Loss: 1.5844\n",
            "Epoch: 5/50... Step: 2250... Loss: 1.4122... Val Loss: 1.5797\n",
            "Epoch: 5/50... Step: 2260... Loss: 1.3521... Val Loss: 1.5770\n",
            "Epoch: 5/50... Step: 2270... Loss: 1.3405... Val Loss: 1.5730\n",
            "Epoch: 5/50... Step: 2280... Loss: 1.2844... Val Loss: 1.5824\n",
            "Epoch: 5/50... Step: 2290... Loss: 1.3643... Val Loss: 1.5831\n",
            "Epoch: 5/50... Step: 2300... Loss: 1.3347... Val Loss: 1.5780\n",
            "Epoch: 5/50... Step: 2310... Loss: 1.3379... Val Loss: 1.5772\n",
            "Epoch: 5/50... Step: 2320... Loss: 1.4225... Val Loss: 1.5733\n",
            "Epoch: 5/50... Step: 2330... Loss: 1.3148... Val Loss: 1.5761\n",
            "Epoch: 5/50... Step: 2340... Loss: 1.3750... Val Loss: 1.5763\n",
            "Epoch: 5/50... Step: 2350... Loss: 1.3648... Val Loss: 1.5690\n",
            "Epoch: 5/50... Step: 2360... Loss: 1.3522... Val Loss: 1.5752\n",
            "Epoch: 5/50... Step: 2370... Loss: 1.3211... Val Loss: 1.5752\n",
            "Epoch: 5/50... Step: 2380... Loss: 1.3259... Val Loss: 1.5837\n",
            "Epoch: 5/50... Step: 2390... Loss: 1.3658... Val Loss: 1.5745\n",
            "Epoch: 5/50... Step: 2400... Loss: 1.2838... Val Loss: 1.5775\n",
            "Epoch: 5/50... Step: 2410... Loss: 1.3839... Val Loss: 1.5801\n",
            "Epoch: 5/50... Step: 2420... Loss: 1.4738... Val Loss: 1.5748\n",
            "Epoch: 5/50... Step: 2430... Loss: 1.2709... Val Loss: 1.5828\n",
            "Epoch: 5/50... Step: 2440... Loss: 1.3374... Val Loss: 1.5780\n",
            "Epoch: 5/50... Step: 2450... Loss: 1.3599... Val Loss: 1.5795\n",
            "Epoch: 5/50... Step: 2460... Loss: 1.4209... Val Loss: 1.5759\n",
            "Epoch: 5/50... Step: 2470... Loss: 1.2690... Val Loss: 1.5734\n",
            "Epoch: 5/50... Step: 2480... Loss: 1.3701... Val Loss: 1.5745\n",
            "Epoch: 5/50... Step: 2490... Loss: 1.3066... Val Loss: 1.5754\n",
            "Epoch: 5/50... Step: 2500... Loss: 1.3680... Val Loss: 1.5761\n",
            "Epoch: 5/50... Step: 2510... Loss: 1.3496... Val Loss: 1.5775\n",
            "Epoch: 5/50... Step: 2520... Loss: 1.4076... Val Loss: 1.5832\n",
            "Epoch: 5/50... Step: 2530... Loss: 1.3220... Val Loss: 1.5748\n",
            "Epoch: 5/50... Step: 2540... Loss: 1.3339... Val Loss: 1.5752\n",
            "Epoch: 5/50... Step: 2550... Loss: 1.2996... Val Loss: 1.5721\n",
            "Epoch: 5/50... Step: 2560... Loss: 1.3480... Val Loss: 1.5801\n",
            "Epoch: 5/50... Step: 2570... Loss: 1.3886... Val Loss: 1.5745\n",
            "Epoch: 5/50... Step: 2580... Loss: 1.3726... Val Loss: 1.5721\n",
            "Epoch: 5/50... Step: 2590... Loss: 1.2966... Val Loss: 1.5793\n",
            "Epoch: 5/50... Step: 2600... Loss: 1.8539... Val Loss: 1.5742\n",
            "Epoch: 6/50... Step: 2610... Loss: 1.3358... Val Loss: 1.5757\n",
            "Epoch: 6/50... Step: 2620... Loss: 1.3412... Val Loss: 1.5827\n",
            "Epoch: 6/50... Step: 2630... Loss: 1.3051... Val Loss: 1.5807\n",
            "Epoch: 6/50... Step: 2640... Loss: 1.3419... Val Loss: 1.5908\n",
            "Epoch: 6/50... Step: 2650... Loss: 1.2846... Val Loss: 1.5822\n",
            "Epoch: 6/50... Step: 2660... Loss: 1.3520... Val Loss: 1.5932\n",
            "Epoch: 6/50... Step: 2670... Loss: 1.3306... Val Loss: 1.5781\n",
            "Epoch: 6/50... Step: 2680... Loss: 1.2399... Val Loss: 1.5804\n",
            "Epoch: 6/50... Step: 2690... Loss: 1.3197... Val Loss: 1.5804\n",
            "Epoch: 6/50... Step: 2700... Loss: 1.2790... Val Loss: 1.5809\n",
            "Epoch: 6/50... Step: 2710... Loss: 1.3851... Val Loss: 1.5785\n",
            "Epoch: 6/50... Step: 2720... Loss: 1.3348... Val Loss: 1.5787\n",
            "Epoch: 6/50... Step: 2730... Loss: 1.3943... Val Loss: 1.5785\n",
            "Epoch: 6/50... Step: 2740... Loss: 1.3569... Val Loss: 1.5782\n",
            "Epoch: 6/50... Step: 2750... Loss: 1.3730... Val Loss: 1.5809\n",
            "Epoch: 6/50... Step: 2760... Loss: 1.3430... Val Loss: 1.5782\n",
            "Epoch: 6/50... Step: 2770... Loss: 1.4013... Val Loss: 1.5770\n",
            "Epoch: 6/50... Step: 2780... Loss: 1.3334... Val Loss: 1.5728\n",
            "Epoch: 6/50... Step: 2790... Loss: 1.3350... Val Loss: 1.5732\n",
            "Epoch: 6/50... Step: 2800... Loss: 1.2710... Val Loss: 1.5704\n",
            "Epoch: 6/50... Step: 2810... Loss: 1.3733... Val Loss: 1.5750\n",
            "Epoch: 6/50... Step: 2820... Loss: 1.2808... Val Loss: 1.5753\n",
            "Epoch: 6/50... Step: 2830... Loss: 1.3694... Val Loss: 1.5743\n",
            "Epoch: 6/50... Step: 2840... Loss: 1.4136... Val Loss: 1.5756\n",
            "Epoch: 6/50... Step: 2850... Loss: 1.3292... Val Loss: 1.5724\n",
            "Epoch: 6/50... Step: 2860... Loss: 1.3603... Val Loss: 1.5854\n",
            "Epoch: 6/50... Step: 2870... Loss: 1.3371... Val Loss: 1.5683\n",
            "Epoch: 6/50... Step: 2880... Loss: 1.3336... Val Loss: 1.5799\n",
            "Epoch: 6/50... Step: 2890... Loss: 1.3759... Val Loss: 1.5778\n",
            "Epoch: 6/50... Step: 2900... Loss: 1.3035... Val Loss: 1.5860\n",
            "Epoch: 6/50... Step: 2910... Loss: 1.3592... Val Loss: 1.5817\n",
            "Epoch: 6/50... Step: 2920... Loss: 1.3010... Val Loss: 1.5782\n",
            "Epoch: 6/50... Step: 2930... Loss: 1.3554... Val Loss: 1.5812\n",
            "Epoch: 6/50... Step: 2940... Loss: 1.4713... Val Loss: 1.5794\n",
            "Epoch: 6/50... Step: 2950... Loss: 1.2930... Val Loss: 1.5853\n",
            "Epoch: 6/50... Step: 2960... Loss: 1.3199... Val Loss: 1.5863\n",
            "Epoch: 6/50... Step: 2970... Loss: 1.3490... Val Loss: 1.5761\n",
            "Epoch: 6/50... Step: 2980... Loss: 1.3785... Val Loss: 1.5759\n",
            "Epoch: 6/50... Step: 2990... Loss: 1.2690... Val Loss: 1.5801\n",
            "Epoch: 6/50... Step: 3000... Loss: 1.3545... Val Loss: 1.5829\n",
            "Epoch: 6/50... Step: 3010... Loss: 1.2997... Val Loss: 1.5794\n",
            "Epoch: 6/50... Step: 3020... Loss: 1.3475... Val Loss: 1.5790\n",
            "Epoch: 6/50... Step: 3030... Loss: 1.3091... Val Loss: 1.5801\n",
            "Epoch: 6/50... Step: 3040... Loss: 1.3832... Val Loss: 1.5804\n",
            "Epoch: 6/50... Step: 3050... Loss: 1.3284... Val Loss: 1.5823\n",
            "Epoch: 6/50... Step: 3060... Loss: 1.3419... Val Loss: 1.5749\n",
            "Epoch: 6/50... Step: 3070... Loss: 1.2874... Val Loss: 1.5742\n",
            "Epoch: 6/50... Step: 3080... Loss: 1.3124... Val Loss: 1.5764\n",
            "Epoch: 6/50... Step: 3090... Loss: 1.3880... Val Loss: 1.5750\n",
            "Epoch: 6/50... Step: 3100... Loss: 1.3925... Val Loss: 1.5781\n",
            "Epoch: 6/50... Step: 3110... Loss: 1.3119... Val Loss: 1.5790\n",
            "Epoch: 6/50... Step: 3120... Loss: 1.8100... Val Loss: 1.5774\n",
            "Epoch: 7/50... Step: 3130... Loss: 1.3282... Val Loss: 1.5752\n",
            "Epoch: 7/50... Step: 3140... Loss: 1.3117... Val Loss: 1.5866\n",
            "Epoch: 7/50... Step: 3150... Loss: 1.3057... Val Loss: 1.5875\n",
            "Epoch: 7/50... Step: 3160... Loss: 1.3048... Val Loss: 1.5886\n",
            "Epoch: 7/50... Step: 3170... Loss: 1.3094... Val Loss: 1.5818\n",
            "Epoch: 7/50... Step: 3180... Loss: 1.3425... Val Loss: 1.5832\n",
            "Epoch: 7/50... Step: 3190... Loss: 1.3320... Val Loss: 1.5818\n",
            "Epoch: 7/50... Step: 3200... Loss: 1.2367... Val Loss: 1.5722\n",
            "Epoch: 7/50... Step: 3210... Loss: 1.3201... Val Loss: 1.5841\n",
            "Epoch: 7/50... Step: 3220... Loss: 1.2770... Val Loss: 1.5835\n",
            "Epoch: 7/50... Step: 3230... Loss: 1.3570... Val Loss: 1.5843\n",
            "Epoch: 7/50... Step: 3240... Loss: 1.3598... Val Loss: 1.5808\n",
            "Epoch: 7/50... Step: 3250... Loss: 1.3854... Val Loss: 1.5765\n",
            "Epoch: 7/50... Step: 3260... Loss: 1.3157... Val Loss: 1.5814\n",
            "Epoch: 7/50... Step: 3270... Loss: 1.3491... Val Loss: 1.5731\n",
            "Epoch: 7/50... Step: 3280... Loss: 1.3488... Val Loss: 1.5790\n",
            "Epoch: 7/50... Step: 3290... Loss: 1.3878... Val Loss: 1.5769\n",
            "Epoch: 7/50... Step: 3300... Loss: 1.3022... Val Loss: 1.5811\n",
            "Epoch: 7/50... Step: 3310... Loss: 1.3289... Val Loss: 1.5739\n",
            "Epoch: 7/50... Step: 3320... Loss: 1.2700... Val Loss: 1.5735\n",
            "Epoch: 7/50... Step: 3330... Loss: 1.3881... Val Loss: 1.5745\n",
            "Epoch: 7/50... Step: 3340... Loss: 1.2820... Val Loss: 1.5772\n",
            "Epoch: 7/50... Step: 3350... Loss: 1.3262... Val Loss: 1.5755\n",
            "Epoch: 7/50... Step: 3360... Loss: 1.3959... Val Loss: 1.5739\n",
            "Epoch: 7/50... Step: 3370... Loss: 1.3369... Val Loss: 1.5757\n",
            "Epoch: 7/50... Step: 3380... Loss: 1.3387... Val Loss: 1.5782\n",
            "Epoch: 7/50... Step: 3390... Loss: 1.3218... Val Loss: 1.5666\n",
            "Epoch: 7/50... Step: 3400... Loss: 1.3350... Val Loss: 1.5778\n",
            "Epoch: 7/50... Step: 3410... Loss: 1.3325... Val Loss: 1.5766\n",
            "Epoch: 7/50... Step: 3420... Loss: 1.2762... Val Loss: 1.5826\n",
            "Epoch: 7/50... Step: 3430... Loss: 1.3712... Val Loss: 1.5795\n",
            "Epoch: 7/50... Step: 3440... Loss: 1.3073... Val Loss: 1.5757\n",
            "Epoch: 7/50... Step: 3450... Loss: 1.3458... Val Loss: 1.5768\n",
            "Epoch: 7/50... Step: 3460... Loss: 1.4703... Val Loss: 1.5784\n",
            "Epoch: 7/50... Step: 3470... Loss: 1.2573... Val Loss: 1.5807\n",
            "Epoch: 7/50... Step: 3480... Loss: 1.2859... Val Loss: 1.5750\n",
            "Epoch: 7/50... Step: 3490... Loss: 1.3463... Val Loss: 1.5790\n",
            "Epoch: 7/50... Step: 3500... Loss: 1.3708... Val Loss: 1.5711\n",
            "Epoch: 7/50... Step: 3510... Loss: 1.2594... Val Loss: 1.5780\n",
            "Epoch: 7/50... Step: 3520... Loss: 1.3639... Val Loss: 1.5746\n",
            "Epoch: 7/50... Step: 3530... Loss: 1.2510... Val Loss: 1.5793\n",
            "Epoch: 7/50... Step: 3540... Loss: 1.3496... Val Loss: 1.5807\n",
            "Epoch: 7/50... Step: 3550... Loss: 1.3270... Val Loss: 1.5738\n",
            "Epoch: 7/50... Step: 3560... Loss: 1.4226... Val Loss: 1.5765\n",
            "Epoch: 7/50... Step: 3570... Loss: 1.3109... Val Loss: 1.5791\n",
            "Epoch: 7/50... Step: 3580... Loss: 1.3245... Val Loss: 1.5756\n",
            "Epoch: 7/50... Step: 3590... Loss: 1.2933... Val Loss: 1.5724\n",
            "Epoch: 7/50... Step: 3600... Loss: 1.3343... Val Loss: 1.5774\n",
            "Epoch: 7/50... Step: 3610... Loss: 1.3461... Val Loss: 1.5775\n",
            "Epoch: 7/50... Step: 3620... Loss: 1.3508... Val Loss: 1.5733\n",
            "Epoch: 7/50... Step: 3630... Loss: 1.2918... Val Loss: 1.5818\n",
            "Epoch: 7/50... Step: 3640... Loss: 1.8360... Val Loss: 1.5829\n",
            "Epoch: 8/50... Step: 3650... Loss: 1.3067... Val Loss: 1.5745\n",
            "Epoch: 8/50... Step: 3660... Loss: 1.2997... Val Loss: 1.5883\n",
            "Epoch: 8/50... Step: 3670... Loss: 1.3136... Val Loss: 1.5842\n",
            "Epoch: 8/50... Step: 3680... Loss: 1.2854... Val Loss: 1.5804\n",
            "Epoch: 8/50... Step: 3690... Loss: 1.2921... Val Loss: 1.5792\n",
            "Epoch: 8/50... Step: 3700... Loss: 1.3370... Val Loss: 1.5840\n",
            "Epoch: 8/50... Step: 3710... Loss: 1.2998... Val Loss: 1.5780\n",
            "Epoch: 8/50... Step: 3720... Loss: 1.2356... Val Loss: 1.5748\n",
            "Epoch: 8/50... Step: 3730... Loss: 1.2751... Val Loss: 1.5780\n",
            "Epoch: 8/50... Step: 3740... Loss: 1.2542... Val Loss: 1.5758\n",
            "Epoch: 8/50... Step: 3750... Loss: 1.3537... Val Loss: 1.5745\n",
            "Epoch: 8/50... Step: 3760... Loss: 1.3171... Val Loss: 1.5752\n",
            "Epoch: 8/50... Step: 3770... Loss: 1.3664... Val Loss: 1.5791\n",
            "Epoch: 8/50... Step: 3780... Loss: 1.3339... Val Loss: 1.5793\n",
            "Epoch: 8/50... Step: 3790... Loss: 1.3499... Val Loss: 1.5734\n",
            "Epoch: 8/50... Step: 3800... Loss: 1.3395... Val Loss: 1.5758\n",
            "Epoch: 8/50... Step: 3810... Loss: 1.3658... Val Loss: 1.5764\n",
            "Epoch: 8/50... Step: 3820... Loss: 1.3202... Val Loss: 1.5708\n",
            "Epoch: 8/50... Step: 3830... Loss: 1.3089... Val Loss: 1.5677\n",
            "Epoch: 8/50... Step: 3840... Loss: 1.2498... Val Loss: 1.5728\n",
            "Epoch: 8/50... Step: 3850... Loss: 1.3543... Val Loss: 1.5716\n",
            "Epoch: 8/50... Step: 3860... Loss: 1.2744... Val Loss: 1.5709\n",
            "Epoch: 8/50... Step: 3870... Loss: 1.3226... Val Loss: 1.5702\n",
            "Epoch: 8/50... Step: 3880... Loss: 1.3732... Val Loss: 1.5671\n",
            "Epoch: 8/50... Step: 3890... Loss: 1.2607... Val Loss: 1.5697\n",
            "Epoch: 8/50... Step: 3900... Loss: 1.3424... Val Loss: 1.5786\n",
            "Epoch: 8/50... Step: 3910... Loss: 1.3253... Val Loss: 1.5686\n",
            "Epoch: 8/50... Step: 3920... Loss: 1.3182... Val Loss: 1.5741\n",
            "Epoch: 8/50... Step: 3930... Loss: 1.3106... Val Loss: 1.5800\n",
            "Epoch: 8/50... Step: 3940... Loss: 1.2916... Val Loss: 1.5742\n",
            "Epoch: 8/50... Step: 3950... Loss: 1.3593... Val Loss: 1.5684\n",
            "Epoch: 8/50... Step: 3960... Loss: 1.2667... Val Loss: 1.5718\n",
            "Epoch: 8/50... Step: 3970... Loss: 1.3306... Val Loss: 1.5713\n",
            "Epoch: 8/50... Step: 3980... Loss: 1.4254... Val Loss: 1.5699\n",
            "Epoch: 8/50... Step: 3990... Loss: 1.2633... Val Loss: 1.5757\n",
            "Epoch: 8/50... Step: 4000... Loss: 1.3048... Val Loss: 1.5699\n",
            "Epoch: 8/50... Step: 4010... Loss: 1.3537... Val Loss: 1.5703\n",
            "Epoch: 8/50... Step: 4020... Loss: 1.3704... Val Loss: 1.5692\n",
            "Epoch: 8/50... Step: 4030... Loss: 1.2926... Val Loss: 1.5705\n",
            "Epoch: 8/50... Step: 4040... Loss: 1.3559... Val Loss: 1.5718\n",
            "Epoch: 8/50... Step: 4050... Loss: 1.2730... Val Loss: 1.5725\n",
            "Epoch: 8/50... Step: 4060... Loss: 1.3107... Val Loss: 1.5780\n",
            "Epoch: 8/50... Step: 4070... Loss: 1.3157... Val Loss: 1.5714\n",
            "Epoch: 8/50... Step: 4080... Loss: 1.3651... Val Loss: 1.5694\n",
            "Epoch: 8/50... Step: 4090... Loss: 1.3095... Val Loss: 1.5733\n",
            "Epoch: 8/50... Step: 4100... Loss: 1.3309... Val Loss: 1.5696\n",
            "Epoch: 8/50... Step: 4110... Loss: 1.2771... Val Loss: 1.5714\n",
            "Epoch: 8/50... Step: 4120... Loss: 1.3014... Val Loss: 1.5752\n",
            "Epoch: 8/50... Step: 4130... Loss: 1.3494... Val Loss: 1.5716\n",
            "Epoch: 8/50... Step: 4140... Loss: 1.3160... Val Loss: 1.5767\n",
            "Epoch: 8/50... Step: 4150... Loss: 1.2486... Val Loss: 1.5873\n",
            "Epoch: 8/50... Step: 4160... Loss: 1.7993... Val Loss: 1.5766\n",
            "Epoch: 9/50... Step: 4170... Loss: 1.3150... Val Loss: 1.5810\n",
            "Epoch: 9/50... Step: 4180... Loss: 1.3028... Val Loss: 1.5788\n",
            "Epoch: 9/50... Step: 4190... Loss: 1.2837... Val Loss: 1.5877\n",
            "Epoch: 9/50... Step: 4200... Loss: 1.2936... Val Loss: 1.5894\n",
            "Epoch: 9/50... Step: 4210... Loss: 1.2807... Val Loss: 1.5856\n",
            "Epoch: 9/50... Step: 4220... Loss: 1.3234... Val Loss: 1.5813\n",
            "Epoch: 9/50... Step: 4230... Loss: 1.2877... Val Loss: 1.5873\n",
            "Epoch: 9/50... Step: 4240... Loss: 1.2271... Val Loss: 1.5732\n",
            "Epoch: 9/50... Step: 4250... Loss: 1.2920... Val Loss: 1.5872\n",
            "Epoch: 9/50... Step: 4260... Loss: 1.2355... Val Loss: 1.5762\n",
            "Epoch: 9/50... Step: 4270... Loss: 1.3549... Val Loss: 1.5843\n",
            "Epoch: 9/50... Step: 4280... Loss: 1.3065... Val Loss: 1.5779\n",
            "Epoch: 9/50... Step: 4290... Loss: 1.3266... Val Loss: 1.5794\n",
            "Epoch: 9/50... Step: 4300... Loss: 1.3227... Val Loss: 1.5832\n",
            "Epoch: 9/50... Step: 4310... Loss: 1.3209... Val Loss: 1.5811\n",
            "Epoch: 9/50... Step: 4320... Loss: 1.3228... Val Loss: 1.5817\n",
            "Epoch: 9/50... Step: 4330... Loss: 1.3694... Val Loss: 1.5725\n",
            "Epoch: 9/50... Step: 4340... Loss: 1.3084... Val Loss: 1.5836\n",
            "Epoch: 9/50... Step: 4350... Loss: 1.2974... Val Loss: 1.5753\n",
            "Epoch: 9/50... Step: 4360... Loss: 1.2520... Val Loss: 1.5722\n",
            "Epoch: 9/50... Step: 4370... Loss: 1.3704... Val Loss: 1.5724\n",
            "Epoch: 9/50... Step: 4380... Loss: 1.2673... Val Loss: 1.5750\n",
            "Epoch: 9/50... Step: 4390... Loss: 1.3067... Val Loss: 1.5790\n",
            "Epoch: 9/50... Step: 4400... Loss: 1.3466... Val Loss: 1.5710\n",
            "Epoch: 9/50... Step: 4410... Loss: 1.2917... Val Loss: 1.5732\n",
            "Epoch: 9/50... Step: 4420... Loss: 1.3217... Val Loss: 1.5814\n",
            "Epoch: 9/50... Step: 4430... Loss: 1.3124... Val Loss: 1.5758\n",
            "Epoch: 9/50... Step: 4440... Loss: 1.3174... Val Loss: 1.5733\n",
            "Epoch: 9/50... Step: 4450... Loss: 1.3179... Val Loss: 1.5843\n",
            "Epoch: 9/50... Step: 4460... Loss: 1.2878... Val Loss: 1.5780\n",
            "Epoch: 9/50... Step: 4470... Loss: 1.3056... Val Loss: 1.5777\n",
            "Epoch: 9/50... Step: 4480... Loss: 1.2664... Val Loss: 1.5726\n",
            "Epoch: 9/50... Step: 4490... Loss: 1.3371... Val Loss: 1.5729\n",
            "Epoch: 9/50... Step: 4500... Loss: 1.4041... Val Loss: 1.5754\n",
            "Epoch: 9/50... Step: 4510... Loss: 1.2212... Val Loss: 1.5765\n",
            "Epoch: 9/50... Step: 4520... Loss: 1.2816... Val Loss: 1.5818\n",
            "Epoch: 9/50... Step: 4530... Loss: 1.3678... Val Loss: 1.5807\n",
            "Epoch: 9/50... Step: 4540... Loss: 1.3637... Val Loss: 1.5689\n",
            "Epoch: 9/50... Step: 4550... Loss: 1.2337... Val Loss: 1.5766\n",
            "Epoch: 9/50... Step: 4560... Loss: 1.3250... Val Loss: 1.5739\n",
            "Epoch: 9/50... Step: 4570... Loss: 1.2859... Val Loss: 1.5741\n",
            "Epoch: 9/50... Step: 4580... Loss: 1.3671... Val Loss: 1.5740\n",
            "Epoch: 9/50... Step: 4590... Loss: 1.2893... Val Loss: 1.5735\n",
            "Epoch: 9/50... Step: 4600... Loss: 1.3466... Val Loss: 1.5723\n",
            "Epoch: 9/50... Step: 4610... Loss: 1.3112... Val Loss: 1.5782\n",
            "Epoch: 9/50... Step: 4620... Loss: 1.3222... Val Loss: 1.5774\n",
            "Epoch: 9/50... Step: 4630... Loss: 1.2748... Val Loss: 1.5715\n",
            "Epoch: 9/50... Step: 4640... Loss: 1.2923... Val Loss: 1.5768\n",
            "Epoch: 9/50... Step: 4650... Loss: 1.3325... Val Loss: 1.5787\n",
            "Epoch: 9/50... Step: 4660... Loss: 1.3300... Val Loss: 1.5704\n",
            "Epoch: 9/50... Step: 4670... Loss: 1.2729... Val Loss: 1.5848\n",
            "Epoch: 9/50... Step: 4680... Loss: 1.8124... Val Loss: 1.5798\n",
            "Epoch: 10/50... Step: 4690... Loss: 1.2943... Val Loss: 1.5761\n",
            "Epoch: 10/50... Step: 4700... Loss: 1.3048... Val Loss: 1.5832\n",
            "Epoch: 10/50... Step: 4710... Loss: 1.2609... Val Loss: 1.5835\n",
            "Epoch: 10/50... Step: 4720... Loss: 1.2846... Val Loss: 1.5854\n",
            "Epoch: 10/50... Step: 4730... Loss: 1.2656... Val Loss: 1.5818\n",
            "Epoch: 10/50... Step: 4740... Loss: 1.3071... Val Loss: 1.5746\n",
            "Epoch: 10/50... Step: 4750... Loss: 1.3021... Val Loss: 1.5826\n",
            "Epoch: 10/50... Step: 4760... Loss: 1.2345... Val Loss: 1.5738\n",
            "Epoch: 10/50... Step: 4770... Loss: 1.3016... Val Loss: 1.5757\n",
            "Epoch: 10/50... Step: 4780... Loss: 1.2130... Val Loss: 1.5772\n",
            "Epoch: 10/50... Step: 4790... Loss: 1.3301... Val Loss: 1.5834\n",
            "Epoch: 10/50... Step: 4800... Loss: 1.3064... Val Loss: 1.5793\n",
            "Epoch: 10/50... Step: 4810... Loss: 1.3453... Val Loss: 1.5769\n",
            "Epoch: 10/50... Step: 4820... Loss: 1.3254... Val Loss: 1.5736\n",
            "Epoch: 10/50... Step: 4830... Loss: 1.3591... Val Loss: 1.5751\n",
            "Epoch: 10/50... Step: 4840... Loss: 1.3393... Val Loss: 1.5755\n",
            "Epoch: 10/50... Step: 4850... Loss: 1.3693... Val Loss: 1.5711\n",
            "Epoch: 10/50... Step: 4860... Loss: 1.2659... Val Loss: 1.5733\n",
            "Epoch: 10/50... Step: 4870... Loss: 1.3071... Val Loss: 1.5733\n",
            "Epoch: 10/50... Step: 4880... Loss: 1.2951... Val Loss: 1.5710\n",
            "Epoch: 10/50... Step: 4890... Loss: 1.3360... Val Loss: 1.5764\n",
            "Epoch: 10/50... Step: 4900... Loss: 1.2717... Val Loss: 1.5745\n",
            "Epoch: 10/50... Step: 4910... Loss: 1.3313... Val Loss: 1.5824\n",
            "Epoch: 10/50... Step: 4920... Loss: 1.3570... Val Loss: 1.5735\n",
            "Epoch: 10/50... Step: 4930... Loss: 1.2534... Val Loss: 1.5743\n",
            "Epoch: 10/50... Step: 4940... Loss: 1.2883... Val Loss: 1.5827\n",
            "Epoch: 10/50... Step: 4950... Loss: 1.3136... Val Loss: 1.5712\n",
            "Epoch: 10/50... Step: 4960... Loss: 1.3294... Val Loss: 1.5764\n",
            "Epoch: 10/50... Step: 4970... Loss: 1.3073... Val Loss: 1.5798\n",
            "Epoch: 10/50... Step: 4980... Loss: 1.2561... Val Loss: 1.5756\n",
            "Epoch: 10/50... Step: 4990... Loss: 1.3285... Val Loss: 1.5801\n",
            "Epoch: 10/50... Step: 5000... Loss: 1.2367... Val Loss: 1.5726\n",
            "Epoch: 10/50... Step: 5010... Loss: 1.3097... Val Loss: 1.5726\n",
            "Epoch: 10/50... Step: 5020... Loss: 1.4170... Val Loss: 1.5742\n",
            "Epoch: 10/50... Step: 5030... Loss: 1.2442... Val Loss: 1.5767\n",
            "Epoch: 10/50... Step: 5040... Loss: 1.2606... Val Loss: 1.5854\n",
            "Epoch: 10/50... Step: 5050... Loss: 1.3369... Val Loss: 1.5793\n",
            "Epoch: 10/50... Step: 5060... Loss: 1.3638... Val Loss: 1.5742\n",
            "Epoch: 10/50... Step: 5070... Loss: 1.2174... Val Loss: 1.5783\n",
            "Epoch: 10/50... Step: 5080... Loss: 1.3062... Val Loss: 1.5818\n",
            "Epoch: 10/50... Step: 5090... Loss: 1.2582... Val Loss: 1.5795\n",
            "Epoch: 10/50... Step: 5100... Loss: 1.3007... Val Loss: 1.5781\n",
            "Epoch: 10/50... Step: 5110... Loss: 1.2725... Val Loss: 1.5805\n",
            "Epoch: 10/50... Step: 5120... Loss: 1.3381... Val Loss: 1.5816\n",
            "Epoch: 10/50... Step: 5130... Loss: 1.2946... Val Loss: 1.5807\n",
            "Epoch: 10/50... Step: 5140... Loss: 1.3244... Val Loss: 1.5720\n",
            "Epoch: 10/50... Step: 5150... Loss: 1.2562... Val Loss: 1.5766\n",
            "Epoch: 10/50... Step: 5160... Loss: 1.2794... Val Loss: 1.5740\n",
            "Epoch: 10/50... Step: 5170... Loss: 1.3232... Val Loss: 1.5795\n",
            "Epoch: 10/50... Step: 5180... Loss: 1.2937... Val Loss: 1.5732\n",
            "Epoch: 10/50... Step: 5190... Loss: 1.2642... Val Loss: 1.5828\n",
            "Epoch: 10/50... Step: 5200... Loss: 1.7846... Val Loss: 1.5788\n",
            "Epoch: 11/50... Step: 5210... Loss: 1.2766... Val Loss: 1.5790\n",
            "Epoch: 11/50... Step: 5220... Loss: 1.2779... Val Loss: 1.5856\n",
            "Epoch: 11/50... Step: 5230... Loss: 1.2908... Val Loss: 1.5892\n",
            "Epoch: 11/50... Step: 5240... Loss: 1.2843... Val Loss: 1.5847\n",
            "Epoch: 11/50... Step: 5250... Loss: 1.2229... Val Loss: 1.5836\n",
            "Epoch: 11/50... Step: 5260... Loss: 1.2975... Val Loss: 1.5883\n",
            "Epoch: 11/50... Step: 5270... Loss: 1.2869... Val Loss: 1.5821\n",
            "Epoch: 11/50... Step: 5280... Loss: 1.2217... Val Loss: 1.5766\n",
            "Epoch: 11/50... Step: 5290... Loss: 1.3030... Val Loss: 1.5815\n",
            "Epoch: 11/50... Step: 5300... Loss: 1.2148... Val Loss: 1.5862\n",
            "Epoch: 11/50... Step: 5310... Loss: 1.3325... Val Loss: 1.5829\n",
            "Epoch: 11/50... Step: 5320... Loss: 1.3038... Val Loss: 1.5863\n",
            "Epoch: 11/50... Step: 5330... Loss: 1.3237... Val Loss: 1.5818\n",
            "Epoch: 11/50... Step: 5340... Loss: 1.3039... Val Loss: 1.5864\n",
            "Epoch: 11/50... Step: 5350... Loss: 1.2965... Val Loss: 1.5791\n",
            "Epoch: 11/50... Step: 5360... Loss: 1.3221... Val Loss: 1.5848\n",
            "Epoch: 11/50... Step: 5370... Loss: 1.3561... Val Loss: 1.5823\n",
            "Epoch: 11/50... Step: 5380... Loss: 1.2666... Val Loss: 1.5761\n",
            "Epoch: 11/50... Step: 5390... Loss: 1.2760... Val Loss: 1.5782\n",
            "Epoch: 11/50... Step: 5400... Loss: 1.2020... Val Loss: 1.5789\n",
            "Epoch: 11/50... Step: 5410... Loss: 1.3301... Val Loss: 1.5834\n",
            "Epoch: 11/50... Step: 5420... Loss: 1.2388... Val Loss: 1.5814\n",
            "Epoch: 11/50... Step: 5430... Loss: 1.2838... Val Loss: 1.5837\n",
            "Epoch: 11/50... Step: 5440... Loss: 1.3413... Val Loss: 1.5783\n",
            "Epoch: 11/50... Step: 5450... Loss: 1.2747... Val Loss: 1.5777\n",
            "Epoch: 11/50... Step: 5460... Loss: 1.2699... Val Loss: 1.5819\n",
            "Epoch: 11/50... Step: 5470... Loss: 1.3041... Val Loss: 1.5763\n",
            "Epoch: 11/50... Step: 5480... Loss: 1.2980... Val Loss: 1.5790\n",
            "Epoch: 11/50... Step: 5490... Loss: 1.3026... Val Loss: 1.5861\n",
            "Epoch: 11/50... Step: 5500... Loss: 1.2549... Val Loss: 1.5839\n",
            "Epoch: 11/50... Step: 5510... Loss: 1.2892... Val Loss: 1.5854\n",
            "Epoch: 11/50... Step: 5520... Loss: 1.2480... Val Loss: 1.5775\n",
            "Epoch: 11/50... Step: 5530... Loss: 1.3415... Val Loss: 1.5776\n",
            "Epoch: 11/50... Step: 5540... Loss: 1.4190... Val Loss: 1.5812\n",
            "Epoch: 11/50... Step: 5550... Loss: 1.2303... Val Loss: 1.5789\n",
            "Epoch: 11/50... Step: 5560... Loss: 1.2439... Val Loss: 1.5856\n",
            "Epoch: 11/50... Step: 5570... Loss: 1.2989... Val Loss: 1.5845\n",
            "Epoch: 11/50... Step: 5580... Loss: 1.3393... Val Loss: 1.5721\n",
            "Epoch: 11/50... Step: 5590... Loss: 1.2162... Val Loss: 1.5823\n",
            "Epoch: 11/50... Step: 5600... Loss: 1.3363... Val Loss: 1.5795\n",
            "Epoch: 11/50... Step: 5610... Loss: 1.2368... Val Loss: 1.5807\n",
            "Epoch: 11/50... Step: 5620... Loss: 1.2751... Val Loss: 1.5772\n",
            "Epoch: 11/50... Step: 5630... Loss: 1.2829... Val Loss: 1.5788\n",
            "Epoch: 11/50... Step: 5640... Loss: 1.3405... Val Loss: 1.5819\n",
            "Epoch: 11/50... Step: 5650... Loss: 1.2748... Val Loss: 1.5843\n",
            "Epoch: 11/50... Step: 5660... Loss: 1.2613... Val Loss: 1.5776\n",
            "Epoch: 11/50... Step: 5670... Loss: 1.2244... Val Loss: 1.5755\n",
            "Epoch: 11/50... Step: 5680... Loss: 1.2945... Val Loss: 1.5758\n",
            "Epoch: 11/50... Step: 5690... Loss: 1.3152... Val Loss: 1.5797\n",
            "Epoch: 11/50... Step: 5700... Loss: 1.3095... Val Loss: 1.5768\n",
            "Epoch: 11/50... Step: 5710... Loss: 1.2818... Val Loss: 1.5811\n",
            "Epoch: 11/50... Step: 5720... Loss: 1.7695... Val Loss: 1.5805\n",
            "Epoch: 12/50... Step: 5730... Loss: 1.2595... Val Loss: 1.5765\n",
            "Epoch: 12/50... Step: 5740... Loss: 1.2885... Val Loss: 1.5870\n",
            "Epoch: 12/50... Step: 5750... Loss: 1.2982... Val Loss: 1.5890\n",
            "Epoch: 12/50... Step: 5760... Loss: 1.2571... Val Loss: 1.5905\n",
            "Epoch: 12/50... Step: 5770... Loss: 1.2675... Val Loss: 1.5890\n",
            "Epoch: 12/50... Step: 5780... Loss: 1.2918... Val Loss: 1.5854\n",
            "Epoch: 12/50... Step: 5790... Loss: 1.2901... Val Loss: 1.5857\n",
            "Epoch: 12/50... Step: 5800... Loss: 1.1996... Val Loss: 1.5805\n",
            "Epoch: 12/50... Step: 5810... Loss: 1.2870... Val Loss: 1.5817\n",
            "Epoch: 12/50... Step: 5820... Loss: 1.2431... Val Loss: 1.5843\n",
            "Epoch: 12/50... Step: 5830... Loss: 1.3123... Val Loss: 1.5800\n",
            "Epoch: 12/50... Step: 5840... Loss: 1.2662... Val Loss: 1.5821\n",
            "Epoch: 12/50... Step: 5850... Loss: 1.3468... Val Loss: 1.5859\n",
            "Epoch: 12/50... Step: 5860... Loss: 1.2571... Val Loss: 1.5837\n",
            "Epoch: 12/50... Step: 5870... Loss: 1.2967... Val Loss: 1.5792\n",
            "Epoch: 12/50... Step: 5880... Loss: 1.2906... Val Loss: 1.5839\n",
            "Epoch: 12/50... Step: 5890... Loss: 1.3295... Val Loss: 1.5785\n",
            "Epoch: 12/50... Step: 5900... Loss: 1.2798... Val Loss: 1.5808\n",
            "Epoch: 12/50... Step: 5910... Loss: 1.2832... Val Loss: 1.5766\n",
            "Epoch: 12/50... Step: 5920... Loss: 1.2141... Val Loss: 1.5724\n",
            "Epoch: 12/50... Step: 5930... Loss: 1.3221... Val Loss: 1.5781\n",
            "Epoch: 12/50... Step: 5940... Loss: 1.2407... Val Loss: 1.5779\n",
            "Epoch: 12/50... Step: 5950... Loss: 1.2899... Val Loss: 1.5805\n",
            "Epoch: 12/50... Step: 5960... Loss: 1.3593... Val Loss: 1.5749\n",
            "Epoch: 12/50... Step: 5970... Loss: 1.2974... Val Loss: 1.5737\n",
            "Epoch: 12/50... Step: 5980... Loss: 1.3075... Val Loss: 1.5907\n",
            "Epoch: 12/50... Step: 5990... Loss: 1.2837... Val Loss: 1.5731\n",
            "Epoch: 12/50... Step: 6000... Loss: 1.3289... Val Loss: 1.5723\n",
            "Epoch: 12/50... Step: 6010... Loss: 1.2817... Val Loss: 1.5798\n",
            "Epoch: 12/50... Step: 6020... Loss: 1.2446... Val Loss: 1.5792\n",
            "Epoch: 12/50... Step: 6030... Loss: 1.2870... Val Loss: 1.5755\n",
            "Epoch: 12/50... Step: 6040... Loss: 1.2383... Val Loss: 1.5778\n",
            "Epoch: 12/50... Step: 6050... Loss: 1.2941... Val Loss: 1.5764\n",
            "Epoch: 12/50... Step: 6060... Loss: 1.3848... Val Loss: 1.5774\n",
            "Epoch: 12/50... Step: 6070... Loss: 1.2316... Val Loss: 1.5808\n",
            "Epoch: 12/50... Step: 6080... Loss: 1.2608... Val Loss: 1.5810\n",
            "Epoch: 12/50... Step: 6090... Loss: 1.2930... Val Loss: 1.5789\n",
            "Epoch: 12/50... Step: 6100... Loss: 1.3363... Val Loss: 1.5759\n",
            "Epoch: 12/50... Step: 6110... Loss: 1.2310... Val Loss: 1.5854\n",
            "Epoch: 12/50... Step: 6120... Loss: 1.3156... Val Loss: 1.5825\n",
            "Epoch: 12/50... Step: 6130... Loss: 1.2572... Val Loss: 1.5815\n",
            "Epoch: 12/50... Step: 6140... Loss: 1.3074... Val Loss: 1.5794\n",
            "Epoch: 12/50... Step: 6150... Loss: 1.2834... Val Loss: 1.5829\n",
            "Epoch: 12/50... Step: 6160... Loss: 1.2958... Val Loss: 1.5850\n",
            "Epoch: 12/50... Step: 6170... Loss: 1.2539... Val Loss: 1.5839\n",
            "Epoch: 12/50... Step: 6180... Loss: 1.2722... Val Loss: 1.5767\n",
            "Epoch: 12/50... Step: 6190... Loss: 1.2463... Val Loss: 1.5816\n",
            "Epoch: 12/50... Step: 6200... Loss: 1.2877... Val Loss: 1.5806\n",
            "Epoch: 12/50... Step: 6210... Loss: 1.3366... Val Loss: 1.5809\n",
            "Epoch: 12/50... Step: 6220... Loss: 1.3162... Val Loss: 1.5776\n",
            "Epoch: 12/50... Step: 6230... Loss: 1.2516... Val Loss: 1.5868\n",
            "Epoch: 12/50... Step: 6240... Loss: 1.7645... Val Loss: 1.5769\n",
            "Epoch: 13/50... Step: 6250... Loss: 1.2915... Val Loss: 1.5749\n",
            "Epoch: 13/50... Step: 6260... Loss: 1.2863... Val Loss: 1.5819\n",
            "Epoch: 13/50... Step: 6270... Loss: 1.2779... Val Loss: 1.5925\n",
            "Epoch: 13/50... Step: 6280... Loss: 1.2334... Val Loss: 1.5859\n",
            "Epoch: 13/50... Step: 6290... Loss: 1.2619... Val Loss: 1.5849\n",
            "Epoch: 13/50... Step: 6300... Loss: 1.3039... Val Loss: 1.5852\n",
            "Epoch: 13/50... Step: 6310... Loss: 1.2487... Val Loss: 1.5834\n",
            "Epoch: 13/50... Step: 6320... Loss: 1.2027... Val Loss: 1.5815\n",
            "Epoch: 13/50... Step: 6330... Loss: 1.2777... Val Loss: 1.5813\n",
            "Epoch: 13/50... Step: 6340... Loss: 1.2007... Val Loss: 1.5833\n",
            "Epoch: 13/50... Step: 6350... Loss: 1.3212... Val Loss: 1.5865\n",
            "Epoch: 13/50... Step: 6360... Loss: 1.2785... Val Loss: 1.5834\n",
            "Epoch: 13/50... Step: 6370... Loss: 1.3126... Val Loss: 1.5836\n",
            "Epoch: 13/50... Step: 6380... Loss: 1.2666... Val Loss: 1.5876\n",
            "Epoch: 13/50... Step: 6390... Loss: 1.3063... Val Loss: 1.5845\n",
            "Epoch: 13/50... Step: 6400... Loss: 1.3173... Val Loss: 1.5876\n",
            "Epoch: 13/50... Step: 6410... Loss: 1.3530... Val Loss: 1.5849\n",
            "Epoch: 13/50... Step: 6420... Loss: 1.2691... Val Loss: 1.5801\n",
            "Epoch: 13/50... Step: 6430... Loss: 1.2735... Val Loss: 1.5817\n",
            "Epoch: 13/50... Step: 6440... Loss: 1.2200... Val Loss: 1.5758\n",
            "Epoch: 13/50... Step: 6450... Loss: 1.3353... Val Loss: 1.5852\n",
            "Epoch: 13/50... Step: 6460... Loss: 1.2459... Val Loss: 1.5810\n",
            "Epoch: 13/50... Step: 6470... Loss: 1.2563... Val Loss: 1.5828\n",
            "Epoch: 13/50... Step: 6480... Loss: 1.3177... Val Loss: 1.5815\n",
            "Epoch: 13/50... Step: 6490... Loss: 1.2490... Val Loss: 1.5735\n",
            "Epoch: 13/50... Step: 6500... Loss: 1.2709... Val Loss: 1.5866\n",
            "Epoch: 13/50... Step: 6510... Loss: 1.2829... Val Loss: 1.5824\n",
            "Epoch: 13/50... Step: 6520... Loss: 1.2680... Val Loss: 1.5762\n",
            "Epoch: 13/50... Step: 6530... Loss: 1.2956... Val Loss: 1.5922\n",
            "Epoch: 13/50... Step: 6540... Loss: 1.2488... Val Loss: 1.5921\n",
            "Epoch: 13/50... Step: 6550... Loss: 1.3061... Val Loss: 1.5854\n",
            "Epoch: 13/50... Step: 6560... Loss: 1.2270... Val Loss: 1.5829\n",
            "Epoch: 13/50... Step: 6570... Loss: 1.3028... Val Loss: 1.5789\n",
            "Epoch: 13/50... Step: 6580... Loss: 1.3844... Val Loss: 1.5792\n",
            "Epoch: 13/50... Step: 6590... Loss: 1.2213... Val Loss: 1.5828\n",
            "Epoch: 13/50... Step: 6600... Loss: 1.2322... Val Loss: 1.5783\n",
            "Epoch: 13/50... Step: 6610... Loss: 1.2991... Val Loss: 1.5856\n",
            "Epoch: 13/50... Step: 6620... Loss: 1.3063... Val Loss: 1.5762\n",
            "Epoch: 13/50... Step: 6630... Loss: 1.2255... Val Loss: 1.5820\n",
            "Epoch: 13/50... Step: 6640... Loss: 1.2824... Val Loss: 1.5804\n",
            "Epoch: 13/50... Step: 6650... Loss: 1.2670... Val Loss: 1.5860\n",
            "Epoch: 13/50... Step: 6660... Loss: 1.2695... Val Loss: 1.5814\n",
            "Epoch: 13/50... Step: 6670... Loss: 1.2786... Val Loss: 1.5771\n",
            "Epoch: 13/50... Step: 6680... Loss: 1.3630... Val Loss: 1.5869\n",
            "Epoch: 13/50... Step: 6690... Loss: 1.2568... Val Loss: 1.5822\n",
            "Epoch: 13/50... Step: 6700... Loss: 1.2994... Val Loss: 1.5797\n",
            "Epoch: 13/50... Step: 6710... Loss: 1.2531... Val Loss: 1.5733\n",
            "Epoch: 13/50... Step: 6720... Loss: 1.2729... Val Loss: 1.5781\n",
            "Epoch: 13/50... Step: 6730... Loss: 1.3108... Val Loss: 1.5789\n",
            "Epoch: 13/50... Step: 6740... Loss: 1.3052... Val Loss: 1.5732\n",
            "Epoch: 13/50... Step: 6750... Loss: 1.2276... Val Loss: 1.5849\n",
            "Epoch: 13/50... Step: 6760... Loss: 1.7409... Val Loss: 1.5779\n",
            "Epoch: 14/50... Step: 6770... Loss: 1.2671... Val Loss: 1.5744\n",
            "Epoch: 14/50... Step: 6780... Loss: 1.2584... Val Loss: 1.5908\n",
            "Epoch: 14/50... Step: 6790... Loss: 1.2471... Val Loss: 1.5926\n",
            "Epoch: 14/50... Step: 6800... Loss: 1.2592... Val Loss: 1.5922\n",
            "Epoch: 14/50... Step: 6810... Loss: 1.2504... Val Loss: 1.5938\n",
            "Epoch: 14/50... Step: 6820... Loss: 1.2958... Val Loss: 1.5875\n",
            "Epoch: 14/50... Step: 6830... Loss: 1.2733... Val Loss: 1.5900\n",
            "Epoch: 14/50... Step: 6840... Loss: 1.1701... Val Loss: 1.5837\n",
            "Epoch: 14/50... Step: 6850... Loss: 1.2605... Val Loss: 1.5859\n",
            "Epoch: 14/50... Step: 6860... Loss: 1.2262... Val Loss: 1.5865\n",
            "Epoch: 14/50... Step: 6870... Loss: 1.3233... Val Loss: 1.5868\n",
            "Epoch: 14/50... Step: 6880... Loss: 1.2526... Val Loss: 1.5832\n",
            "Epoch: 14/50... Step: 6890... Loss: 1.3294... Val Loss: 1.5828\n",
            "Epoch: 14/50... Step: 6900... Loss: 1.2609... Val Loss: 1.5893\n",
            "Epoch: 14/50... Step: 6910... Loss: 1.2785... Val Loss: 1.5860\n",
            "Epoch: 14/50... Step: 6920... Loss: 1.2946... Val Loss: 1.5901\n",
            "Epoch: 14/50... Step: 6930... Loss: 1.3250... Val Loss: 1.5836\n",
            "Epoch: 14/50... Step: 6940... Loss: 1.2833... Val Loss: 1.5804\n",
            "Epoch: 14/50... Step: 6950... Loss: 1.2573... Val Loss: 1.5840\n",
            "Epoch: 14/50... Step: 6960... Loss: 1.2074... Val Loss: 1.5778\n",
            "Epoch: 14/50... Step: 6970... Loss: 1.3107... Val Loss: 1.5843\n",
            "Epoch: 14/50... Step: 6980... Loss: 1.2464... Val Loss: 1.5854\n",
            "Epoch: 14/50... Step: 6990... Loss: 1.2520... Val Loss: 1.5863\n",
            "Epoch: 14/50... Step: 7000... Loss: 1.3496... Val Loss: 1.5771\n",
            "Epoch: 14/50... Step: 7010... Loss: 1.2361... Val Loss: 1.5785\n",
            "Epoch: 14/50... Step: 7020... Loss: 1.2707... Val Loss: 1.5907\n",
            "Epoch: 14/50... Step: 7030... Loss: 1.2748... Val Loss: 1.5815\n",
            "Epoch: 14/50... Step: 7040... Loss: 1.2603... Val Loss: 1.5836\n",
            "Epoch: 14/50... Step: 7050... Loss: 1.2253... Val Loss: 1.5890\n",
            "Epoch: 14/50... Step: 7060... Loss: 1.2411... Val Loss: 1.5931\n",
            "Epoch: 14/50... Step: 7070... Loss: 1.3162... Val Loss: 1.5874\n",
            "Epoch: 14/50... Step: 7080... Loss: 1.2237... Val Loss: 1.5861\n",
            "Epoch: 14/50... Step: 7090... Loss: 1.2947... Val Loss: 1.5874\n",
            "Epoch: 14/50... Step: 7100... Loss: 1.3831... Val Loss: 1.5823\n",
            "Epoch: 14/50... Step: 7110... Loss: 1.2056... Val Loss: 1.5879\n",
            "Epoch: 14/50... Step: 7120... Loss: 1.2199... Val Loss: 1.5908\n",
            "Epoch: 14/50... Step: 7130... Loss: 1.3005... Val Loss: 1.5939\n",
            "Epoch: 14/50... Step: 7140... Loss: 1.3172... Val Loss: 1.5804\n",
            "Epoch: 14/50... Step: 7150... Loss: 1.2137... Val Loss: 1.5850\n",
            "Epoch: 14/50... Step: 7160... Loss: 1.2666... Val Loss: 1.5874\n",
            "Epoch: 14/50... Step: 7170... Loss: 1.2334... Val Loss: 1.5873\n",
            "Epoch: 14/50... Step: 7180... Loss: 1.2852... Val Loss: 1.5875\n",
            "Epoch: 14/50... Step: 7190... Loss: 1.2694... Val Loss: 1.5842\n",
            "Epoch: 14/50... Step: 7200... Loss: 1.3082... Val Loss: 1.5911\n",
            "Epoch: 14/50... Step: 7210... Loss: 1.2435... Val Loss: 1.5864\n",
            "Epoch: 14/50... Step: 7220... Loss: 1.2575... Val Loss: 1.5851\n",
            "Epoch: 14/50... Step: 7230... Loss: 1.2642... Val Loss: 1.5814\n",
            "Epoch: 14/50... Step: 7240... Loss: 1.2610... Val Loss: 1.5826\n",
            "Epoch: 14/50... Step: 7250... Loss: 1.2987... Val Loss: 1.5853\n",
            "Epoch: 14/50... Step: 7260... Loss: 1.2787... Val Loss: 1.5845\n",
            "Epoch: 14/50... Step: 7270... Loss: 1.2122... Val Loss: 1.5939\n",
            "Epoch: 14/50... Step: 7280... Loss: 1.7936... Val Loss: 1.5898\n",
            "Epoch: 15/50... Step: 7290... Loss: 1.2453... Val Loss: 1.5785\n",
            "Epoch: 15/50... Step: 7300... Loss: 1.2553... Val Loss: 1.5934\n",
            "Epoch: 15/50... Step: 7310... Loss: 1.2455... Val Loss: 1.5982\n",
            "Epoch: 15/50... Step: 7320... Loss: 1.2607... Val Loss: 1.5945\n",
            "Epoch: 15/50... Step: 7330... Loss: 1.2445... Val Loss: 1.5979\n",
            "Epoch: 15/50... Step: 7340... Loss: 1.2687... Val Loss: 1.6042\n",
            "Epoch: 15/50... Step: 7350... Loss: 1.2633... Val Loss: 1.5949\n",
            "Epoch: 15/50... Step: 7360... Loss: 1.1974... Val Loss: 1.5941\n",
            "Epoch: 15/50... Step: 7370... Loss: 1.2743... Val Loss: 1.5937\n",
            "Epoch: 15/50... Step: 7380... Loss: 1.2066... Val Loss: 1.5901\n",
            "Epoch: 15/50... Step: 7390... Loss: 1.2956... Val Loss: 1.5890\n",
            "Epoch: 15/50... Step: 7400... Loss: 1.2737... Val Loss: 1.5974\n",
            "Epoch: 15/50... Step: 7410... Loss: 1.3178... Val Loss: 1.5912\n",
            "Epoch: 15/50... Step: 7420... Loss: 1.2716... Val Loss: 1.5958\n",
            "Epoch: 15/50... Step: 7430... Loss: 1.2408... Val Loss: 1.5932\n",
            "Epoch: 15/50... Step: 7440... Loss: 1.2814... Val Loss: 1.5899\n",
            "Epoch: 15/50... Step: 7450... Loss: 1.3203... Val Loss: 1.5862\n",
            "Epoch: 15/50... Step: 7460... Loss: 1.2812... Val Loss: 1.5930\n",
            "Epoch: 15/50... Step: 7470... Loss: 1.2322... Val Loss: 1.5864\n",
            "Epoch: 15/50... Step: 7480... Loss: 1.1818... Val Loss: 1.5841\n",
            "Epoch: 15/50... Step: 7490... Loss: 1.3294... Val Loss: 1.5829\n",
            "Epoch: 15/50... Step: 7500... Loss: 1.2049... Val Loss: 1.5871\n",
            "Epoch: 15/50... Step: 7510... Loss: 1.2712... Val Loss: 1.5893\n",
            "Epoch: 15/50... Step: 7520... Loss: 1.3165... Val Loss: 1.5867\n",
            "Epoch: 15/50... Step: 7530... Loss: 1.2574... Val Loss: 1.5805\n",
            "Epoch: 15/50... Step: 7540... Loss: 1.2514... Val Loss: 1.5914\n",
            "Epoch: 15/50... Step: 7550... Loss: 1.2787... Val Loss: 1.5839\n",
            "Epoch: 15/50... Step: 7560... Loss: 1.2454... Val Loss: 1.5853\n",
            "Epoch: 15/50... Step: 7570... Loss: 1.2467... Val Loss: 1.5901\n",
            "Epoch: 15/50... Step: 7580... Loss: 1.2281... Val Loss: 1.5886\n",
            "Epoch: 15/50... Step: 7590... Loss: 1.2639... Val Loss: 1.5926\n",
            "Epoch: 15/50... Step: 7600... Loss: 1.2259... Val Loss: 1.5870\n",
            "Epoch: 15/50... Step: 7610... Loss: 1.3035... Val Loss: 1.5880\n",
            "Epoch: 15/50... Step: 7620... Loss: 1.3716... Val Loss: 1.5910\n",
            "Epoch: 15/50... Step: 7630... Loss: 1.1891... Val Loss: 1.5913\n",
            "Epoch: 15/50... Step: 7640... Loss: 1.2329... Val Loss: 1.5972\n",
            "Epoch: 15/50... Step: 7650... Loss: 1.2834... Val Loss: 1.5889\n",
            "Epoch: 15/50... Step: 7660... Loss: 1.2667... Val Loss: 1.5860\n",
            "Epoch: 15/50... Step: 7670... Loss: 1.1546... Val Loss: 1.5917\n",
            "Epoch: 15/50... Step: 7680... Loss: 1.3029... Val Loss: 1.5922\n",
            "Epoch: 15/50... Step: 7690... Loss: 1.2269... Val Loss: 1.5917\n",
            "Epoch: 15/50... Step: 7700... Loss: 1.2651... Val Loss: 1.5956\n",
            "Epoch: 15/50... Step: 7710... Loss: 1.2772... Val Loss: 1.5876\n",
            "Epoch: 15/50... Step: 7720... Loss: 1.3046... Val Loss: 1.5861\n",
            "Epoch: 15/50... Step: 7730... Loss: 1.2307... Val Loss: 1.5945\n",
            "Epoch: 15/50... Step: 7740... Loss: 1.2702... Val Loss: 1.5876\n",
            "Epoch: 15/50... Step: 7750... Loss: 1.2252... Val Loss: 1.5836\n",
            "Epoch: 15/50... Step: 7760... Loss: 1.2630... Val Loss: 1.5900\n",
            "Epoch: 15/50... Step: 7770... Loss: 1.2888... Val Loss: 1.5888\n",
            "Epoch: 15/50... Step: 7780... Loss: 1.2758... Val Loss: 1.5855\n",
            "Epoch: 15/50... Step: 7790... Loss: 1.2382... Val Loss: 1.5940\n",
            "Epoch: 15/50... Step: 7800... Loss: 1.6923... Val Loss: 1.5896\n",
            "Epoch: 16/50... Step: 7810... Loss: 1.2312... Val Loss: 1.5828\n",
            "Epoch: 16/50... Step: 7820... Loss: 1.2734... Val Loss: 1.5903\n",
            "Epoch: 16/50... Step: 7830... Loss: 1.2381... Val Loss: 1.6078\n",
            "Epoch: 16/50... Step: 7840... Loss: 1.2219... Val Loss: 1.6013\n",
            "Epoch: 16/50... Step: 7850... Loss: 1.2277... Val Loss: 1.5994\n",
            "Epoch: 16/50... Step: 7860... Loss: 1.2713... Val Loss: 1.5966\n",
            "Epoch: 16/50... Step: 7870... Loss: 1.2694... Val Loss: 1.5949\n",
            "Epoch: 16/50... Step: 7880... Loss: 1.1503... Val Loss: 1.5950\n",
            "Epoch: 16/50... Step: 7890... Loss: 1.2432... Val Loss: 1.5951\n",
            "Epoch: 16/50... Step: 7900... Loss: 1.1844... Val Loss: 1.5910\n",
            "Epoch: 16/50... Step: 7910... Loss: 1.2532... Val Loss: 1.5918\n",
            "Epoch: 16/50... Step: 7920... Loss: 1.2774... Val Loss: 1.5904\n",
            "Epoch: 16/50... Step: 7930... Loss: 1.3187... Val Loss: 1.5911\n",
            "Epoch: 16/50... Step: 7940... Loss: 1.2739... Val Loss: 1.5937\n",
            "Epoch: 16/50... Step: 7950... Loss: 1.2766... Val Loss: 1.5946\n",
            "Epoch: 16/50... Step: 7960... Loss: 1.2973... Val Loss: 1.5913\n",
            "Epoch: 16/50... Step: 7970... Loss: 1.2903... Val Loss: 1.5950\n",
            "Epoch: 16/50... Step: 7980... Loss: 1.2476... Val Loss: 1.5943\n",
            "Epoch: 16/50... Step: 7990... Loss: 1.2402... Val Loss: 1.5909\n",
            "Epoch: 16/50... Step: 8000... Loss: 1.1927... Val Loss: 1.5886\n",
            "Epoch: 16/50... Step: 8010... Loss: 1.3242... Val Loss: 1.5940\n",
            "Epoch: 16/50... Step: 8020... Loss: 1.2094... Val Loss: 1.5926\n",
            "Epoch: 16/50... Step: 8030... Loss: 1.2579... Val Loss: 1.5964\n",
            "Epoch: 16/50... Step: 8040... Loss: 1.2805... Val Loss: 1.5855\n",
            "Epoch: 16/50... Step: 8050... Loss: 1.2265... Val Loss: 1.5887\n",
            "Epoch: 16/50... Step: 8060... Loss: 1.2521... Val Loss: 1.5953\n",
            "Epoch: 16/50... Step: 8070... Loss: 1.2921... Val Loss: 1.5886\n",
            "Epoch: 16/50... Step: 8080... Loss: 1.2550... Val Loss: 1.5837\n",
            "Epoch: 16/50... Step: 8090... Loss: 1.2752... Val Loss: 1.5903\n",
            "Epoch: 16/50... Step: 8100... Loss: 1.2312... Val Loss: 1.5911\n",
            "Epoch: 16/50... Step: 8110... Loss: 1.2893... Val Loss: 1.5903\n",
            "Epoch: 16/50... Step: 8120... Loss: 1.2116... Val Loss: 1.5904\n",
            "Epoch: 16/50... Step: 8130... Loss: 1.2876... Val Loss: 1.5847\n",
            "Epoch: 16/50... Step: 8140... Loss: 1.3727... Val Loss: 1.5888\n",
            "Epoch: 16/50... Step: 8150... Loss: 1.2041... Val Loss: 1.5927\n",
            "Epoch: 16/50... Step: 8160... Loss: 1.2058... Val Loss: 1.5944\n",
            "Epoch: 16/50... Step: 8170... Loss: 1.2915... Val Loss: 1.5962\n",
            "Epoch: 16/50... Step: 8180... Loss: 1.2607... Val Loss: 1.5884\n",
            "Epoch: 16/50... Step: 8190... Loss: 1.1963... Val Loss: 1.5932\n",
            "Epoch: 16/50... Step: 8200... Loss: 1.2911... Val Loss: 1.5959\n",
            "Epoch: 16/50... Step: 8210... Loss: 1.2132... Val Loss: 1.5964\n",
            "Epoch: 16/50... Step: 8220... Loss: 1.2708... Val Loss: 1.5882\n",
            "Epoch: 16/50... Step: 8230... Loss: 1.2433... Val Loss: 1.5842\n",
            "Epoch: 16/50... Step: 8240... Loss: 1.3206... Val Loss: 1.5946\n",
            "Epoch: 16/50... Step: 8250... Loss: 1.2250... Val Loss: 1.5927\n",
            "Epoch: 16/50... Step: 8260... Loss: 1.2822... Val Loss: 1.5838\n",
            "Epoch: 16/50... Step: 8270... Loss: 1.2070... Val Loss: 1.5824\n",
            "Epoch: 16/50... Step: 8280... Loss: 1.2207... Val Loss: 1.5847\n",
            "Epoch: 16/50... Step: 8290... Loss: 1.2615... Val Loss: 1.5899\n",
            "Epoch: 16/50... Step: 8300... Loss: 1.2709... Val Loss: 1.5872\n",
            "Epoch: 16/50... Step: 8310... Loss: 1.2325... Val Loss: 1.5930\n",
            "Epoch: 16/50... Step: 8320... Loss: 1.7136... Val Loss: 1.5942\n",
            "Epoch: 17/50... Step: 8330... Loss: 1.2589... Val Loss: 1.5859\n",
            "Epoch: 17/50... Step: 8340... Loss: 1.2377... Val Loss: 1.5942\n",
            "Epoch: 17/50... Step: 8350... Loss: 1.2250... Val Loss: 1.6037\n",
            "Epoch: 17/50... Step: 8360... Loss: 1.2320... Val Loss: 1.6010\n",
            "Epoch: 17/50... Step: 8370... Loss: 1.2222... Val Loss: 1.5970\n",
            "Epoch: 17/50... Step: 8380... Loss: 1.2589... Val Loss: 1.6017\n",
            "Epoch: 17/50... Step: 8390... Loss: 1.2599... Val Loss: 1.5978\n",
            "Epoch: 17/50... Step: 8400... Loss: 1.1470... Val Loss: 1.5877\n",
            "Epoch: 17/50... Step: 8410... Loss: 1.2135... Val Loss: 1.5967\n",
            "Epoch: 17/50... Step: 8420... Loss: 1.2187... Val Loss: 1.5948\n",
            "Epoch: 17/50... Step: 8430... Loss: 1.2906... Val Loss: 1.5965\n",
            "Epoch: 17/50... Step: 8440... Loss: 1.2674... Val Loss: 1.5985\n",
            "Epoch: 17/50... Step: 8450... Loss: 1.3254... Val Loss: 1.5974\n",
            "Epoch: 17/50... Step: 8460... Loss: 1.2479... Val Loss: 1.5982\n",
            "Epoch: 17/50... Step: 8470... Loss: 1.3075... Val Loss: 1.5932\n",
            "Epoch: 17/50... Step: 8480... Loss: 1.2578... Val Loss: 1.5957\n",
            "Epoch: 17/50... Step: 8490... Loss: 1.3397... Val Loss: 1.5967\n",
            "Epoch: 17/50... Step: 8500... Loss: 1.2539... Val Loss: 1.5909\n",
            "Epoch: 17/50... Step: 8510... Loss: 1.2545... Val Loss: 1.5936\n",
            "Epoch: 17/50... Step: 8520... Loss: 1.1755... Val Loss: 1.5872\n",
            "Epoch: 17/50... Step: 8530... Loss: 1.2964... Val Loss: 1.5927\n",
            "Epoch: 17/50... Step: 8540... Loss: 1.2146... Val Loss: 1.5909\n",
            "Epoch: 17/50... Step: 8550... Loss: 1.2369... Val Loss: 1.5935\n",
            "Epoch: 17/50... Step: 8560... Loss: 1.2954... Val Loss: 1.5969\n",
            "Epoch: 17/50... Step: 8570... Loss: 1.2360... Val Loss: 1.5814\n",
            "Epoch: 17/50... Step: 8580... Loss: 1.2453... Val Loss: 1.5911\n",
            "Epoch: 17/50... Step: 8590... Loss: 1.2421... Val Loss: 1.5869\n",
            "Epoch: 17/50... Step: 8600... Loss: 1.2416... Val Loss: 1.5848\n",
            "Epoch: 17/50... Step: 8610... Loss: 1.2533... Val Loss: 1.5960\n",
            "Epoch: 17/50... Step: 8620... Loss: 1.1805... Val Loss: 1.5930\n",
            "Epoch: 17/50... Step: 8630... Loss: 1.2519... Val Loss: 1.5940\n",
            "Epoch: 17/50... Step: 8640... Loss: 1.2214... Val Loss: 1.5886\n",
            "Epoch: 17/50... Step: 8650... Loss: 1.2721... Val Loss: 1.5899\n",
            "Epoch: 17/50... Step: 8660... Loss: 1.3444... Val Loss: 1.5897\n",
            "Epoch: 17/50... Step: 8670... Loss: 1.1973... Val Loss: 1.5915\n",
            "Epoch: 17/50... Step: 8680... Loss: 1.2087... Val Loss: 1.5990\n",
            "Epoch: 17/50... Step: 8690... Loss: 1.2728... Val Loss: 1.5962\n",
            "Epoch: 17/50... Step: 8700... Loss: 1.3136... Val Loss: 1.5902\n",
            "Epoch: 17/50... Step: 8710... Loss: 1.1869... Val Loss: 1.5922\n",
            "Epoch: 17/50... Step: 8720... Loss: 1.2673... Val Loss: 1.5936\n",
            "Epoch: 17/50... Step: 8730... Loss: 1.2501... Val Loss: 1.6007\n",
            "Epoch: 17/50... Step: 8740... Loss: 1.2670... Val Loss: 1.5934\n",
            "Epoch: 17/50... Step: 8750... Loss: 1.2378... Val Loss: 1.5917\n",
            "Epoch: 17/50... Step: 8760... Loss: 1.2906... Val Loss: 1.5991\n",
            "Epoch: 17/50... Step: 8770... Loss: 1.2572... Val Loss: 1.5972\n",
            "Epoch: 17/50... Step: 8780... Loss: 1.2253... Val Loss: 1.5894\n",
            "Epoch: 17/50... Step: 8790... Loss: 1.2031... Val Loss: 1.5890\n",
            "Epoch: 17/50... Step: 8800... Loss: 1.2428... Val Loss: 1.5839\n",
            "Epoch: 17/50... Step: 8810... Loss: 1.2961... Val Loss: 1.5875\n",
            "Epoch: 17/50... Step: 8820... Loss: 1.2471... Val Loss: 1.5876\n",
            "Epoch: 17/50... Step: 8830... Loss: 1.2208... Val Loss: 1.5960\n",
            "Epoch: 17/50... Step: 8840... Loss: 1.7555... Val Loss: 1.5913\n",
            "Epoch: 18/50... Step: 8850... Loss: 1.2378... Val Loss: 1.5930\n",
            "Epoch: 18/50... Step: 8860... Loss: 1.2362... Val Loss: 1.5960\n",
            "Epoch: 18/50... Step: 8870... Loss: 1.2523... Val Loss: 1.6049\n",
            "Epoch: 18/50... Step: 8880... Loss: 1.2449... Val Loss: 1.6007\n",
            "Epoch: 18/50... Step: 8890... Loss: 1.2006... Val Loss: 1.6015\n",
            "Epoch: 18/50... Step: 8900... Loss: 1.2988... Val Loss: 1.6018\n",
            "Epoch: 18/50... Step: 8910... Loss: 1.2594... Val Loss: 1.6001\n",
            "Epoch: 18/50... Step: 8920... Loss: 1.1625... Val Loss: 1.5891\n",
            "Epoch: 18/50... Step: 8930... Loss: 1.2208... Val Loss: 1.5993\n",
            "Epoch: 18/50... Step: 8940... Loss: 1.1900... Val Loss: 1.5954\n",
            "Epoch: 18/50... Step: 8950... Loss: 1.2704... Val Loss: 1.5910\n",
            "Epoch: 18/50... Step: 8960... Loss: 1.2338... Val Loss: 1.5953\n",
            "Epoch: 18/50... Step: 8970... Loss: 1.3002... Val Loss: 1.5956\n",
            "Epoch: 18/50... Step: 8980... Loss: 1.2639... Val Loss: 1.5987\n",
            "Epoch: 18/50... Step: 8990... Loss: 1.2306... Val Loss: 1.5969\n",
            "Epoch: 18/50... Step: 9000... Loss: 1.2589... Val Loss: 1.5967\n",
            "Epoch: 18/50... Step: 9010... Loss: 1.3094... Val Loss: 1.5975\n",
            "Epoch: 18/50... Step: 9020... Loss: 1.2386... Val Loss: 1.5904\n",
            "Epoch: 18/50... Step: 9030... Loss: 1.2406... Val Loss: 1.5939\n",
            "Epoch: 18/50... Step: 9040... Loss: 1.1670... Val Loss: 1.5954\n",
            "Epoch: 18/50... Step: 9050... Loss: 1.2957... Val Loss: 1.5935\n",
            "Epoch: 18/50... Step: 9060... Loss: 1.2034... Val Loss: 1.5957\n",
            "Epoch: 18/50... Step: 9070... Loss: 1.2743... Val Loss: 1.6015\n",
            "Epoch: 18/50... Step: 9080... Loss: 1.2854... Val Loss: 1.5958\n",
            "Epoch: 18/50... Step: 9090... Loss: 1.2216... Val Loss: 1.5888\n",
            "Epoch: 18/50... Step: 9100... Loss: 1.2819... Val Loss: 1.5980\n",
            "Epoch: 18/50... Step: 9110... Loss: 1.2680... Val Loss: 1.5935\n",
            "Epoch: 18/50... Step: 9120... Loss: 1.2777... Val Loss: 1.5880\n",
            "Epoch: 18/50... Step: 9130... Loss: 1.2596... Val Loss: 1.5937\n",
            "Epoch: 18/50... Step: 9140... Loss: 1.1872... Val Loss: 1.5968\n",
            "Epoch: 18/50... Step: 9150... Loss: 1.2628... Val Loss: 1.5907\n",
            "Epoch: 18/50... Step: 9160... Loss: 1.1899... Val Loss: 1.5923\n",
            "Epoch: 18/50... Step: 9170... Loss: 1.2489... Val Loss: 1.5911\n",
            "Epoch: 18/50... Step: 9180... Loss: 1.3294... Val Loss: 1.5865\n",
            "Epoch: 18/50... Step: 9190... Loss: 1.2076... Val Loss: 1.5954\n",
            "Epoch: 18/50... Step: 9200... Loss: 1.2007... Val Loss: 1.5970\n",
            "Epoch: 18/50... Step: 9210... Loss: 1.2736... Val Loss: 1.5938\n",
            "Epoch: 18/50... Step: 9220... Loss: 1.2960... Val Loss: 1.5872\n",
            "Epoch: 18/50... Step: 9230... Loss: 1.1762... Val Loss: 1.5958\n",
            "Epoch: 18/50... Step: 9240... Loss: 1.2710... Val Loss: 1.5914\n",
            "Epoch: 18/50... Step: 9250... Loss: 1.2057... Val Loss: 1.5901\n",
            "Epoch: 18/50... Step: 9260... Loss: 1.2884... Val Loss: 1.5977\n",
            "Epoch: 18/50... Step: 9270... Loss: 1.2649... Val Loss: 1.5910\n",
            "Epoch: 18/50... Step: 9280... Loss: 1.2737... Val Loss: 1.5959\n",
            "Epoch: 18/50... Step: 9290... Loss: 1.2482... Val Loss: 1.6014\n",
            "Epoch: 18/50... Step: 9300... Loss: 1.2322... Val Loss: 1.5887\n",
            "Epoch: 18/50... Step: 9310... Loss: 1.2111... Val Loss: 1.5905\n",
            "Epoch: 18/50... Step: 9320... Loss: 1.2416... Val Loss: 1.5837\n",
            "Epoch: 18/50... Step: 9330... Loss: 1.2602... Val Loss: 1.5914\n",
            "Epoch: 18/50... Step: 9340... Loss: 1.2549... Val Loss: 1.5883\n",
            "Epoch: 18/50... Step: 9350... Loss: 1.2023... Val Loss: 1.5991\n",
            "Epoch: 18/50... Step: 9360... Loss: 1.7305... Val Loss: 1.6012\n",
            "Epoch: 19/50... Step: 9370... Loss: 1.2321... Val Loss: 1.5892\n",
            "Epoch: 19/50... Step: 9380... Loss: 1.2422... Val Loss: 1.5968\n",
            "Epoch: 19/50... Step: 9390... Loss: 1.2196... Val Loss: 1.6080\n",
            "Epoch: 19/50... Step: 9400... Loss: 1.2382... Val Loss: 1.6078\n",
            "Epoch: 19/50... Step: 9410... Loss: 1.1986... Val Loss: 1.6036\n",
            "Epoch: 19/50... Step: 9420... Loss: 1.2753... Val Loss: 1.6027\n",
            "Epoch: 19/50... Step: 9430... Loss: 1.2553... Val Loss: 1.6037\n",
            "Epoch: 19/50... Step: 9440... Loss: 1.1557... Val Loss: 1.5971\n",
            "Epoch: 19/50... Step: 9450... Loss: 1.2018... Val Loss: 1.5996\n",
            "Epoch: 19/50... Step: 9460... Loss: 1.2035... Val Loss: 1.6009\n",
            "Epoch: 19/50... Step: 9470... Loss: 1.2875... Val Loss: 1.5922\n",
            "Epoch: 19/50... Step: 9480... Loss: 1.2434... Val Loss: 1.5959\n",
            "Epoch: 19/50... Step: 9490... Loss: 1.3047... Val Loss: 1.6001\n",
            "Epoch: 19/50... Step: 9500... Loss: 1.2359... Val Loss: 1.6057\n",
            "Epoch: 19/50... Step: 9510... Loss: 1.2613... Val Loss: 1.6026\n",
            "Epoch: 19/50... Step: 9520... Loss: 1.2526... Val Loss: 1.5937\n",
            "Epoch: 19/50... Step: 9530... Loss: 1.3322... Val Loss: 1.5940\n",
            "Epoch: 19/50... Step: 9540... Loss: 1.2499... Val Loss: 1.5977\n",
            "Epoch: 19/50... Step: 9550... Loss: 1.2153... Val Loss: 1.6051\n",
            "Epoch: 19/50... Step: 9560... Loss: 1.1994... Val Loss: 1.5962\n",
            "Epoch: 19/50... Step: 9570... Loss: 1.3089... Val Loss: 1.5960\n",
            "Epoch: 19/50... Step: 9580... Loss: 1.1879... Val Loss: 1.5998\n",
            "Epoch: 19/50... Step: 9590... Loss: 1.2483... Val Loss: 1.6034\n",
            "Epoch: 19/50... Step: 9600... Loss: 1.3144... Val Loss: 1.5986\n",
            "Epoch: 19/50... Step: 9610... Loss: 1.1998... Val Loss: 1.5921\n",
            "Epoch: 19/50... Step: 9620... Loss: 1.2537... Val Loss: 1.5922\n",
            "Epoch: 19/50... Step: 9630... Loss: 1.2326... Val Loss: 1.5962\n",
            "Epoch: 19/50... Step: 9640... Loss: 1.2520... Val Loss: 1.5910\n",
            "Epoch: 19/50... Step: 9650... Loss: 1.2371... Val Loss: 1.5944\n",
            "Epoch: 19/50... Step: 9660... Loss: 1.1877... Val Loss: 1.6033\n",
            "Epoch: 19/50... Step: 9670... Loss: 1.2557... Val Loss: 1.6039\n",
            "Epoch: 19/50... Step: 9680... Loss: 1.1903... Val Loss: 1.5996\n",
            "Epoch: 19/50... Step: 9690... Loss: 1.2639... Val Loss: 1.6004\n",
            "Epoch: 19/50... Step: 9700... Loss: 1.3432... Val Loss: 1.5968\n",
            "Epoch: 19/50... Step: 9710... Loss: 1.1757... Val Loss: 1.5927\n",
            "Epoch: 19/50... Step: 9720... Loss: 1.2053... Val Loss: 1.6003\n",
            "Epoch: 19/50... Step: 9730... Loss: 1.2741... Val Loss: 1.6055\n",
            "Epoch: 19/50... Step: 9740... Loss: 1.2598... Val Loss: 1.5891\n",
            "Epoch: 19/50... Step: 9750... Loss: 1.1791... Val Loss: 1.5974\n",
            "Epoch: 19/50... Step: 9760... Loss: 1.2694... Val Loss: 1.5994\n",
            "Epoch: 19/50... Step: 9770... Loss: 1.2015... Val Loss: 1.5974\n",
            "Epoch: 19/50... Step: 9780... Loss: 1.2569... Val Loss: 1.5989\n",
            "Epoch: 19/50... Step: 9790... Loss: 1.2545... Val Loss: 1.5938\n",
            "Epoch: 19/50... Step: 9800... Loss: 1.2858... Val Loss: 1.5977\n",
            "Epoch: 19/50... Step: 9810... Loss: 1.2373... Val Loss: 1.6023\n",
            "Epoch: 19/50... Step: 9820... Loss: 1.2484... Val Loss: 1.5925\n",
            "Epoch: 19/50... Step: 9830... Loss: 1.1984... Val Loss: 1.5918\n",
            "Epoch: 19/50... Step: 9840... Loss: 1.2499... Val Loss: 1.5966\n",
            "Epoch: 19/50... Step: 9850... Loss: 1.2792... Val Loss: 1.5901\n",
            "Epoch: 19/50... Step: 9860... Loss: 1.2467... Val Loss: 1.5961\n",
            "Epoch: 19/50... Step: 9870... Loss: 1.1941... Val Loss: 1.6043\n",
            "Epoch: 19/50... Step: 9880... Loss: 1.6487... Val Loss: 1.5990\n",
            "Epoch: 20/50... Step: 9890... Loss: 1.2405... Val Loss: 1.5916\n",
            "Epoch: 20/50... Step: 9900... Loss: 1.2004... Val Loss: 1.5970\n",
            "Epoch: 20/50... Step: 9910... Loss: 1.2350... Val Loss: 1.6077\n",
            "Epoch: 20/50... Step: 9920... Loss: 1.2191... Val Loss: 1.6083\n",
            "Epoch: 20/50... Step: 9930... Loss: 1.1911... Val Loss: 1.5996\n",
            "Epoch: 20/50... Step: 9940... Loss: 1.2578... Val Loss: 1.6034\n",
            "Epoch: 20/50... Step: 9950... Loss: 1.2399... Val Loss: 1.5988\n",
            "Epoch: 20/50... Step: 9960... Loss: 1.1465... Val Loss: 1.5931\n",
            "Epoch: 20/50... Step: 9970... Loss: 1.2428... Val Loss: 1.5972\n",
            "Epoch: 20/50... Step: 9980... Loss: 1.1975... Val Loss: 1.6009\n",
            "Epoch: 20/50... Step: 9990... Loss: 1.2859... Val Loss: 1.5943\n",
            "Epoch: 20/50... Step: 10000... Loss: 1.2248... Val Loss: 1.5951\n",
            "Epoch: 20/50... Step: 10010... Loss: 1.2866... Val Loss: 1.5927\n",
            "Epoch: 20/50... Step: 10020... Loss: 1.2343... Val Loss: 1.6037\n",
            "Epoch: 20/50... Step: 10030... Loss: 1.2781... Val Loss: 1.5992\n",
            "Epoch: 20/50... Step: 10040... Loss: 1.2681... Val Loss: 1.5911\n",
            "Epoch: 20/50... Step: 10050... Loss: 1.2923... Val Loss: 1.5932\n",
            "Epoch: 20/50... Step: 10060... Loss: 1.2474... Val Loss: 1.5953\n",
            "Epoch: 20/50... Step: 10070... Loss: 1.2223... Val Loss: 1.5939\n",
            "Epoch: 20/50... Step: 10080... Loss: 1.2024... Val Loss: 1.5917\n",
            "Epoch: 20/50... Step: 10090... Loss: 1.2850... Val Loss: 1.5980\n",
            "Epoch: 20/50... Step: 10100... Loss: 1.1922... Val Loss: 1.5951\n",
            "Epoch: 20/50... Step: 10110... Loss: 1.2659... Val Loss: 1.6011\n",
            "Epoch: 20/50... Step: 10120... Loss: 1.2830... Val Loss: 1.5967\n",
            "Epoch: 20/50... Step: 10130... Loss: 1.2022... Val Loss: 1.5935\n",
            "Epoch: 20/50... Step: 10140... Loss: 1.2220... Val Loss: 1.5980\n",
            "Epoch: 20/50... Step: 10150... Loss: 1.2603... Val Loss: 1.5958\n",
            "Epoch: 20/50... Step: 10160... Loss: 1.2383... Val Loss: 1.5978\n",
            "Epoch: 20/50... Step: 10170... Loss: 1.2646... Val Loss: 1.6015\n",
            "Epoch: 20/50... Step: 10180... Loss: 1.1829... Val Loss: 1.6006\n",
            "Epoch: 20/50... Step: 10190... Loss: 1.2455... Val Loss: 1.5959\n",
            "Epoch: 20/50... Step: 10200... Loss: 1.1854... Val Loss: 1.5981\n",
            "Epoch: 20/50... Step: 10210... Loss: 1.2383... Val Loss: 1.5975\n",
            "Epoch: 20/50... Step: 10220... Loss: 1.3268... Val Loss: 1.5959\n",
            "Epoch: 20/50... Step: 10230... Loss: 1.1840... Val Loss: 1.6013\n",
            "Epoch: 20/50... Step: 10240... Loss: 1.2003... Val Loss: 1.6073\n",
            "Epoch: 20/50... Step: 10250... Loss: 1.2748... Val Loss: 1.6005\n",
            "Epoch: 20/50... Step: 10260... Loss: 1.2466... Val Loss: 1.5944\n",
            "Epoch: 20/50... Step: 10270... Loss: 1.1687... Val Loss: 1.6016\n",
            "Epoch: 20/50... Step: 10280... Loss: 1.2473... Val Loss: 1.6031\n",
            "Epoch: 20/50... Step: 10290... Loss: 1.1919... Val Loss: 1.6080\n",
            "Epoch: 20/50... Step: 10300... Loss: 1.2234... Val Loss: 1.6096\n",
            "Epoch: 20/50... Step: 10310... Loss: 1.2301... Val Loss: 1.6011\n",
            "Epoch: 20/50... Step: 10320... Loss: 1.2744... Val Loss: 1.6007\n",
            "Epoch: 20/50... Step: 10330... Loss: 1.2080... Val Loss: 1.6064\n",
            "Epoch: 20/50... Step: 10340... Loss: 1.2501... Val Loss: 1.6003\n",
            "Epoch: 20/50... Step: 10350... Loss: 1.1868... Val Loss: 1.5908\n",
            "Epoch: 20/50... Step: 10360... Loss: 1.2020... Val Loss: 1.5953\n",
            "Epoch: 20/50... Step: 10370... Loss: 1.2382... Val Loss: 1.5973\n",
            "Epoch: 20/50... Step: 10380... Loss: 1.2656... Val Loss: 1.5919\n",
            "Epoch: 20/50... Step: 10390... Loss: 1.2042... Val Loss: 1.6020\n",
            "Epoch: 20/50... Step: 10400... Loss: 1.7195... Val Loss: 1.6044\n",
            "Epoch: 21/50... Step: 10410... Loss: 1.2182... Val Loss: 1.5965\n",
            "Epoch: 21/50... Step: 10420... Loss: 1.2055... Val Loss: 1.6000\n",
            "Epoch: 21/50... Step: 10430... Loss: 1.1947... Val Loss: 1.6170\n",
            "Epoch: 21/50... Step: 10440... Loss: 1.2169... Val Loss: 1.6163\n",
            "Epoch: 21/50... Step: 10450... Loss: 1.1888... Val Loss: 1.6054\n",
            "Epoch: 21/50... Step: 10460... Loss: 1.2504... Val Loss: 1.6122\n",
            "Epoch: 21/50... Step: 10470... Loss: 1.2528... Val Loss: 1.6091\n",
            "Epoch: 21/50... Step: 10480... Loss: 1.1406... Val Loss: 1.6049\n",
            "Epoch: 21/50... Step: 10490... Loss: 1.2262... Val Loss: 1.6076\n",
            "Epoch: 21/50... Step: 10500... Loss: 1.2090... Val Loss: 1.6044\n",
            "Epoch: 21/50... Step: 10510... Loss: 1.2324... Val Loss: 1.6011\n",
            "Epoch: 21/50... Step: 10520... Loss: 1.2390... Val Loss: 1.6010\n",
            "Epoch: 21/50... Step: 10530... Loss: 1.3036... Val Loss: 1.6023\n",
            "Epoch: 21/50... Step: 10540... Loss: 1.2316... Val Loss: 1.5992\n",
            "Epoch: 21/50... Step: 10550... Loss: 1.2323... Val Loss: 1.5972\n",
            "Epoch: 21/50... Step: 10560... Loss: 1.2507... Val Loss: 1.5989\n",
            "Epoch: 21/50... Step: 10570... Loss: 1.3109... Val Loss: 1.5947\n",
            "Epoch: 21/50... Step: 10580... Loss: 1.2101... Val Loss: 1.5957\n",
            "Epoch: 21/50... Step: 10590... Loss: 1.2149... Val Loss: 1.5973\n",
            "Epoch: 21/50... Step: 10600... Loss: 1.1771... Val Loss: 1.5967\n",
            "Epoch: 21/50... Step: 10610... Loss: 1.2904... Val Loss: 1.5941\n",
            "Epoch: 21/50... Step: 10620... Loss: 1.1952... Val Loss: 1.5975\n",
            "Epoch: 21/50... Step: 10630... Loss: 1.2166... Val Loss: 1.5995\n",
            "Epoch: 21/50... Step: 10640... Loss: 1.2819... Val Loss: 1.5900\n",
            "Epoch: 21/50... Step: 10650... Loss: 1.2133... Val Loss: 1.5902\n",
            "Epoch: 21/50... Step: 10660... Loss: 1.2243... Val Loss: 1.6034\n",
            "Epoch: 21/50... Step: 10670... Loss: 1.2559... Val Loss: 1.6038\n",
            "Epoch: 21/50... Step: 10680... Loss: 1.2352... Val Loss: 1.5929\n",
            "Epoch: 21/50... Step: 10690... Loss: 1.2343... Val Loss: 1.5987\n",
            "Epoch: 21/50... Step: 10700... Loss: 1.1818... Val Loss: 1.6064\n",
            "Epoch: 21/50... Step: 10710... Loss: 1.2409... Val Loss: 1.6042\n",
            "Epoch: 21/50... Step: 10720... Loss: 1.1904... Val Loss: 1.5984\n",
            "Epoch: 21/50... Step: 10730... Loss: 1.2404... Val Loss: 1.5961\n",
            "Epoch: 21/50... Step: 10740... Loss: 1.3278... Val Loss: 1.6010\n",
            "Epoch: 21/50... Step: 10750... Loss: 1.1727... Val Loss: 1.5978\n",
            "Epoch: 21/50... Step: 10760... Loss: 1.2214... Val Loss: 1.6001\n",
            "Epoch: 21/50... Step: 10770... Loss: 1.2725... Val Loss: 1.6102\n",
            "Epoch: 21/50... Step: 10780... Loss: 1.2707... Val Loss: 1.5963\n",
            "Epoch: 21/50... Step: 10790... Loss: 1.1809... Val Loss: 1.6029\n",
            "Epoch: 21/50... Step: 10800... Loss: 1.2477... Val Loss: 1.6108\n",
            "Epoch: 21/50... Step: 10810... Loss: 1.2303... Val Loss: 1.6012\n",
            "Epoch: 21/50... Step: 10820... Loss: 1.2469... Val Loss: 1.6043\n",
            "Epoch: 21/50... Step: 10830... Loss: 1.2306... Val Loss: 1.6047\n",
            "Epoch: 21/50... Step: 10840... Loss: 1.2851... Val Loss: 1.6026\n",
            "Epoch: 21/50... Step: 10850... Loss: 1.2505... Val Loss: 1.6043\n",
            "Epoch: 21/50... Step: 10860... Loss: 1.2202... Val Loss: 1.6012\n",
            "Epoch: 21/50... Step: 10870... Loss: 1.1871... Val Loss: 1.5895\n",
            "Epoch: 21/50... Step: 10880... Loss: 1.1914... Val Loss: 1.5931\n",
            "Epoch: 21/50... Step: 10890... Loss: 1.2570... Val Loss: 1.6018\n",
            "Epoch: 21/50... Step: 10900... Loss: 1.2513... Val Loss: 1.5940\n",
            "Epoch: 21/50... Step: 10910... Loss: 1.2008... Val Loss: 1.6027\n",
            "Epoch: 21/50... Step: 10920... Loss: 1.6625... Val Loss: 1.6086\n",
            "Epoch: 22/50... Step: 10930... Loss: 1.2039... Val Loss: 1.5977\n",
            "Epoch: 22/50... Step: 10940... Loss: 1.2239... Val Loss: 1.6032\n",
            "Epoch: 22/50... Step: 10950... Loss: 1.2032... Val Loss: 1.6167\n",
            "Epoch: 22/50... Step: 10960... Loss: 1.2249... Val Loss: 1.6095\n",
            "Epoch: 22/50... Step: 10970... Loss: 1.1891... Val Loss: 1.6099\n",
            "Epoch: 22/50... Step: 10980... Loss: 1.2536... Val Loss: 1.6072\n",
            "Epoch: 22/50... Step: 10990... Loss: 1.2292... Val Loss: 1.6074\n",
            "Epoch: 22/50... Step: 11000... Loss: 1.1353... Val Loss: 1.5978\n",
            "Epoch: 22/50... Step: 11010... Loss: 1.1886... Val Loss: 1.6029\n",
            "Epoch: 22/50... Step: 11020... Loss: 1.1776... Val Loss: 1.6059\n",
            "Epoch: 22/50... Step: 11030... Loss: 1.2193... Val Loss: 1.6023\n",
            "Epoch: 22/50... Step: 11040... Loss: 1.2604... Val Loss: 1.5984\n",
            "Epoch: 22/50... Step: 11050... Loss: 1.2741... Val Loss: 1.6019\n",
            "Epoch: 22/50... Step: 11060... Loss: 1.1928... Val Loss: 1.6023\n",
            "Epoch: 22/50... Step: 11070... Loss: 1.2204... Val Loss: 1.6129\n",
            "Epoch: 22/50... Step: 11080... Loss: 1.2461... Val Loss: 1.6042\n",
            "Epoch: 22/50... Step: 11090... Loss: 1.2961... Val Loss: 1.5933\n",
            "Epoch: 22/50... Step: 11100... Loss: 1.2155... Val Loss: 1.5974\n",
            "Epoch: 22/50... Step: 11110... Loss: 1.2054... Val Loss: 1.5965\n",
            "Epoch: 22/50... Step: 11120... Loss: 1.1884... Val Loss: 1.5974\n",
            "Epoch: 22/50... Step: 11130... Loss: 1.2578... Val Loss: 1.6020\n",
            "Epoch: 22/50... Step: 11140... Loss: 1.1808... Val Loss: 1.6047\n",
            "Epoch: 22/50... Step: 11150... Loss: 1.2226... Val Loss: 1.6077\n",
            "Epoch: 22/50... Step: 11160... Loss: 1.2967... Val Loss: 1.6035\n",
            "Epoch: 22/50... Step: 11170... Loss: 1.1826... Val Loss: 1.5929\n",
            "Epoch: 22/50... Step: 11180... Loss: 1.2387... Val Loss: 1.6015\n",
            "Epoch: 22/50... Step: 11190... Loss: 1.2551... Val Loss: 1.5993\n",
            "Epoch: 22/50... Step: 11200... Loss: 1.2294... Val Loss: 1.5948\n",
            "Epoch: 22/50... Step: 11210... Loss: 1.2255... Val Loss: 1.6049\n",
            "Epoch: 22/50... Step: 11220... Loss: 1.1998... Val Loss: 1.6116\n",
            "Epoch: 22/50... Step: 11230... Loss: 1.2109... Val Loss: 1.6083\n",
            "Epoch: 22/50... Step: 11240... Loss: 1.1680... Val Loss: 1.6052\n",
            "Epoch: 22/50... Step: 11250... Loss: 1.2344... Val Loss: 1.6054\n",
            "Epoch: 22/50... Step: 11260... Loss: 1.3233... Val Loss: 1.6051\n",
            "Epoch: 22/50... Step: 11270... Loss: 1.1328... Val Loss: 1.5989\n",
            "Epoch: 22/50... Step: 11280... Loss: 1.1908... Val Loss: 1.5999\n",
            "Epoch: 22/50... Step: 11290... Loss: 1.2623... Val Loss: 1.6066\n",
            "Epoch: 22/50... Step: 11300... Loss: 1.2708... Val Loss: 1.6014\n",
            "Epoch: 22/50... Step: 11310... Loss: 1.1541... Val Loss: 1.5983\n",
            "Epoch: 22/50... Step: 11320... Loss: 1.2341... Val Loss: 1.6023\n",
            "Epoch: 22/50... Step: 11330... Loss: 1.2008... Val Loss: 1.6016\n",
            "Epoch: 22/50... Step: 11340... Loss: 1.2218... Val Loss: 1.6053\n",
            "Epoch: 22/50... Step: 11350... Loss: 1.2115... Val Loss: 1.6042\n",
            "Epoch: 22/50... Step: 11360... Loss: 1.2618... Val Loss: 1.6015\n",
            "Epoch: 22/50... Step: 11370... Loss: 1.2142... Val Loss: 1.6064\n",
            "Epoch: 22/50... Step: 11380... Loss: 1.2193... Val Loss: 1.6009\n",
            "Epoch: 22/50... Step: 11390... Loss: 1.2015... Val Loss: 1.6018\n",
            "Epoch: 22/50... Step: 11400... Loss: 1.2223... Val Loss: 1.6013\n",
            "Epoch: 22/50... Step: 11410... Loss: 1.2398... Val Loss: 1.5992\n",
            "Epoch: 22/50... Step: 11420... Loss: 1.2418... Val Loss: 1.5960\n",
            "Epoch: 22/50... Step: 11430... Loss: 1.1989... Val Loss: 1.6037\n",
            "Epoch: 22/50... Step: 11440... Loss: 1.6452... Val Loss: 1.6013\n",
            "Epoch: 23/50... Step: 11450... Loss: 1.2169... Val Loss: 1.5996\n",
            "Epoch: 23/50... Step: 11460... Loss: 1.2235... Val Loss: 1.6055\n",
            "Epoch: 23/50... Step: 11470... Loss: 1.1969... Val Loss: 1.6176\n",
            "Epoch: 23/50... Step: 11480... Loss: 1.2218... Val Loss: 1.6178\n",
            "Epoch: 23/50... Step: 11490... Loss: 1.1684... Val Loss: 1.6149\n",
            "Epoch: 23/50... Step: 11500... Loss: 1.2120... Val Loss: 1.6112\n",
            "Epoch: 23/50... Step: 11510... Loss: 1.2463... Val Loss: 1.6103\n",
            "Epoch: 23/50... Step: 11520... Loss: 1.1673... Val Loss: 1.6069\n",
            "Epoch: 23/50... Step: 11530... Loss: 1.2324... Val Loss: 1.6105\n",
            "Epoch: 23/50... Step: 11540... Loss: 1.1849... Val Loss: 1.6071\n",
            "Epoch: 23/50... Step: 11550... Loss: 1.2302... Val Loss: 1.6074\n",
            "Epoch: 23/50... Step: 11560... Loss: 1.2135... Val Loss: 1.6068\n",
            "Epoch: 23/50... Step: 11570... Loss: 1.2815... Val Loss: 1.6106\n",
            "Epoch: 23/50... Step: 11580... Loss: 1.1968... Val Loss: 1.6152\n",
            "Epoch: 23/50... Step: 11590... Loss: 1.2319... Val Loss: 1.6125\n",
            "Epoch: 23/50... Step: 11600... Loss: 1.2379... Val Loss: 1.6062\n",
            "Epoch: 23/50... Step: 11610... Loss: 1.2645... Val Loss: 1.6004\n",
            "Epoch: 23/50... Step: 11620... Loss: 1.2432... Val Loss: 1.6007\n",
            "Epoch: 23/50... Step: 11630... Loss: 1.1841... Val Loss: 1.6049\n",
            "Epoch: 23/50... Step: 11640... Loss: 1.1716... Val Loss: 1.6029\n",
            "Epoch: 23/50... Step: 11650... Loss: 1.2519... Val Loss: 1.6079\n",
            "Epoch: 23/50... Step: 11660... Loss: 1.1875... Val Loss: 1.6042\n",
            "Epoch: 23/50... Step: 11670... Loss: 1.2180... Val Loss: 1.6070\n",
            "Epoch: 23/50... Step: 11680... Loss: 1.2792... Val Loss: 1.6053\n",
            "Epoch: 23/50... Step: 11690... Loss: 1.1991... Val Loss: 1.6056\n",
            "Epoch: 23/50... Step: 11700... Loss: 1.2192... Val Loss: 1.6083\n",
            "Epoch: 23/50... Step: 11710... Loss: 1.2141... Val Loss: 1.6080\n",
            "Epoch: 23/50... Step: 11720... Loss: 1.2125... Val Loss: 1.6017\n",
            "Epoch: 23/50... Step: 11730... Loss: 1.2198... Val Loss: 1.6000\n",
            "Epoch: 23/50... Step: 11740... Loss: 1.1527... Val Loss: 1.6169\n",
            "Epoch: 23/50... Step: 11750... Loss: 1.2591... Val Loss: 1.6136\n",
            "Epoch: 23/50... Step: 11760... Loss: 1.1924... Val Loss: 1.6055\n",
            "Epoch: 23/50... Step: 11770... Loss: 1.2422... Val Loss: 1.6011\n",
            "Epoch: 23/50... Step: 11780... Loss: 1.3212... Val Loss: 1.6057\n",
            "Epoch: 23/50... Step: 11790... Loss: 1.1606... Val Loss: 1.6028\n",
            "Epoch: 23/50... Step: 11800... Loss: 1.1960... Val Loss: 1.6071\n",
            "Epoch: 23/50... Step: 11810... Loss: 1.2337... Val Loss: 1.6164\n",
            "Epoch: 23/50... Step: 11820... Loss: 1.2382... Val Loss: 1.6063\n",
            "Epoch: 23/50... Step: 11830... Loss: 1.1876... Val Loss: 1.6065\n",
            "Epoch: 23/50... Step: 11840... Loss: 1.2186... Val Loss: 1.6176\n",
            "Epoch: 23/50... Step: 11850... Loss: 1.2046... Val Loss: 1.6137\n",
            "Epoch: 23/50... Step: 11860... Loss: 1.2312... Val Loss: 1.6120\n",
            "Epoch: 23/50... Step: 11870... Loss: 1.2025... Val Loss: 1.6077\n",
            "Epoch: 23/50... Step: 11880... Loss: 1.2445... Val Loss: 1.6103\n",
            "Epoch: 23/50... Step: 11890... Loss: 1.2135... Val Loss: 1.6137\n",
            "Epoch: 23/50... Step: 11900... Loss: 1.2230... Val Loss: 1.6154\n",
            "Epoch: 23/50... Step: 11910... Loss: 1.1850... Val Loss: 1.6082\n",
            "Epoch: 23/50... Step: 11920... Loss: 1.2186... Val Loss: 1.6030\n",
            "Epoch: 23/50... Step: 11930... Loss: 1.2532... Val Loss: 1.6082\n",
            "Epoch: 23/50... Step: 11940... Loss: 1.2305... Val Loss: 1.6052\n",
            "Epoch: 23/50... Step: 11950... Loss: 1.1826... Val Loss: 1.6065\n",
            "Epoch: 23/50... Step: 11960... Loss: 1.6474... Val Loss: 1.6069\n",
            "Epoch: 24/50... Step: 11970... Loss: 1.2174... Val Loss: 1.6006\n",
            "Epoch: 24/50... Step: 11980... Loss: 1.2034... Val Loss: 1.5987\n",
            "Epoch: 24/50... Step: 11990... Loss: 1.2043... Val Loss: 1.6142\n",
            "Epoch: 24/50... Step: 12000... Loss: 1.2469... Val Loss: 1.6129\n",
            "Epoch: 24/50... Step: 12010... Loss: 1.1730... Val Loss: 1.6117\n",
            "Epoch: 24/50... Step: 12020... Loss: 1.2463... Val Loss: 1.6164\n",
            "Epoch: 24/50... Step: 12030... Loss: 1.2392... Val Loss: 1.6175\n",
            "Epoch: 24/50... Step: 12040... Loss: 1.1386... Val Loss: 1.6055\n",
            "Epoch: 24/50... Step: 12050... Loss: 1.1949... Val Loss: 1.6117\n",
            "Epoch: 24/50... Step: 12060... Loss: 1.1947... Val Loss: 1.6107\n",
            "Epoch: 24/50... Step: 12070... Loss: 1.2341... Val Loss: 1.6055\n",
            "Epoch: 24/50... Step: 12080... Loss: 1.2179... Val Loss: 1.6075\n",
            "Epoch: 24/50... Step: 12090... Loss: 1.2607... Val Loss: 1.6130\n",
            "Epoch: 24/50... Step: 12100... Loss: 1.2208... Val Loss: 1.6088\n",
            "Epoch: 24/50... Step: 12110... Loss: 1.2194... Val Loss: 1.6081\n",
            "Epoch: 24/50... Step: 12120... Loss: 1.2442... Val Loss: 1.6058\n",
            "Epoch: 24/50... Step: 12130... Loss: 1.2763... Val Loss: 1.6030\n",
            "Epoch: 24/50... Step: 12140... Loss: 1.1881... Val Loss: 1.6047\n",
            "Epoch: 24/50... Step: 12150... Loss: 1.2229... Val Loss: 1.6000\n",
            "Epoch: 24/50... Step: 12160... Loss: 1.1549... Val Loss: 1.6027\n",
            "Epoch: 24/50... Step: 12170... Loss: 1.2787... Val Loss: 1.6088\n",
            "Epoch: 24/50... Step: 12180... Loss: 1.1804... Val Loss: 1.6017\n",
            "Epoch: 24/50... Step: 12190... Loss: 1.2239... Val Loss: 1.6136\n",
            "Epoch: 24/50... Step: 12200... Loss: 1.2598... Val Loss: 1.6110\n",
            "Epoch: 24/50... Step: 12210... Loss: 1.1842... Val Loss: 1.6046\n",
            "Epoch: 24/50... Step: 12220... Loss: 1.2065... Val Loss: 1.6124\n",
            "Epoch: 24/50... Step: 12230... Loss: 1.2233... Val Loss: 1.6171\n",
            "Epoch: 24/50... Step: 12240... Loss: 1.2214... Val Loss: 1.6038\n",
            "Epoch: 24/50... Step: 12250... Loss: 1.1947... Val Loss: 1.6086\n",
            "Epoch: 24/50... Step: 12260... Loss: 1.1825... Val Loss: 1.6153\n",
            "Epoch: 24/50... Step: 12270... Loss: 1.2317... Val Loss: 1.6138\n",
            "Epoch: 24/50... Step: 12280... Loss: 1.1643... Val Loss: 1.6132\n",
            "Epoch: 24/50... Step: 12290... Loss: 1.2534... Val Loss: 1.6081\n",
            "Epoch: 24/50... Step: 12300... Loss: 1.2943... Val Loss: 1.6025\n",
            "Epoch: 24/50... Step: 12310... Loss: 1.1273... Val Loss: 1.6072\n",
            "Epoch: 24/50... Step: 12320... Loss: 1.1756... Val Loss: 1.6112\n",
            "Epoch: 24/50... Step: 12330... Loss: 1.2513... Val Loss: 1.6149\n",
            "Epoch: 24/50... Step: 12340... Loss: 1.2400... Val Loss: 1.6019\n",
            "Epoch: 24/50... Step: 12350... Loss: 1.1578... Val Loss: 1.6070\n",
            "Epoch: 24/50... Step: 12360... Loss: 1.2210... Val Loss: 1.6136\n",
            "Epoch: 24/50... Step: 12370... Loss: 1.1955... Val Loss: 1.6138\n",
            "Epoch: 24/50... Step: 12380... Loss: 1.2374... Val Loss: 1.6071\n",
            "Epoch: 24/50... Step: 12390... Loss: 1.2198... Val Loss: 1.6087\n",
            "Epoch: 24/50... Step: 12400... Loss: 1.2317... Val Loss: 1.6115\n",
            "Epoch: 24/50... Step: 12410... Loss: 1.2057... Val Loss: 1.6115\n",
            "Epoch: 24/50... Step: 12420... Loss: 1.1923... Val Loss: 1.6022\n",
            "Epoch: 24/50... Step: 12430... Loss: 1.1744... Val Loss: 1.6061\n",
            "Epoch: 24/50... Step: 12440... Loss: 1.1964... Val Loss: 1.6098\n",
            "Epoch: 24/50... Step: 12450... Loss: 1.2448... Val Loss: 1.6144\n",
            "Epoch: 24/50... Step: 12460... Loss: 1.2350... Val Loss: 1.6032\n",
            "Epoch: 24/50... Step: 12470... Loss: 1.1953... Val Loss: 1.6109\n",
            "Epoch: 24/50... Step: 12480... Loss: 1.6895... Val Loss: 1.6145\n",
            "Epoch: 25/50... Step: 12490... Loss: 1.1796... Val Loss: 1.6050\n",
            "Epoch: 25/50... Step: 12500... Loss: 1.2037... Val Loss: 1.6062\n",
            "Epoch: 25/50... Step: 12510... Loss: 1.1875... Val Loss: 1.6153\n",
            "Epoch: 25/50... Step: 12520... Loss: 1.1935... Val Loss: 1.6195\n",
            "Epoch: 25/50... Step: 12530... Loss: 1.1580... Val Loss: 1.6204\n",
            "Epoch: 25/50... Step: 12540... Loss: 1.1986... Val Loss: 1.6193\n",
            "Epoch: 25/50... Step: 12550... Loss: 1.2257... Val Loss: 1.6220\n",
            "Epoch: 25/50... Step: 12560... Loss: 1.1313... Val Loss: 1.6111\n",
            "Epoch: 25/50... Step: 12570... Loss: 1.2294... Val Loss: 1.6144\n",
            "Epoch: 25/50... Step: 12580... Loss: 1.1525... Val Loss: 1.6148\n",
            "Epoch: 25/50... Step: 12590... Loss: 1.2286... Val Loss: 1.6103\n",
            "Epoch: 25/50... Step: 12600... Loss: 1.2164... Val Loss: 1.6097\n",
            "Epoch: 25/50... Step: 12610... Loss: 1.2511... Val Loss: 1.6136\n",
            "Epoch: 25/50... Step: 12620... Loss: 1.2243... Val Loss: 1.6120\n",
            "Epoch: 25/50... Step: 12630... Loss: 1.2247... Val Loss: 1.6109\n",
            "Epoch: 25/50... Step: 12640... Loss: 1.2173... Val Loss: 1.6051\n",
            "Epoch: 25/50... Step: 12650... Loss: 1.2719... Val Loss: 1.6017\n",
            "Epoch: 25/50... Step: 12660... Loss: 1.2091... Val Loss: 1.6007\n",
            "Epoch: 25/50... Step: 12670... Loss: 1.2103... Val Loss: 1.6090\n",
            "Epoch: 25/50... Step: 12680... Loss: 1.1764... Val Loss: 1.6134\n",
            "Epoch: 25/50... Step: 12690... Loss: 1.2678... Val Loss: 1.6054\n",
            "Epoch: 25/50... Step: 12700... Loss: 1.2065... Val Loss: 1.6067\n",
            "Epoch: 25/50... Step: 12710... Loss: 1.2163... Val Loss: 1.6153\n",
            "Epoch: 25/50... Step: 12720... Loss: 1.2787... Val Loss: 1.6143\n",
            "Epoch: 25/50... Step: 12730... Loss: 1.1635... Val Loss: 1.6058\n",
            "Epoch: 25/50... Step: 12740... Loss: 1.1999... Val Loss: 1.6097\n",
            "Epoch: 25/50... Step: 12750... Loss: 1.2183... Val Loss: 1.6157\n",
            "Epoch: 25/50... Step: 12760... Loss: 1.1981... Val Loss: 1.6142\n",
            "Epoch: 25/50... Step: 12770... Loss: 1.2174... Val Loss: 1.6093\n",
            "Epoch: 25/50... Step: 12780... Loss: 1.1487... Val Loss: 1.6169\n",
            "Epoch: 25/50... Step: 12790... Loss: 1.2479... Val Loss: 1.6164\n",
            "Epoch: 25/50... Step: 12800... Loss: 1.1344... Val Loss: 1.6143\n",
            "Epoch: 25/50... Step: 12810... Loss: 1.2188... Val Loss: 1.6126\n",
            "Epoch: 25/50... Step: 12820... Loss: 1.3438... Val Loss: 1.6133\n",
            "Epoch: 25/50... Step: 12830... Loss: 1.1579... Val Loss: 1.6098\n",
            "Epoch: 25/50... Step: 12840... Loss: 1.1899... Val Loss: 1.6125\n",
            "Epoch: 25/50... Step: 12850... Loss: 1.2508... Val Loss: 1.6177\n",
            "Epoch: 25/50... Step: 12860... Loss: 1.2682... Val Loss: 1.6109\n",
            "Epoch: 25/50... Step: 12870... Loss: 1.1360... Val Loss: 1.6150\n",
            "Epoch: 25/50... Step: 12880... Loss: 1.2145... Val Loss: 1.6168\n",
            "Epoch: 25/50... Step: 12890... Loss: 1.1987... Val Loss: 1.6145\n",
            "Epoch: 25/50... Step: 12900... Loss: 1.2097... Val Loss: 1.6161\n",
            "Epoch: 25/50... Step: 12910... Loss: 1.2226... Val Loss: 1.6107\n",
            "Epoch: 25/50... Step: 12920... Loss: 1.2563... Val Loss: 1.6104\n",
            "Epoch: 25/50... Step: 12930... Loss: 1.2058... Val Loss: 1.6187\n",
            "Epoch: 25/50... Step: 12940... Loss: 1.2077... Val Loss: 1.6131\n",
            "Epoch: 25/50... Step: 12950... Loss: 1.1773... Val Loss: 1.6113\n",
            "Epoch: 25/50... Step: 12960... Loss: 1.2032... Val Loss: 1.6071\n",
            "Epoch: 25/50... Step: 12970... Loss: 1.2141... Val Loss: 1.6084\n",
            "Epoch: 25/50... Step: 12980... Loss: 1.1939... Val Loss: 1.6087\n",
            "Epoch: 25/50... Step: 12990... Loss: 1.1831... Val Loss: 1.6176\n",
            "Epoch: 25/50... Step: 13000... Loss: 1.6518... Val Loss: 1.6174\n",
            "Epoch: 26/50... Step: 13010... Loss: 1.2059... Val Loss: 1.6095\n",
            "Epoch: 26/50... Step: 13020... Loss: 1.1671... Val Loss: 1.6106\n",
            "Epoch: 26/50... Step: 13030... Loss: 1.2048... Val Loss: 1.6276\n",
            "Epoch: 26/50... Step: 13040... Loss: 1.2026... Val Loss: 1.6267\n",
            "Epoch: 26/50... Step: 13050... Loss: 1.1831... Val Loss: 1.6142\n",
            "Epoch: 26/50... Step: 13060... Loss: 1.1978... Val Loss: 1.6194\n",
            "Epoch: 26/50... Step: 13070... Loss: 1.2204... Val Loss: 1.6175\n",
            "Epoch: 26/50... Step: 13080... Loss: 1.1328... Val Loss: 1.6187\n",
            "Epoch: 26/50... Step: 13090... Loss: 1.2040... Val Loss: 1.6230\n",
            "Epoch: 26/50... Step: 13100... Loss: 1.1637... Val Loss: 1.6092\n",
            "Epoch: 26/50... Step: 13110... Loss: 1.2502... Val Loss: 1.6115\n",
            "Epoch: 26/50... Step: 13120... Loss: 1.2097... Val Loss: 1.6199\n",
            "Epoch: 26/50... Step: 13130... Loss: 1.2476... Val Loss: 1.6170\n",
            "Epoch: 26/50... Step: 13140... Loss: 1.2461... Val Loss: 1.6187\n",
            "Epoch: 26/50... Step: 13150... Loss: 1.2431... Val Loss: 1.6221\n",
            "Epoch: 26/50... Step: 13160... Loss: 1.2385... Val Loss: 1.6110\n",
            "Epoch: 26/50... Step: 13170... Loss: 1.2772... Val Loss: 1.6058\n",
            "Epoch: 26/50... Step: 13180... Loss: 1.2034... Val Loss: 1.6098\n",
            "Epoch: 26/50... Step: 13190... Loss: 1.1839... Val Loss: 1.6078\n",
            "Epoch: 26/50... Step: 13200... Loss: 1.1568... Val Loss: 1.6089\n",
            "Epoch: 26/50... Step: 13210... Loss: 1.2551... Val Loss: 1.6062\n",
            "Epoch: 26/50... Step: 13220... Loss: 1.1484... Val Loss: 1.6122\n",
            "Epoch: 26/50... Step: 13230... Loss: 1.1879... Val Loss: 1.6170\n",
            "Epoch: 26/50... Step: 13240... Loss: 1.2662... Val Loss: 1.6177\n",
            "Epoch: 26/50... Step: 13250... Loss: 1.2043... Val Loss: 1.6062\n",
            "Epoch: 26/50... Step: 13260... Loss: 1.2247... Val Loss: 1.6103\n",
            "Epoch: 26/50... Step: 13270... Loss: 1.2094... Val Loss: 1.6156\n",
            "Epoch: 26/50... Step: 13280... Loss: 1.2256... Val Loss: 1.6108\n",
            "Epoch: 26/50... Step: 13290... Loss: 1.2348... Val Loss: 1.6090\n",
            "Epoch: 26/50... Step: 13300... Loss: 1.1713... Val Loss: 1.6160\n",
            "Epoch: 26/50... Step: 13310... Loss: 1.2044... Val Loss: 1.6225\n",
            "Epoch: 26/50... Step: 13320... Loss: 1.1868... Val Loss: 1.6171\n",
            "Epoch: 26/50... Step: 13330... Loss: 1.1978... Val Loss: 1.6083\n",
            "Epoch: 26/50... Step: 13340... Loss: 1.2900... Val Loss: 1.6111\n",
            "Epoch: 26/50... Step: 13350... Loss: 1.1163... Val Loss: 1.6139\n",
            "Epoch: 26/50... Step: 13360... Loss: 1.1708... Val Loss: 1.6157\n",
            "Epoch: 26/50... Step: 13370... Loss: 1.2243... Val Loss: 1.6203\n",
            "Epoch: 26/50... Step: 13380... Loss: 1.2437... Val Loss: 1.6085\n",
            "Epoch: 26/50... Step: 13390... Loss: 1.1436... Val Loss: 1.6103\n",
            "Epoch: 26/50... Step: 13400... Loss: 1.2040... Val Loss: 1.6206\n",
            "Epoch: 26/50... Step: 13410... Loss: 1.1622... Val Loss: 1.6178\n",
            "Epoch: 26/50... Step: 13420... Loss: 1.1883... Val Loss: 1.6086\n",
            "Epoch: 26/50... Step: 13430... Loss: 1.1969... Val Loss: 1.6110\n",
            "Epoch: 26/50... Step: 13440... Loss: 1.2271... Val Loss: 1.6126\n",
            "Epoch: 26/50... Step: 13450... Loss: 1.1914... Val Loss: 1.6164\n",
            "Epoch: 26/50... Step: 13460... Loss: 1.1886... Val Loss: 1.6159\n",
            "Epoch: 26/50... Step: 13470... Loss: 1.1812... Val Loss: 1.6154\n",
            "Epoch: 26/50... Step: 13480... Loss: 1.1850... Val Loss: 1.6093\n",
            "Epoch: 26/50... Step: 13490... Loss: 1.2328... Val Loss: 1.6149\n",
            "Epoch: 26/50... Step: 13500... Loss: 1.2306... Val Loss: 1.6141\n",
            "Epoch: 26/50... Step: 13510... Loss: 1.1580... Val Loss: 1.6187\n",
            "Epoch: 26/50... Step: 13520... Loss: 1.6331... Val Loss: 1.6218\n",
            "Epoch: 27/50... Step: 13530... Loss: 1.1927... Val Loss: 1.6121\n",
            "Epoch: 27/50... Step: 13540... Loss: 1.2030... Val Loss: 1.6095\n",
            "Epoch: 27/50... Step: 13550... Loss: 1.1766... Val Loss: 1.6195\n",
            "Epoch: 27/50... Step: 13560... Loss: 1.1891... Val Loss: 1.6264\n",
            "Epoch: 27/50... Step: 13570... Loss: 1.1762... Val Loss: 1.6301\n",
            "Epoch: 27/50... Step: 13580... Loss: 1.2163... Val Loss: 1.6292\n",
            "Epoch: 27/50... Step: 13590... Loss: 1.2083... Val Loss: 1.6277\n",
            "Epoch: 27/50... Step: 13600... Loss: 1.1004... Val Loss: 1.6223\n",
            "Epoch: 27/50... Step: 13610... Loss: 1.1896... Val Loss: 1.6257\n",
            "Epoch: 27/50... Step: 13620... Loss: 1.1758... Val Loss: 1.6222\n",
            "Epoch: 27/50... Step: 13630... Loss: 1.2101... Val Loss: 1.6197\n",
            "Epoch: 27/50... Step: 13640... Loss: 1.2098... Val Loss: 1.6181\n",
            "Epoch: 27/50... Step: 13650... Loss: 1.2669... Val Loss: 1.6211\n",
            "Epoch: 27/50... Step: 13660... Loss: 1.2252... Val Loss: 1.6249\n",
            "Epoch: 27/50... Step: 13670... Loss: 1.2475... Val Loss: 1.6255\n",
            "Epoch: 27/50... Step: 13680... Loss: 1.2232... Val Loss: 1.6202\n",
            "Epoch: 27/50... Step: 13690... Loss: 1.2647... Val Loss: 1.6154\n",
            "Epoch: 27/50... Step: 13700... Loss: 1.1976... Val Loss: 1.6176\n",
            "Epoch: 27/50... Step: 13710... Loss: 1.1995... Val Loss: 1.6150\n",
            "Epoch: 27/50... Step: 13720... Loss: 1.1415... Val Loss: 1.6168\n",
            "Epoch: 27/50... Step: 13730... Loss: 1.2566... Val Loss: 1.6168\n",
            "Epoch: 27/50... Step: 13740... Loss: 1.1703... Val Loss: 1.6162\n",
            "Epoch: 27/50... Step: 13750... Loss: 1.2014... Val Loss: 1.6244\n",
            "Epoch: 27/50... Step: 13760... Loss: 1.2503... Val Loss: 1.6234\n",
            "Epoch: 27/50... Step: 13770... Loss: 1.1804... Val Loss: 1.6118\n",
            "Epoch: 27/50... Step: 13780... Loss: 1.2293... Val Loss: 1.6205\n",
            "Epoch: 27/50... Step: 13790... Loss: 1.2322... Val Loss: 1.6194\n",
            "Epoch: 27/50... Step: 13800... Loss: 1.2186... Val Loss: 1.6156\n",
            "Epoch: 27/50... Step: 13810... Loss: 1.1759... Val Loss: 1.6160\n",
            "Epoch: 27/50... Step: 13820... Loss: 1.1538... Val Loss: 1.6185\n",
            "Epoch: 27/50... Step: 13830... Loss: 1.2153... Val Loss: 1.6177\n",
            "Epoch: 27/50... Step: 13840... Loss: 1.1532... Val Loss: 1.6175\n",
            "Epoch: 27/50... Step: 13850... Loss: 1.2202... Val Loss: 1.6189\n",
            "Epoch: 27/50... Step: 13860... Loss: 1.3151... Val Loss: 1.6207\n",
            "Epoch: 27/50... Step: 13870... Loss: 1.1473... Val Loss: 1.6182\n",
            "Epoch: 27/50... Step: 13880... Loss: 1.1787... Val Loss: 1.6147\n",
            "Epoch: 27/50... Step: 13890... Loss: 1.2441... Val Loss: 1.6244\n",
            "Epoch: 27/50... Step: 13900... Loss: 1.2453... Val Loss: 1.6122\n",
            "Epoch: 27/50... Step: 13910... Loss: 1.1590... Val Loss: 1.6115\n",
            "Epoch: 27/50... Step: 13920... Loss: 1.1839... Val Loss: 1.6169\n",
            "Epoch: 27/50... Step: 13930... Loss: 1.1849... Val Loss: 1.6199\n",
            "Epoch: 27/50... Step: 13940... Loss: 1.2000... Val Loss: 1.6217\n",
            "Epoch: 27/50... Step: 13950... Loss: 1.2073... Val Loss: 1.6217\n",
            "Epoch: 27/50... Step: 13960... Loss: 1.2098... Val Loss: 1.6195\n",
            "Epoch: 27/50... Step: 13970... Loss: 1.1762... Val Loss: 1.6238\n",
            "Epoch: 27/50... Step: 13980... Loss: 1.2018... Val Loss: 1.6218\n",
            "Epoch: 27/50... Step: 13990... Loss: 1.1948... Val Loss: 1.6148\n",
            "Epoch: 27/50... Step: 14000... Loss: 1.1641... Val Loss: 1.6206\n",
            "Epoch: 27/50... Step: 14010... Loss: 1.2233... Val Loss: 1.6248\n",
            "Epoch: 27/50... Step: 14020... Loss: 1.1704... Val Loss: 1.6205\n",
            "Epoch: 27/50... Step: 14030... Loss: 1.1982... Val Loss: 1.6253\n",
            "Epoch: 27/50... Step: 14040... Loss: 1.6181... Val Loss: 1.6210\n",
            "Epoch: 28/50... Step: 14050... Loss: 1.1896... Val Loss: 1.6153\n",
            "Epoch: 28/50... Step: 14060... Loss: 1.1711... Val Loss: 1.6166\n",
            "Epoch: 28/50... Step: 14070... Loss: 1.2048... Val Loss: 1.6284\n",
            "Epoch: 28/50... Step: 14080... Loss: 1.1889... Val Loss: 1.6345\n",
            "Epoch: 28/50... Step: 14090... Loss: 1.1742... Val Loss: 1.6292\n",
            "Epoch: 28/50... Step: 14100... Loss: 1.1844... Val Loss: 1.6282\n",
            "Epoch: 28/50... Step: 14110... Loss: 1.2185... Val Loss: 1.6277\n",
            "Epoch: 28/50... Step: 14120... Loss: 1.1060... Val Loss: 1.6203\n",
            "Epoch: 28/50... Step: 14130... Loss: 1.1719... Val Loss: 1.6174\n",
            "Epoch: 28/50... Step: 14140... Loss: 1.1487... Val Loss: 1.6182\n",
            "Epoch: 28/50... Step: 14150... Loss: 1.2569... Val Loss: 1.6136\n",
            "Epoch: 28/50... Step: 14160... Loss: 1.1918... Val Loss: 1.6167\n",
            "Epoch: 28/50... Step: 14170... Loss: 1.2189... Val Loss: 1.6128\n",
            "Epoch: 28/50... Step: 14180... Loss: 1.2052... Val Loss: 1.6178\n",
            "Epoch: 28/50... Step: 14190... Loss: 1.2192... Val Loss: 1.6195\n",
            "Epoch: 28/50... Step: 14200... Loss: 1.2136... Val Loss: 1.6156\n",
            "Epoch: 28/50... Step: 14210... Loss: 1.2320... Val Loss: 1.6119\n",
            "Epoch: 28/50... Step: 14220... Loss: 1.1707... Val Loss: 1.6163\n",
            "Epoch: 28/50... Step: 14230... Loss: 1.2180... Val Loss: 1.6214\n",
            "Epoch: 28/50... Step: 14240... Loss: 1.1442... Val Loss: 1.6168\n",
            "Epoch: 28/50... Step: 14250... Loss: 1.2484... Val Loss: 1.6124\n",
            "Epoch: 28/50... Step: 14260... Loss: 1.1338... Val Loss: 1.6125\n",
            "Epoch: 28/50... Step: 14270... Loss: 1.1871... Val Loss: 1.6200\n",
            "Epoch: 28/50... Step: 14280... Loss: 1.2343... Val Loss: 1.6231\n",
            "Epoch: 28/50... Step: 14290... Loss: 1.1902... Val Loss: 1.6150\n",
            "Epoch: 28/50... Step: 14300... Loss: 1.2071... Val Loss: 1.6186\n",
            "Epoch: 28/50... Step: 14310... Loss: 1.2283... Val Loss: 1.6242\n",
            "Epoch: 28/50... Step: 14320... Loss: 1.2421... Val Loss: 1.6168\n",
            "Epoch: 28/50... Step: 14330... Loss: 1.1942... Val Loss: 1.6142\n",
            "Epoch: 28/50... Step: 14340... Loss: 1.1718... Val Loss: 1.6214\n",
            "Epoch: 28/50... Step: 14350... Loss: 1.2173... Val Loss: 1.6167\n",
            "Epoch: 28/50... Step: 14360... Loss: 1.1329... Val Loss: 1.6189\n",
            "Epoch: 28/50... Step: 14370... Loss: 1.2098... Val Loss: 1.6198\n",
            "Epoch: 28/50... Step: 14380... Loss: 1.2859... Val Loss: 1.6163\n",
            "Epoch: 28/50... Step: 14390... Loss: 1.1295... Val Loss: 1.6186\n",
            "Epoch: 28/50... Step: 14400... Loss: 1.1564... Val Loss: 1.6212\n",
            "Epoch: 28/50... Step: 14410... Loss: 1.2113... Val Loss: 1.6281\n",
            "Epoch: 28/50... Step: 14420... Loss: 1.2574... Val Loss: 1.6183\n",
            "Epoch: 28/50... Step: 14430... Loss: 1.1415... Val Loss: 1.6132\n",
            "Epoch: 28/50... Step: 14440... Loss: 1.2399... Val Loss: 1.6193\n",
            "Epoch: 28/50... Step: 14450... Loss: 1.1817... Val Loss: 1.6227\n",
            "Epoch: 28/50... Step: 14460... Loss: 1.1823... Val Loss: 1.6200\n",
            "Epoch: 28/50... Step: 14470... Loss: 1.1985... Val Loss: 1.6225\n",
            "Epoch: 28/50... Step: 14480... Loss: 1.2419... Val Loss: 1.6178\n",
            "Epoch: 28/50... Step: 14490... Loss: 1.1967... Val Loss: 1.6238\n",
            "Epoch: 28/50... Step: 14500... Loss: 1.2103... Val Loss: 1.6199\n",
            "Epoch: 28/50... Step: 14510... Loss: 1.1795... Val Loss: 1.6170\n",
            "Epoch: 28/50... Step: 14520... Loss: 1.2039... Val Loss: 1.6246\n",
            "Epoch: 28/50... Step: 14530... Loss: 1.2332... Val Loss: 1.6251\n",
            "Epoch: 28/50... Step: 14540... Loss: 1.2263... Val Loss: 1.6177\n",
            "Epoch: 28/50... Step: 14550... Loss: 1.1640... Val Loss: 1.6196\n",
            "Epoch: 28/50... Step: 14560... Loss: 1.6174... Val Loss: 1.6289\n",
            "Epoch: 29/50... Step: 14570... Loss: 1.1706... Val Loss: 1.6196\n",
            "Epoch: 29/50... Step: 14580... Loss: 1.1714... Val Loss: 1.6131\n",
            "Epoch: 29/50... Step: 14590... Loss: 1.1675... Val Loss: 1.6294\n",
            "Epoch: 29/50... Step: 14600... Loss: 1.1889... Val Loss: 1.6357\n",
            "Epoch: 29/50... Step: 14610... Loss: 1.1526... Val Loss: 1.6273\n",
            "Epoch: 29/50... Step: 14620... Loss: 1.2161... Val Loss: 1.6288\n",
            "Epoch: 29/50... Step: 14630... Loss: 1.1867... Val Loss: 1.6311\n",
            "Epoch: 29/50... Step: 14640... Loss: 1.1392... Val Loss: 1.6187\n",
            "Epoch: 29/50... Step: 14650... Loss: 1.1631... Val Loss: 1.6195\n",
            "Epoch: 29/50... Step: 14660... Loss: 1.1479... Val Loss: 1.6249\n",
            "Epoch: 29/50... Step: 14670... Loss: 1.2188... Val Loss: 1.6202\n",
            "Epoch: 29/50... Step: 14680... Loss: 1.1819... Val Loss: 1.6185\n",
            "Epoch: 29/50... Step: 14690... Loss: 1.2250... Val Loss: 1.6225\n",
            "Epoch: 29/50... Step: 14700... Loss: 1.2096... Val Loss: 1.6191\n",
            "Epoch: 29/50... Step: 14710... Loss: 1.1980... Val Loss: 1.6210\n",
            "Epoch: 29/50... Step: 14720... Loss: 1.2316... Val Loss: 1.6209\n",
            "Epoch: 29/50... Step: 14730... Loss: 1.2494... Val Loss: 1.6138\n",
            "Epoch: 29/50... Step: 14740... Loss: 1.1819... Val Loss: 1.6155\n",
            "Epoch: 29/50... Step: 14750... Loss: 1.1814... Val Loss: 1.6219\n",
            "Epoch: 29/50... Step: 14760... Loss: 1.1350... Val Loss: 1.6166\n",
            "Epoch: 29/50... Step: 14770... Loss: 1.2404... Val Loss: 1.6106\n",
            "Epoch: 29/50... Step: 14780... Loss: 1.1944... Val Loss: 1.6101\n",
            "Epoch: 29/50... Step: 14790... Loss: 1.2002... Val Loss: 1.6232\n",
            "Epoch: 29/50... Step: 14800... Loss: 1.2361... Val Loss: 1.6294\n",
            "Epoch: 29/50... Step: 14810... Loss: 1.1499... Val Loss: 1.6178\n",
            "Epoch: 29/50... Step: 14820... Loss: 1.1636... Val Loss: 1.6190\n",
            "Epoch: 29/50... Step: 14830... Loss: 1.2088... Val Loss: 1.6233\n",
            "Epoch: 29/50... Step: 14840... Loss: 1.2206... Val Loss: 1.6185\n",
            "Epoch: 29/50... Step: 14850... Loss: 1.1904... Val Loss: 1.6118\n",
            "Epoch: 29/50... Step: 14860... Loss: 1.1648... Val Loss: 1.6197\n",
            "Epoch: 29/50... Step: 14870... Loss: 1.2164... Val Loss: 1.6265\n",
            "Epoch: 29/50... Step: 14880... Loss: 1.1635... Val Loss: 1.6217\n",
            "Epoch: 29/50... Step: 14890... Loss: 1.2109... Val Loss: 1.6185\n",
            "Epoch: 29/50... Step: 14900... Loss: 1.2624... Val Loss: 1.6169\n",
            "Epoch: 29/50... Step: 14910... Loss: 1.1103... Val Loss: 1.6192\n",
            "Epoch: 29/50... Step: 14920... Loss: 1.1775... Val Loss: 1.6160\n",
            "Epoch: 29/50... Step: 14930... Loss: 1.2169... Val Loss: 1.6238\n",
            "Epoch: 29/50... Step: 14940... Loss: 1.2345... Val Loss: 1.6167\n",
            "Epoch: 29/50... Step: 14950... Loss: 1.1617... Val Loss: 1.6162\n",
            "Epoch: 29/50... Step: 14960... Loss: 1.1909... Val Loss: 1.6209\n",
            "Epoch: 29/50... Step: 14970... Loss: 1.1607... Val Loss: 1.6213\n",
            "Epoch: 29/50... Step: 14980... Loss: 1.2230... Val Loss: 1.6244\n",
            "Epoch: 29/50... Step: 14990... Loss: 1.2369... Val Loss: 1.6224\n",
            "Epoch: 29/50... Step: 15000... Loss: 1.2192... Val Loss: 1.6199\n",
            "Epoch: 29/50... Step: 15010... Loss: 1.1778... Val Loss: 1.6246\n",
            "Epoch: 29/50... Step: 15020... Loss: 1.2001... Val Loss: 1.6222\n",
            "Epoch: 29/50... Step: 15030... Loss: 1.1742... Val Loss: 1.6168\n",
            "Epoch: 29/50... Step: 15040... Loss: 1.1859... Val Loss: 1.6168\n",
            "Epoch: 29/50... Step: 15050... Loss: 1.2559... Val Loss: 1.6217\n",
            "Epoch: 29/50... Step: 15060... Loss: 1.2217... Val Loss: 1.6190\n",
            "Epoch: 29/50... Step: 15070... Loss: 1.1710... Val Loss: 1.6208\n",
            "Epoch: 29/50... Step: 15080... Loss: 1.5824... Val Loss: 1.6245\n",
            "Epoch: 30/50... Step: 15090... Loss: 1.1540... Val Loss: 1.6215\n",
            "Epoch: 30/50... Step: 15100... Loss: 1.1866... Val Loss: 1.6193\n",
            "Epoch: 30/50... Step: 15110... Loss: 1.1982... Val Loss: 1.6250\n",
            "Epoch: 30/50... Step: 15120... Loss: 1.2112... Val Loss: 1.6327\n",
            "Epoch: 30/50... Step: 15130... Loss: 1.1691... Val Loss: 1.6273\n",
            "Epoch: 30/50... Step: 15140... Loss: 1.1885... Val Loss: 1.6302\n",
            "Epoch: 30/50... Step: 15150... Loss: 1.2034... Val Loss: 1.6332\n",
            "Epoch: 30/50... Step: 15160... Loss: 1.1115... Val Loss: 1.6217\n",
            "Epoch: 30/50... Step: 15170... Loss: 1.1654... Val Loss: 1.6256\n",
            "Epoch: 30/50... Step: 15180... Loss: 1.1643... Val Loss: 1.6258\n",
            "Epoch: 30/50... Step: 15190... Loss: 1.2282... Val Loss: 1.6259\n",
            "Epoch: 30/50... Step: 15200... Loss: 1.2152... Val Loss: 1.6278\n",
            "Epoch: 30/50... Step: 15210... Loss: 1.2420... Val Loss: 1.6278\n",
            "Epoch: 30/50... Step: 15220... Loss: 1.1926... Val Loss: 1.6273\n",
            "Epoch: 30/50... Step: 15230... Loss: 1.2122... Val Loss: 1.6301\n",
            "Epoch: 30/50... Step: 15240... Loss: 1.2103... Val Loss: 1.6249\n",
            "Epoch: 30/50... Step: 15250... Loss: 1.2320... Val Loss: 1.6175\n",
            "Epoch: 30/50... Step: 15260... Loss: 1.2254... Val Loss: 1.6198\n",
            "Epoch: 30/50... Step: 15270... Loss: 1.2020... Val Loss: 1.6239\n",
            "Epoch: 30/50... Step: 15280... Loss: 1.1633... Val Loss: 1.6188\n",
            "Epoch: 30/50... Step: 15290... Loss: 1.2550... Val Loss: 1.6213\n",
            "Epoch: 30/50... Step: 15300... Loss: 1.1297... Val Loss: 1.6207\n",
            "Epoch: 30/50... Step: 15310... Loss: 1.1639... Val Loss: 1.6236\n",
            "Epoch: 30/50... Step: 15320... Loss: 1.2755... Val Loss: 1.6312\n",
            "Epoch: 30/50... Step: 15330... Loss: 1.1536... Val Loss: 1.6289\n",
            "Epoch: 30/50... Step: 15340... Loss: 1.2163... Val Loss: 1.6278\n",
            "Epoch: 30/50... Step: 15350... Loss: 1.2178... Val Loss: 1.6275\n",
            "Epoch: 30/50... Step: 15360... Loss: 1.1863... Val Loss: 1.6288\n",
            "Epoch: 30/50... Step: 15370... Loss: 1.1908... Val Loss: 1.6264\n",
            "Epoch: 30/50... Step: 15380... Loss: 1.1626... Val Loss: 1.6305\n",
            "Epoch: 30/50... Step: 15390... Loss: 1.2135... Val Loss: 1.6257\n",
            "Epoch: 30/50... Step: 15400... Loss: 1.1418... Val Loss: 1.6239\n",
            "Epoch: 30/50... Step: 15410... Loss: 1.2145... Val Loss: 1.6267\n",
            "Epoch: 30/50... Step: 15420... Loss: 1.3129... Val Loss: 1.6274\n",
            "Epoch: 30/50... Step: 15430... Loss: 1.1106... Val Loss: 1.6282\n",
            "Epoch: 30/50... Step: 15440... Loss: 1.1869... Val Loss: 1.6245\n",
            "Epoch: 30/50... Step: 15450... Loss: 1.2147... Val Loss: 1.6348\n",
            "Epoch: 30/50... Step: 15460... Loss: 1.2075... Val Loss: 1.6217\n",
            "Epoch: 30/50... Step: 15470... Loss: 1.1292... Val Loss: 1.6148\n",
            "Epoch: 30/50... Step: 15480... Loss: 1.2060... Val Loss: 1.6218\n",
            "Epoch: 30/50... Step: 15490... Loss: 1.1762... Val Loss: 1.6281\n",
            "Epoch: 30/50... Step: 15500... Loss: 1.2172... Val Loss: 1.6282\n",
            "Epoch: 30/50... Step: 15510... Loss: 1.1830... Val Loss: 1.6256\n",
            "Epoch: 30/50... Step: 15520... Loss: 1.2431... Val Loss: 1.6228\n",
            "Epoch: 30/50... Step: 15530... Loss: 1.1644... Val Loss: 1.6290\n",
            "Epoch: 30/50... Step: 15540... Loss: 1.1670... Val Loss: 1.6259\n",
            "Epoch: 30/50... Step: 15550... Loss: 1.1505... Val Loss: 1.6221\n",
            "Epoch: 30/50... Step: 15560... Loss: 1.1677... Val Loss: 1.6225\n",
            "Epoch: 30/50... Step: 15570... Loss: 1.2072... Val Loss: 1.6210\n",
            "Epoch: 30/50... Step: 15580... Loss: 1.2030... Val Loss: 1.6171\n",
            "Epoch: 30/50... Step: 15590... Loss: 1.1604... Val Loss: 1.6264\n",
            "Epoch: 30/50... Step: 15600... Loss: 1.6043... Val Loss: 1.6269\n",
            "Epoch: 31/50... Step: 15610... Loss: 1.1615... Val Loss: 1.6227\n",
            "Epoch: 31/50... Step: 15620... Loss: 1.1895... Val Loss: 1.6200\n",
            "Epoch: 31/50... Step: 15630... Loss: 1.1790... Val Loss: 1.6296\n",
            "Epoch: 31/50... Step: 15640... Loss: 1.2016... Val Loss: 1.6419\n",
            "Epoch: 31/50... Step: 15650... Loss: 1.1539... Val Loss: 1.6415\n",
            "Epoch: 31/50... Step: 15660... Loss: 1.1830... Val Loss: 1.6375\n",
            "Epoch: 31/50... Step: 15670... Loss: 1.2046... Val Loss: 1.6395\n",
            "Epoch: 31/50... Step: 15680... Loss: 1.1194... Val Loss: 1.6302\n",
            "Epoch: 31/50... Step: 15690... Loss: 1.1718... Val Loss: 1.6319\n",
            "Epoch: 31/50... Step: 15700... Loss: 1.1482... Val Loss: 1.6329\n",
            "Epoch: 31/50... Step: 15710... Loss: 1.2228... Val Loss: 1.6248\n",
            "Epoch: 31/50... Step: 15720... Loss: 1.1650... Val Loss: 1.6237\n",
            "Epoch: 31/50... Step: 15730... Loss: 1.2190... Val Loss: 1.6243\n",
            "Epoch: 31/50... Step: 15740... Loss: 1.2019... Val Loss: 1.6272\n",
            "Epoch: 31/50... Step: 15750... Loss: 1.1823... Val Loss: 1.6268\n",
            "Epoch: 31/50... Step: 15760... Loss: 1.1901... Val Loss: 1.6260\n",
            "Epoch: 31/50... Step: 15770... Loss: 1.2328... Val Loss: 1.6248\n",
            "Epoch: 31/50... Step: 15780... Loss: 1.1828... Val Loss: 1.6258\n",
            "Epoch: 31/50... Step: 15790... Loss: 1.1519... Val Loss: 1.6242\n",
            "Epoch: 31/50... Step: 15800... Loss: 1.1131... Val Loss: 1.6257\n",
            "Epoch: 31/50... Step: 15810... Loss: 1.2616... Val Loss: 1.6227\n",
            "Epoch: 31/50... Step: 15820... Loss: 1.1827... Val Loss: 1.6280\n",
            "Epoch: 31/50... Step: 15830... Loss: 1.1660... Val Loss: 1.6336\n",
            "Epoch: 31/50... Step: 15840... Loss: 1.2531... Val Loss: 1.6296\n",
            "Epoch: 31/50... Step: 15850... Loss: 1.1694... Val Loss: 1.6183\n",
            "Epoch: 31/50... Step: 15860... Loss: 1.1917... Val Loss: 1.6267\n",
            "Epoch: 31/50... Step: 15870... Loss: 1.2059... Val Loss: 1.6324\n",
            "Epoch: 31/50... Step: 15880... Loss: 1.1702... Val Loss: 1.6258\n",
            "Epoch: 31/50... Step: 15890... Loss: 1.1485... Val Loss: 1.6219\n",
            "Epoch: 31/50... Step: 15900... Loss: 1.1584... Val Loss: 1.6314\n",
            "Epoch: 31/50... Step: 15910... Loss: 1.2320... Val Loss: 1.6287\n",
            "Epoch: 31/50... Step: 15920... Loss: 1.1560... Val Loss: 1.6289\n",
            "Epoch: 31/50... Step: 15930... Loss: 1.2279... Val Loss: 1.6240\n",
            "Epoch: 31/50... Step: 15940... Loss: 1.2888... Val Loss: 1.6256\n",
            "Epoch: 31/50... Step: 15950... Loss: 1.1216... Val Loss: 1.6274\n",
            "Epoch: 31/50... Step: 15960... Loss: 1.1609... Val Loss: 1.6226\n",
            "Epoch: 31/50... Step: 15970... Loss: 1.2384... Val Loss: 1.6337\n",
            "Epoch: 31/50... Step: 15980... Loss: 1.2221... Val Loss: 1.6209\n",
            "Epoch: 31/50... Step: 15990... Loss: 1.1494... Val Loss: 1.6165\n",
            "Epoch: 31/50... Step: 16000... Loss: 1.2016... Val Loss: 1.6189\n",
            "Epoch: 31/50... Step: 16010... Loss: 1.1688... Val Loss: 1.6222\n",
            "Epoch: 31/50... Step: 16020... Loss: 1.2265... Val Loss: 1.6285\n",
            "Epoch: 31/50... Step: 16030... Loss: 1.1946... Val Loss: 1.6221\n",
            "Epoch: 31/50... Step: 16040... Loss: 1.2278... Val Loss: 1.6231\n",
            "Epoch: 31/50... Step: 16050... Loss: 1.1851... Val Loss: 1.6265\n",
            "Epoch: 31/50... Step: 16060... Loss: 1.1825... Val Loss: 1.6230\n",
            "Epoch: 31/50... Step: 16070... Loss: 1.1535... Val Loss: 1.6213\n",
            "Epoch: 31/50... Step: 16080... Loss: 1.1427... Val Loss: 1.6226\n",
            "Epoch: 31/50... Step: 16090... Loss: 1.2483... Val Loss: 1.6207\n",
            "Epoch: 31/50... Step: 16100... Loss: 1.2396... Val Loss: 1.6184\n",
            "Epoch: 31/50... Step: 16110... Loss: 1.1512... Val Loss: 1.6230\n",
            "Epoch: 31/50... Step: 16120... Loss: 1.5876... Val Loss: 1.6259\n",
            "Epoch: 32/50... Step: 16130... Loss: 1.1895... Val Loss: 1.6188\n",
            "Epoch: 32/50... Step: 16140... Loss: 1.1478... Val Loss: 1.6189\n",
            "Epoch: 32/50... Step: 16150... Loss: 1.2100... Val Loss: 1.6285\n",
            "Epoch: 32/50... Step: 16160... Loss: 1.1861... Val Loss: 1.6384\n",
            "Epoch: 32/50... Step: 16170... Loss: 1.1513... Val Loss: 1.6344\n",
            "Epoch: 32/50... Step: 16180... Loss: 1.2054... Val Loss: 1.6326\n",
            "Epoch: 32/50... Step: 16190... Loss: 1.1803... Val Loss: 1.6414\n",
            "Epoch: 32/50... Step: 16200... Loss: 1.1226... Val Loss: 1.6338\n",
            "Epoch: 32/50... Step: 16210... Loss: 1.1869... Val Loss: 1.6319\n",
            "Epoch: 32/50... Step: 16220... Loss: 1.1178... Val Loss: 1.6311\n",
            "Epoch: 32/50... Step: 16230... Loss: 1.2085... Val Loss: 1.6276\n",
            "Epoch: 32/50... Step: 16240... Loss: 1.2100... Val Loss: 1.6287\n",
            "Epoch: 32/50... Step: 16250... Loss: 1.2432... Val Loss: 1.6277\n",
            "Epoch: 32/50... Step: 16260... Loss: 1.2129... Val Loss: 1.6239\n",
            "Epoch: 32/50... Step: 16270... Loss: 1.1980... Val Loss: 1.6298\n",
            "Epoch: 32/50... Step: 16280... Loss: 1.2120... Val Loss: 1.6294\n",
            "Epoch: 32/50... Step: 16290... Loss: 1.2494... Val Loss: 1.6194\n",
            "Epoch: 32/50... Step: 16300... Loss: 1.1267... Val Loss: 1.6234\n",
            "Epoch: 32/50... Step: 16310... Loss: 1.1832... Val Loss: 1.6292\n",
            "Epoch: 32/50... Step: 16320... Loss: 1.1110... Val Loss: 1.6233\n",
            "Epoch: 32/50... Step: 16330... Loss: 1.2347... Val Loss: 1.6294\n",
            "Epoch: 32/50... Step: 16340... Loss: 1.1700... Val Loss: 1.6261\n",
            "Epoch: 32/50... Step: 16350... Loss: 1.2045... Val Loss: 1.6312\n",
            "Epoch: 32/50... Step: 16360... Loss: 1.2342... Val Loss: 1.6305\n",
            "Epoch: 32/50... Step: 16370... Loss: 1.1617... Val Loss: 1.6252\n",
            "Epoch: 32/50... Step: 16380... Loss: 1.2043... Val Loss: 1.6289\n",
            "Epoch: 32/50... Step: 16390... Loss: 1.1979... Val Loss: 1.6320\n",
            "Epoch: 32/50... Step: 16400... Loss: 1.1669... Val Loss: 1.6297\n",
            "Epoch: 32/50... Step: 16410... Loss: 1.1851... Val Loss: 1.6282\n",
            "Epoch: 32/50... Step: 16420... Loss: 1.1549... Val Loss: 1.6321\n",
            "Epoch: 32/50... Step: 16430... Loss: 1.2076... Val Loss: 1.6319\n",
            "Epoch: 32/50... Step: 16440... Loss: 1.1449... Val Loss: 1.6260\n",
            "Epoch: 32/50... Step: 16450... Loss: 1.2150... Val Loss: 1.6294\n",
            "Epoch: 32/50... Step: 16460... Loss: 1.2550... Val Loss: 1.6369\n",
            "Epoch: 32/50... Step: 16470... Loss: 1.1307... Val Loss: 1.6270\n",
            "Epoch: 32/50... Step: 16480... Loss: 1.1598... Val Loss: 1.6219\n",
            "Epoch: 32/50... Step: 16490... Loss: 1.2037... Val Loss: 1.6332\n",
            "Epoch: 32/50... Step: 16500... Loss: 1.1981... Val Loss: 1.6299\n",
            "Epoch: 32/50... Step: 16510... Loss: 1.1174... Val Loss: 1.6257\n",
            "Epoch: 32/50... Step: 16520... Loss: 1.2014... Val Loss: 1.6230\n",
            "Epoch: 32/50... Step: 16530... Loss: 1.1431... Val Loss: 1.6266\n",
            "Epoch: 32/50... Step: 16540... Loss: 1.2225... Val Loss: 1.6328\n",
            "Epoch: 32/50... Step: 16550... Loss: 1.1605... Val Loss: 1.6300\n",
            "Epoch: 32/50... Step: 16560... Loss: 1.2581... Val Loss: 1.6275\n",
            "Epoch: 32/50... Step: 16570... Loss: 1.1879... Val Loss: 1.6313\n",
            "Epoch: 32/50... Step: 16580... Loss: 1.1727... Val Loss: 1.6235\n",
            "Epoch: 32/50... Step: 16590... Loss: 1.1757... Val Loss: 1.6221\n",
            "Epoch: 32/50... Step: 16600... Loss: 1.1927... Val Loss: 1.6230\n",
            "Epoch: 32/50... Step: 16610... Loss: 1.2160... Val Loss: 1.6227\n",
            "Epoch: 32/50... Step: 16620... Loss: 1.1780... Val Loss: 1.6232\n",
            "Epoch: 32/50... Step: 16630... Loss: 1.1306... Val Loss: 1.6326\n",
            "Epoch: 32/50... Step: 16640... Loss: 1.5843... Val Loss: 1.6361\n",
            "Epoch: 33/50... Step: 16650... Loss: 1.1846... Val Loss: 1.6273\n",
            "Epoch: 33/50... Step: 16660... Loss: 1.1849... Val Loss: 1.6251\n",
            "Epoch: 33/50... Step: 16670... Loss: 1.1506... Val Loss: 1.6338\n",
            "Epoch: 33/50... Step: 16680... Loss: 1.1989... Val Loss: 1.6467\n",
            "Epoch: 33/50... Step: 16690... Loss: 1.1833... Val Loss: 1.6425\n",
            "Epoch: 33/50... Step: 16700... Loss: 1.2096... Val Loss: 1.6370\n",
            "Epoch: 33/50... Step: 16710... Loss: 1.1651... Val Loss: 1.6436\n",
            "Epoch: 33/50... Step: 16720... Loss: 1.1205... Val Loss: 1.6339\n",
            "Epoch: 33/50... Step: 16730... Loss: 1.1886... Val Loss: 1.6335\n",
            "Epoch: 33/50... Step: 16740... Loss: 1.1481... Val Loss: 1.6339\n",
            "Epoch: 33/50... Step: 16750... Loss: 1.1990... Val Loss: 1.6282\n",
            "Epoch: 33/50... Step: 16760... Loss: 1.1904... Val Loss: 1.6325\n",
            "Epoch: 33/50... Step: 16770... Loss: 1.2420... Val Loss: 1.6281\n",
            "Epoch: 33/50... Step: 16780... Loss: 1.2235... Val Loss: 1.6243\n",
            "Epoch: 33/50... Step: 16790... Loss: 1.2065... Val Loss: 1.6279\n",
            "Epoch: 33/50... Step: 16800... Loss: 1.2074... Val Loss: 1.6302\n",
            "Epoch: 33/50... Step: 16810... Loss: 1.2245... Val Loss: 1.6219\n",
            "Epoch: 33/50... Step: 16820... Loss: 1.1474... Val Loss: 1.6229\n",
            "Epoch: 33/50... Step: 16830... Loss: 1.1576... Val Loss: 1.6254\n",
            "Epoch: 33/50... Step: 16840... Loss: 1.1001... Val Loss: 1.6272\n",
            "Epoch: 33/50... Step: 16850... Loss: 1.2324... Val Loss: 1.6267\n",
            "Epoch: 33/50... Step: 16860... Loss: 1.1225... Val Loss: 1.6224\n",
            "Epoch: 33/50... Step: 16870... Loss: 1.1836... Val Loss: 1.6351\n",
            "Epoch: 33/50... Step: 16880... Loss: 1.2435... Val Loss: 1.6364\n",
            "Epoch: 33/50... Step: 16890... Loss: 1.1461... Val Loss: 1.6237\n",
            "Epoch: 33/50... Step: 16900... Loss: 1.1840... Val Loss: 1.6285\n",
            "Epoch: 33/50... Step: 16910... Loss: 1.2234... Val Loss: 1.6319\n",
            "Epoch: 33/50... Step: 16920... Loss: 1.1833... Val Loss: 1.6319\n",
            "Epoch: 33/50... Step: 16930... Loss: 1.1585... Val Loss: 1.6343\n",
            "Epoch: 33/50... Step: 16940... Loss: 1.1424... Val Loss: 1.6281\n",
            "Epoch: 33/50... Step: 16950... Loss: 1.2095... Val Loss: 1.6265\n",
            "Epoch: 33/50... Step: 16960... Loss: 1.1692... Val Loss: 1.6276\n",
            "Epoch: 33/50... Step: 16970... Loss: 1.2007... Val Loss: 1.6272\n",
            "Epoch: 33/50... Step: 16980... Loss: 1.2955... Val Loss: 1.6267\n",
            "Epoch: 33/50... Step: 16990... Loss: 1.1152... Val Loss: 1.6289\n",
            "Epoch: 33/50... Step: 17000... Loss: 1.1477... Val Loss: 1.6273\n",
            "Epoch: 33/50... Step: 17010... Loss: 1.2019... Val Loss: 1.6360\n",
            "Epoch: 33/50... Step: 17020... Loss: 1.2320... Val Loss: 1.6270\n",
            "Epoch: 33/50... Step: 17030... Loss: 1.1258... Val Loss: 1.6312\n",
            "Epoch: 33/50... Step: 17040... Loss: 1.1976... Val Loss: 1.6265\n",
            "Epoch: 33/50... Step: 17050... Loss: 1.1695... Val Loss: 1.6317\n",
            "Epoch: 33/50... Step: 17060... Loss: 1.1790... Val Loss: 1.6364\n",
            "Epoch: 33/50... Step: 17070... Loss: 1.1809... Val Loss: 1.6321\n",
            "Epoch: 33/50... Step: 17080... Loss: 1.2148... Val Loss: 1.6281\n",
            "Epoch: 33/50... Step: 17090... Loss: 1.1506... Val Loss: 1.6348\n",
            "Epoch: 33/50... Step: 17100... Loss: 1.1435... Val Loss: 1.6321\n",
            "Epoch: 33/50... Step: 17110... Loss: 1.1619... Val Loss: 1.6253\n",
            "Epoch: 33/50... Step: 17120... Loss: 1.1847... Val Loss: 1.6249\n",
            "Epoch: 33/50... Step: 17130... Loss: 1.2230... Val Loss: 1.6347\n",
            "Epoch: 33/50... Step: 17140... Loss: 1.2097... Val Loss: 1.6276\n",
            "Epoch: 33/50... Step: 17150... Loss: 1.1613... Val Loss: 1.6297\n",
            "Epoch: 33/50... Step: 17160... Loss: 1.5862... Val Loss: 1.6349\n",
            "Epoch: 34/50... Step: 17170... Loss: 1.1926... Val Loss: 1.6321\n",
            "Epoch: 34/50... Step: 17180... Loss: 1.1816... Val Loss: 1.6290\n",
            "Epoch: 34/50... Step: 17190... Loss: 1.1601... Val Loss: 1.6363\n",
            "Epoch: 34/50... Step: 17200... Loss: 1.1991... Val Loss: 1.6434\n",
            "Epoch: 34/50... Step: 17210... Loss: 1.1290... Val Loss: 1.6406\n",
            "Epoch: 34/50... Step: 17220... Loss: 1.1921... Val Loss: 1.6394\n",
            "Epoch: 34/50... Step: 17230... Loss: 1.1687... Val Loss: 1.6434\n",
            "Epoch: 34/50... Step: 17240... Loss: 1.1070... Val Loss: 1.6376\n",
            "Epoch: 34/50... Step: 17250... Loss: 1.1746... Val Loss: 1.6344\n",
            "Epoch: 34/50... Step: 17260... Loss: 1.1339... Val Loss: 1.6358\n",
            "Epoch: 34/50... Step: 17270... Loss: 1.1886... Val Loss: 1.6298\n",
            "Epoch: 34/50... Step: 17280... Loss: 1.1928... Val Loss: 1.6300\n",
            "Epoch: 34/50... Step: 17290... Loss: 1.2275... Val Loss: 1.6328\n",
            "Epoch: 34/50... Step: 17300... Loss: 1.1946... Val Loss: 1.6326\n",
            "Epoch: 34/50... Step: 17310... Loss: 1.2051... Val Loss: 1.6387\n",
            "Epoch: 34/50... Step: 17320... Loss: 1.1919... Val Loss: 1.6363\n",
            "Epoch: 34/50... Step: 17330... Loss: 1.2494... Val Loss: 1.6315\n",
            "Epoch: 34/50... Step: 17340... Loss: 1.1793... Val Loss: 1.6275\n",
            "Epoch: 34/50... Step: 17350... Loss: 1.1466... Val Loss: 1.6310\n",
            "Epoch: 34/50... Step: 17360... Loss: 1.1473... Val Loss: 1.6290\n",
            "Epoch: 34/50... Step: 17370... Loss: 1.2431... Val Loss: 1.6336\n",
            "Epoch: 34/50... Step: 17380... Loss: 1.1928... Val Loss: 1.6304\n",
            "Epoch: 34/50... Step: 17390... Loss: 1.1696... Val Loss: 1.6348\n",
            "Epoch: 34/50... Step: 17400... Loss: 1.2167... Val Loss: 1.6328\n",
            "Epoch: 34/50... Step: 17410... Loss: 1.1476... Val Loss: 1.6276\n",
            "Epoch: 34/50... Step: 17420... Loss: 1.1769... Val Loss: 1.6304\n",
            "Epoch: 34/50... Step: 17430... Loss: 1.1934... Val Loss: 1.6356\n",
            "Epoch: 34/50... Step: 17440... Loss: 1.1604... Val Loss: 1.6328\n",
            "Epoch: 34/50... Step: 17450... Loss: 1.1620... Val Loss: 1.6325\n",
            "Epoch: 34/50... Step: 17460... Loss: 1.1488... Val Loss: 1.6343\n",
            "Epoch: 34/50... Step: 17470... Loss: 1.2071... Val Loss: 1.6345\n",
            "Epoch: 34/50... Step: 17480... Loss: 1.1278... Val Loss: 1.6360\n",
            "Epoch: 34/50... Step: 17490... Loss: 1.1995... Val Loss: 1.6350\n",
            "Epoch: 34/50... Step: 17500... Loss: 1.2712... Val Loss: 1.6284\n",
            "Epoch: 34/50... Step: 17510... Loss: 1.0974... Val Loss: 1.6293\n",
            "Epoch: 34/50... Step: 17520... Loss: 1.1711... Val Loss: 1.6283\n",
            "Epoch: 34/50... Step: 17530... Loss: 1.2206... Val Loss: 1.6337\n",
            "Epoch: 34/50... Step: 17540... Loss: 1.2188... Val Loss: 1.6257\n",
            "Epoch: 34/50... Step: 17550... Loss: 1.1166... Val Loss: 1.6216\n",
            "Epoch: 34/50... Step: 17560... Loss: 1.1799... Val Loss: 1.6288\n",
            "Epoch: 34/50... Step: 17570... Loss: 1.1346... Val Loss: 1.6349\n",
            "Epoch: 34/50... Step: 17580... Loss: 1.1882... Val Loss: 1.6338\n",
            "Epoch: 34/50... Step: 17590... Loss: 1.1950... Val Loss: 1.6292\n",
            "Epoch: 34/50... Step: 17600... Loss: 1.2404... Val Loss: 1.6280\n",
            "Epoch: 34/50... Step: 17610... Loss: 1.1834... Val Loss: 1.6321\n",
            "Epoch: 34/50... Step: 17620... Loss: 1.1818... Val Loss: 1.6299\n",
            "Epoch: 34/50... Step: 17630... Loss: 1.1327... Val Loss: 1.6299\n",
            "Epoch: 34/50... Step: 17640... Loss: 1.1794... Val Loss: 1.6248\n",
            "Epoch: 34/50... Step: 17650... Loss: 1.2121... Val Loss: 1.6263\n",
            "Epoch: 34/50... Step: 17660... Loss: 1.1805... Val Loss: 1.6296\n",
            "Epoch: 34/50... Step: 17670... Loss: 1.0923... Val Loss: 1.6317\n",
            "Epoch: 34/50... Step: 17680... Loss: 1.5357... Val Loss: 1.6356\n",
            "Epoch: 35/50... Step: 17690... Loss: 1.1707... Val Loss: 1.6314\n",
            "Epoch: 35/50... Step: 17700... Loss: 1.1417... Val Loss: 1.6260\n",
            "Epoch: 35/50... Step: 17710... Loss: 1.1395... Val Loss: 1.6361\n",
            "Epoch: 35/50... Step: 17720... Loss: 1.1549... Val Loss: 1.6444\n",
            "Epoch: 35/50... Step: 17730... Loss: 1.1231... Val Loss: 1.6444\n",
            "Epoch: 35/50... Step: 17740... Loss: 1.2001... Val Loss: 1.6422\n",
            "Epoch: 35/50... Step: 17750... Loss: 1.1745... Val Loss: 1.6431\n",
            "Epoch: 35/50... Step: 17760... Loss: 1.0905... Val Loss: 1.6369\n",
            "Epoch: 35/50... Step: 17770... Loss: 1.1352... Val Loss: 1.6277\n",
            "Epoch: 35/50... Step: 17780... Loss: 1.1290... Val Loss: 1.6297\n",
            "Epoch: 35/50... Step: 17790... Loss: 1.2364... Val Loss: 1.6313\n",
            "Epoch: 35/50... Step: 17800... Loss: 1.1961... Val Loss: 1.6326\n",
            "Epoch: 35/50... Step: 17810... Loss: 1.2278... Val Loss: 1.6359\n",
            "Epoch: 35/50... Step: 17820... Loss: 1.1892... Val Loss: 1.6366\n",
            "Epoch: 35/50... Step: 17830... Loss: 1.1932... Val Loss: 1.6395\n",
            "Epoch: 35/50... Step: 17840... Loss: 1.1798... Val Loss: 1.6392\n",
            "Epoch: 35/50... Step: 17850... Loss: 1.2455... Val Loss: 1.6302\n",
            "Epoch: 35/50... Step: 17860... Loss: 1.1847... Val Loss: 1.6263\n",
            "Epoch: 35/50... Step: 17870... Loss: 1.1653... Val Loss: 1.6378\n",
            "Epoch: 35/50... Step: 17880... Loss: 1.1134... Val Loss: 1.6322\n",
            "Epoch: 35/50... Step: 17890... Loss: 1.2412... Val Loss: 1.6287\n",
            "Epoch: 35/50... Step: 17900... Loss: 1.1472... Val Loss: 1.6278\n",
            "Epoch: 35/50... Step: 17910... Loss: 1.1388... Val Loss: 1.6342\n",
            "Epoch: 35/50... Step: 17920... Loss: 1.2188... Val Loss: 1.6382\n",
            "Epoch: 35/50... Step: 17930... Loss: 1.1489... Val Loss: 1.6270\n",
            "Epoch: 35/50... Step: 17940... Loss: 1.2017... Val Loss: 1.6359\n",
            "Epoch: 35/50... Step: 17950... Loss: 1.1791... Val Loss: 1.6376\n",
            "Epoch: 35/50... Step: 17960... Loss: 1.1776... Val Loss: 1.6321\n",
            "Epoch: 35/50... Step: 17970... Loss: 1.1982... Val Loss: 1.6348\n",
            "Epoch: 35/50... Step: 17980... Loss: 1.1355... Val Loss: 1.6391\n",
            "Epoch: 35/50... Step: 17990... Loss: 1.1926... Val Loss: 1.6370\n",
            "Epoch: 35/50... Step: 18000... Loss: 1.1563... Val Loss: 1.6345\n",
            "Epoch: 35/50... Step: 18010... Loss: 1.1957... Val Loss: 1.6342\n",
            "Epoch: 35/50... Step: 18020... Loss: 1.2673... Val Loss: 1.6334\n",
            "Epoch: 35/50... Step: 18030... Loss: 1.1604... Val Loss: 1.6285\n",
            "Epoch: 35/50... Step: 18040... Loss: 1.1491... Val Loss: 1.6264\n",
            "Epoch: 35/50... Step: 18050... Loss: 1.1754... Val Loss: 1.6452\n",
            "Epoch: 35/50... Step: 18060... Loss: 1.1983... Val Loss: 1.6369\n",
            "Epoch: 35/50... Step: 18070... Loss: 1.1384... Val Loss: 1.6326\n",
            "Epoch: 35/50... Step: 18080... Loss: 1.1629... Val Loss: 1.6371\n",
            "Epoch: 35/50... Step: 18090... Loss: 1.1476... Val Loss: 1.6349\n",
            "Epoch: 35/50... Step: 18100... Loss: 1.2291... Val Loss: 1.6387\n",
            "Epoch: 35/50... Step: 18110... Loss: 1.1794... Val Loss: 1.6428\n",
            "Epoch: 35/50... Step: 18120... Loss: 1.1977... Val Loss: 1.6335\n",
            "Epoch: 35/50... Step: 18130... Loss: 1.1536... Val Loss: 1.6355\n",
            "Epoch: 35/50... Step: 18140... Loss: 1.1590... Val Loss: 1.6354\n",
            "Epoch: 35/50... Step: 18150... Loss: 1.1603... Val Loss: 1.6298\n",
            "Epoch: 35/50... Step: 18160... Loss: 1.1904... Val Loss: 1.6313\n",
            "Epoch: 35/50... Step: 18170... Loss: 1.2087... Val Loss: 1.6326\n",
            "Epoch: 35/50... Step: 18180... Loss: 1.1851... Val Loss: 1.6306\n",
            "Epoch: 35/50... Step: 18190... Loss: 1.1558... Val Loss: 1.6372\n",
            "Epoch: 35/50... Step: 18200... Loss: 1.5653... Val Loss: 1.6410\n",
            "Epoch: 36/50... Step: 18210... Loss: 1.1494... Val Loss: 1.6334\n",
            "Epoch: 36/50... Step: 18220... Loss: 1.1645... Val Loss: 1.6348\n",
            "Epoch: 36/50... Step: 18230... Loss: 1.1659... Val Loss: 1.6419\n",
            "Epoch: 36/50... Step: 18240... Loss: 1.1914... Val Loss: 1.6447\n",
            "Epoch: 36/50... Step: 18250... Loss: 1.1600... Val Loss: 1.6481\n",
            "Epoch: 36/50... Step: 18260... Loss: 1.1872... Val Loss: 1.6474\n",
            "Epoch: 36/50... Step: 18270... Loss: 1.2060... Val Loss: 1.6438\n",
            "Epoch: 36/50... Step: 18280... Loss: 1.1069... Val Loss: 1.6408\n",
            "Epoch: 36/50... Step: 18290... Loss: 1.1575... Val Loss: 1.6438\n",
            "Epoch: 36/50... Step: 18300... Loss: 1.1173... Val Loss: 1.6415\n",
            "Epoch: 36/50... Step: 18310... Loss: 1.1780... Val Loss: 1.6329\n",
            "Epoch: 36/50... Step: 18320... Loss: 1.1902... Val Loss: 1.6340\n",
            "Epoch: 36/50... Step: 18330... Loss: 1.2276... Val Loss: 1.6413\n",
            "Epoch: 36/50... Step: 18340... Loss: 1.2172... Val Loss: 1.6412\n",
            "Epoch: 36/50... Step: 18350... Loss: 1.1855... Val Loss: 1.6435\n",
            "Epoch: 36/50... Step: 18360... Loss: 1.1842... Val Loss: 1.6420\n",
            "Epoch: 36/50... Step: 18370... Loss: 1.2255... Val Loss: 1.6323\n",
            "Epoch: 36/50... Step: 18380... Loss: 1.1378... Val Loss: 1.6295\n",
            "Epoch: 36/50... Step: 18390... Loss: 1.1744... Val Loss: 1.6365\n",
            "Epoch: 36/50... Step: 18400... Loss: 1.1364... Val Loss: 1.6324\n",
            "Epoch: 36/50... Step: 18410... Loss: 1.2257... Val Loss: 1.6305\n",
            "Epoch: 36/50... Step: 18420... Loss: 1.1432... Val Loss: 1.6316\n",
            "Epoch: 36/50... Step: 18430... Loss: 1.1950... Val Loss: 1.6391\n",
            "Epoch: 36/50... Step: 18440... Loss: 1.2088... Val Loss: 1.6398\n",
            "Epoch: 36/50... Step: 18450... Loss: 1.1772... Val Loss: 1.6348\n",
            "Epoch: 36/50... Step: 18460... Loss: 1.1516... Val Loss: 1.6354\n",
            "Epoch: 36/50... Step: 18470... Loss: 1.1888... Val Loss: 1.6363\n",
            "Epoch: 36/50... Step: 18480... Loss: 1.1609... Val Loss: 1.6325\n",
            "Epoch: 36/50... Step: 18490... Loss: 1.1641... Val Loss: 1.6329\n",
            "Epoch: 36/50... Step: 18500... Loss: 1.1306... Val Loss: 1.6398\n",
            "Epoch: 36/50... Step: 18510... Loss: 1.2053... Val Loss: 1.6368\n",
            "Epoch: 36/50... Step: 18520... Loss: 1.1292... Val Loss: 1.6366\n",
            "Epoch: 36/50... Step: 18530... Loss: 1.2089... Val Loss: 1.6370\n",
            "Epoch: 36/50... Step: 18540... Loss: 1.2685... Val Loss: 1.6395\n",
            "Epoch: 36/50... Step: 18550... Loss: 1.1018... Val Loss: 1.6399\n",
            "Epoch: 36/50... Step: 18560... Loss: 1.1453... Val Loss: 1.6328\n",
            "Epoch: 36/50... Step: 18570... Loss: 1.2243... Val Loss: 1.6349\n",
            "Epoch: 36/50... Step: 18580... Loss: 1.2447... Val Loss: 1.6339\n",
            "Epoch: 36/50... Step: 18590... Loss: 1.0755... Val Loss: 1.6335\n",
            "Epoch: 36/50... Step: 18600... Loss: 1.1925... Val Loss: 1.6350\n",
            "Epoch: 36/50... Step: 18610... Loss: 1.1519... Val Loss: 1.6438\n",
            "Epoch: 36/50... Step: 18620... Loss: 1.1585... Val Loss: 1.6448\n",
            "Epoch: 36/50... Step: 18630... Loss: 1.1657... Val Loss: 1.6334\n",
            "Epoch: 36/50... Step: 18640... Loss: 1.2311... Val Loss: 1.6343\n",
            "Epoch: 36/50... Step: 18650... Loss: 1.1636... Val Loss: 1.6401\n",
            "Epoch: 36/50... Step: 18660... Loss: 1.1894... Val Loss: 1.6402\n",
            "Epoch: 36/50... Step: 18670... Loss: 1.1240... Val Loss: 1.6315\n",
            "Epoch: 36/50... Step: 18680... Loss: 1.1659... Val Loss: 1.6344\n",
            "Epoch: 36/50... Step: 18690... Loss: 1.2177... Val Loss: 1.6394\n",
            "Epoch: 36/50... Step: 18700... Loss: 1.1797... Val Loss: 1.6378\n",
            "Epoch: 36/50... Step: 18710... Loss: 1.1527... Val Loss: 1.6435\n",
            "Epoch: 36/50... Step: 18720... Loss: 1.5507... Val Loss: 1.6531\n",
            "Epoch: 37/50... Step: 18730... Loss: 1.1793... Val Loss: 1.6498\n",
            "Epoch: 37/50... Step: 18740... Loss: 1.1751... Val Loss: 1.6413\n",
            "Epoch: 37/50... Step: 18750... Loss: 1.1695... Val Loss: 1.6454\n",
            "Epoch: 37/50... Step: 18760... Loss: 1.1556... Val Loss: 1.6526\n",
            "Epoch: 37/50... Step: 18770... Loss: 1.1411... Val Loss: 1.6588\n",
            "Epoch: 37/50... Step: 18780... Loss: 1.1983... Val Loss: 1.6490\n",
            "Epoch: 37/50... Step: 18790... Loss: 1.1729... Val Loss: 1.6499\n",
            "Epoch: 37/50... Step: 18800... Loss: 1.0756... Val Loss: 1.6502\n",
            "Epoch: 37/50... Step: 18810... Loss: 1.1600... Val Loss: 1.6459\n",
            "Epoch: 37/50... Step: 18820... Loss: 1.1248... Val Loss: 1.6429\n",
            "Epoch: 37/50... Step: 18830... Loss: 1.2078... Val Loss: 1.6430\n",
            "Epoch: 37/50... Step: 18840... Loss: 1.2032... Val Loss: 1.6432\n",
            "Epoch: 37/50... Step: 18850... Loss: 1.2240... Val Loss: 1.6412\n",
            "Epoch: 37/50... Step: 18860... Loss: 1.2126... Val Loss: 1.6431\n",
            "Epoch: 37/50... Step: 18870... Loss: 1.1996... Val Loss: 1.6447\n",
            "Epoch: 37/50... Step: 18880... Loss: 1.1671... Val Loss: 1.6411\n",
            "Epoch: 37/50... Step: 18890... Loss: 1.2524... Val Loss: 1.6355\n",
            "Epoch: 37/50... Step: 18900... Loss: 1.1677... Val Loss: 1.6349\n",
            "Epoch: 37/50... Step: 18910... Loss: 1.1723... Val Loss: 1.6356\n",
            "Epoch: 37/50... Step: 18920... Loss: 1.1225... Val Loss: 1.6341\n",
            "Epoch: 37/50... Step: 18930... Loss: 1.2284... Val Loss: 1.6296\n",
            "Epoch: 37/50... Step: 18940... Loss: 1.1451... Val Loss: 1.6297\n",
            "Epoch: 37/50... Step: 18950... Loss: 1.1917... Val Loss: 1.6400\n",
            "Epoch: 37/50... Step: 18960... Loss: 1.2414... Val Loss: 1.6467\n",
            "Epoch: 37/50... Step: 18970... Loss: 1.1480... Val Loss: 1.6381\n",
            "Epoch: 37/50... Step: 18980... Loss: 1.1666... Val Loss: 1.6389\n",
            "Epoch: 37/50... Step: 18990... Loss: 1.1876... Val Loss: 1.6374\n",
            "Epoch: 37/50... Step: 19000... Loss: 1.1931... Val Loss: 1.6345\n",
            "Epoch: 37/50... Step: 19010... Loss: 1.1511... Val Loss: 1.6358\n",
            "Epoch: 37/50... Step: 19020... Loss: 1.1117... Val Loss: 1.6403\n",
            "Epoch: 37/50... Step: 19030... Loss: 1.2085... Val Loss: 1.6421\n",
            "Epoch: 37/50... Step: 19040... Loss: 1.1412... Val Loss: 1.6375\n",
            "Epoch: 37/50... Step: 19050... Loss: 1.1906... Val Loss: 1.6404\n",
            "Epoch: 37/50... Step: 19060... Loss: 1.2681... Val Loss: 1.6421\n",
            "Epoch: 37/50... Step: 19070... Loss: 1.1232... Val Loss: 1.6348\n",
            "Epoch: 37/50... Step: 19080... Loss: 1.1409... Val Loss: 1.6349\n",
            "Epoch: 37/50... Step: 19090... Loss: 1.1983... Val Loss: 1.6454\n",
            "Epoch: 37/50... Step: 19100... Loss: 1.2174... Val Loss: 1.6396\n",
            "Epoch: 37/50... Step: 19110... Loss: 1.1256... Val Loss: 1.6365\n",
            "Epoch: 37/50... Step: 19120... Loss: 1.1816... Val Loss: 1.6406\n",
            "Epoch: 37/50... Step: 19130... Loss: 1.1472... Val Loss: 1.6479\n",
            "Epoch: 37/50... Step: 19140... Loss: 1.1998... Val Loss: 1.6447\n",
            "Epoch: 37/50... Step: 19150... Loss: 1.1322... Val Loss: 1.6421\n",
            "Epoch: 37/50... Step: 19160... Loss: 1.2083... Val Loss: 1.6389\n",
            "Epoch: 37/50... Step: 19170... Loss: 1.1533... Val Loss: 1.6379\n",
            "Epoch: 37/50... Step: 19180... Loss: 1.1598... Val Loss: 1.6396\n",
            "Epoch: 37/50... Step: 19190... Loss: 1.1467... Val Loss: 1.6418\n",
            "Epoch: 37/50... Step: 19200... Loss: 1.1605... Val Loss: 1.6416\n",
            "Epoch: 37/50... Step: 19210... Loss: 1.2344... Val Loss: 1.6396\n",
            "Epoch: 37/50... Step: 19220... Loss: 1.1818... Val Loss: 1.6328\n",
            "Epoch: 37/50... Step: 19230... Loss: 1.1471... Val Loss: 1.6436\n",
            "Epoch: 37/50... Step: 19240... Loss: 1.5312... Val Loss: 1.6541\n",
            "Epoch: 38/50... Step: 19250... Loss: 1.1513... Val Loss: 1.6471\n",
            "Epoch: 38/50... Step: 19260... Loss: 1.1834... Val Loss: 1.6391\n",
            "Epoch: 38/50... Step: 19270... Loss: 1.1584... Val Loss: 1.6447\n",
            "Epoch: 38/50... Step: 19280... Loss: 1.1779... Val Loss: 1.6469\n",
            "Epoch: 38/50... Step: 19290... Loss: 1.1566... Val Loss: 1.6431\n",
            "Epoch: 38/50... Step: 19300... Loss: 1.2019... Val Loss: 1.6456\n",
            "Epoch: 38/50... Step: 19310... Loss: 1.1640... Val Loss: 1.6519\n",
            "Epoch: 38/50... Step: 19320... Loss: 1.1010... Val Loss: 1.6481\n",
            "Epoch: 38/50... Step: 19330... Loss: 1.1773... Val Loss: 1.6459\n",
            "Epoch: 38/50... Step: 19340... Loss: 1.1243... Val Loss: 1.6433\n",
            "Epoch: 38/50... Step: 19350... Loss: 1.1807... Val Loss: 1.6394\n",
            "Epoch: 38/50... Step: 19360... Loss: 1.1874... Val Loss: 1.6425\n",
            "Epoch: 38/50... Step: 19370... Loss: 1.2069... Val Loss: 1.6494\n",
            "Epoch: 38/50... Step: 19380... Loss: 1.1870... Val Loss: 1.6430\n",
            "Epoch: 38/50... Step: 19390... Loss: 1.1770... Val Loss: 1.6439\n",
            "Epoch: 38/50... Step: 19400... Loss: 1.2303... Val Loss: 1.6466\n",
            "Epoch: 38/50... Step: 19410... Loss: 1.2184... Val Loss: 1.6444\n",
            "Epoch: 38/50... Step: 19420... Loss: 1.1747... Val Loss: 1.6413\n",
            "Epoch: 38/50... Step: 19430... Loss: 1.1788... Val Loss: 1.6413\n",
            "Epoch: 38/50... Step: 19440... Loss: 1.1311... Val Loss: 1.6397\n",
            "Epoch: 38/50... Step: 19450... Loss: 1.2306... Val Loss: 1.6317\n",
            "Epoch: 38/50... Step: 19460... Loss: 1.1461... Val Loss: 1.6290\n",
            "Epoch: 38/50... Step: 19470... Loss: 1.1945... Val Loss: 1.6400\n",
            "Epoch: 38/50... Step: 19480... Loss: 1.2353... Val Loss: 1.6470\n",
            "Epoch: 38/50... Step: 19490... Loss: 1.1394... Val Loss: 1.6406\n",
            "Epoch: 38/50... Step: 19500... Loss: 1.1681... Val Loss: 1.6450\n",
            "Epoch: 38/50... Step: 19510... Loss: 1.1784... Val Loss: 1.6453\n",
            "Epoch: 38/50... Step: 19520... Loss: 1.1673... Val Loss: 1.6366\n",
            "Epoch: 38/50... Step: 19530... Loss: 1.1543... Val Loss: 1.6443\n",
            "Epoch: 38/50... Step: 19540... Loss: 1.1187... Val Loss: 1.6424\n",
            "Epoch: 38/50... Step: 19550... Loss: 1.1848... Val Loss: 1.6388\n",
            "Epoch: 38/50... Step: 19560... Loss: 1.1171... Val Loss: 1.6397\n",
            "Epoch: 38/50... Step: 19570... Loss: 1.1750... Val Loss: 1.6404\n",
            "Epoch: 38/50... Step: 19580... Loss: 1.2528... Val Loss: 1.6408\n",
            "Epoch: 38/50... Step: 19590... Loss: 1.0878... Val Loss: 1.6408\n",
            "Epoch: 38/50... Step: 19600... Loss: 1.1386... Val Loss: 1.6394\n",
            "Epoch: 38/50... Step: 19610... Loss: 1.2430... Val Loss: 1.6405\n",
            "Epoch: 38/50... Step: 19620... Loss: 1.1900... Val Loss: 1.6364\n",
            "Epoch: 38/50... Step: 19630... Loss: 1.1109... Val Loss: 1.6365\n",
            "Epoch: 38/50... Step: 19640... Loss: 1.1539... Val Loss: 1.6424\n",
            "Epoch: 38/50... Step: 19650... Loss: 1.1324... Val Loss: 1.6400\n",
            "Epoch: 38/50... Step: 19660... Loss: 1.1811... Val Loss: 1.6427\n",
            "Epoch: 38/50... Step: 19670... Loss: 1.1541... Val Loss: 1.6409\n",
            "Epoch: 38/50... Step: 19680... Loss: 1.2078... Val Loss: 1.6399\n",
            "Epoch: 38/50... Step: 19690... Loss: 1.1635... Val Loss: 1.6438\n",
            "Epoch: 38/50... Step: 19700... Loss: 1.1331... Val Loss: 1.6360\n",
            "Epoch: 38/50... Step: 19710... Loss: 1.1390... Val Loss: 1.6335\n",
            "Epoch: 38/50... Step: 19720... Loss: 1.1553... Val Loss: 1.6325\n",
            "Epoch: 38/50... Step: 19730... Loss: 1.1844... Val Loss: 1.6359\n",
            "Epoch: 38/50... Step: 19740... Loss: 1.1824... Val Loss: 1.6353\n",
            "Epoch: 38/50... Step: 19750... Loss: 1.0955... Val Loss: 1.6375\n",
            "Epoch: 38/50... Step: 19760... Loss: 1.4993... Val Loss: 1.6504\n",
            "Epoch: 39/50... Step: 19770... Loss: 1.1640... Val Loss: 1.6464\n",
            "Epoch: 39/50... Step: 19780... Loss: 1.1357... Val Loss: 1.6414\n",
            "Epoch: 39/50... Step: 19790... Loss: 1.1731... Val Loss: 1.6486\n",
            "Epoch: 39/50... Step: 19800... Loss: 1.2031... Val Loss: 1.6554\n",
            "Epoch: 39/50... Step: 19810... Loss: 1.1426... Val Loss: 1.6538\n",
            "Epoch: 39/50... Step: 19820... Loss: 1.2094... Val Loss: 1.6496\n",
            "Epoch: 39/50... Step: 19830... Loss: 1.1703... Val Loss: 1.6496\n",
            "Epoch: 39/50... Step: 19840... Loss: 1.0612... Val Loss: 1.6535\n",
            "Epoch: 39/50... Step: 19850... Loss: 1.1729... Val Loss: 1.6519\n",
            "Epoch: 39/50... Step: 19860... Loss: 1.1362... Val Loss: 1.6474\n",
            "Epoch: 39/50... Step: 19870... Loss: 1.2012... Val Loss: 1.6437\n",
            "Epoch: 39/50... Step: 19880... Loss: 1.1654... Val Loss: 1.6442\n",
            "Epoch: 39/50... Step: 19890... Loss: 1.1911... Val Loss: 1.6443\n",
            "Epoch: 39/50... Step: 19900... Loss: 1.1756... Val Loss: 1.6433\n",
            "Epoch: 39/50... Step: 19910... Loss: 1.1927... Val Loss: 1.6496\n",
            "Epoch: 39/50... Step: 19920... Loss: 1.1909... Val Loss: 1.6472\n",
            "Epoch: 39/50... Step: 19930... Loss: 1.2315... Val Loss: 1.6420\n",
            "Epoch: 39/50... Step: 19940... Loss: 1.1794... Val Loss: 1.6418\n",
            "Epoch: 39/50... Step: 19950... Loss: 1.1600... Val Loss: 1.6432\n",
            "Epoch: 39/50... Step: 19960... Loss: 1.1173... Val Loss: 1.6391\n",
            "Epoch: 39/50... Step: 19970... Loss: 1.2323... Val Loss: 1.6417\n",
            "Epoch: 39/50... Step: 19980... Loss: 1.1333... Val Loss: 1.6357\n",
            "Epoch: 39/50... Step: 19990... Loss: 1.1503... Val Loss: 1.6421\n",
            "Epoch: 39/50... Step: 20000... Loss: 1.2392... Val Loss: 1.6467\n",
            "Epoch: 39/50... Step: 20010... Loss: 1.1398... Val Loss: 1.6362\n",
            "Epoch: 39/50... Step: 20020... Loss: 1.1762... Val Loss: 1.6365\n",
            "Epoch: 39/50... Step: 20030... Loss: 1.1865... Val Loss: 1.6422\n",
            "Epoch: 39/50... Step: 20040... Loss: 1.1613... Val Loss: 1.6401\n",
            "Epoch: 39/50... Step: 20050... Loss: 1.1521... Val Loss: 1.6330\n",
            "Epoch: 39/50... Step: 20060... Loss: 1.1226... Val Loss: 1.6408\n",
            "Epoch: 39/50... Step: 20070... Loss: 1.1831... Val Loss: 1.6367\n",
            "Epoch: 39/50... Step: 20080... Loss: 1.1363... Val Loss: 1.6369\n",
            "Epoch: 39/50... Step: 20090... Loss: 1.1725... Val Loss: 1.6429\n",
            "Epoch: 39/50... Step: 20100... Loss: 1.2445... Val Loss: 1.6474\n",
            "Epoch: 39/50... Step: 20110... Loss: 1.0840... Val Loss: 1.6448\n",
            "Epoch: 39/50... Step: 20120... Loss: 1.1614... Val Loss: 1.6412\n",
            "Epoch: 39/50... Step: 20130... Loss: 1.2121... Val Loss: 1.6444\n",
            "Epoch: 39/50... Step: 20140... Loss: 1.2008... Val Loss: 1.6400\n",
            "Epoch: 39/50... Step: 20150... Loss: 1.1170... Val Loss: 1.6404\n",
            "Epoch: 39/50... Step: 20160... Loss: 1.1966... Val Loss: 1.6381\n",
            "Epoch: 39/50... Step: 20170... Loss: 1.1547... Val Loss: 1.6410\n",
            "Epoch: 39/50... Step: 20180... Loss: 1.1485... Val Loss: 1.6453\n",
            "Epoch: 39/50... Step: 20190... Loss: 1.1671... Val Loss: 1.6431\n",
            "Epoch: 39/50... Step: 20200... Loss: 1.2213... Val Loss: 1.6323\n",
            "Epoch: 39/50... Step: 20210... Loss: 1.1798... Val Loss: 1.6396\n",
            "Epoch: 39/50... Step: 20220... Loss: 1.1466... Val Loss: 1.6455\n",
            "Epoch: 39/50... Step: 20230... Loss: 1.1155... Val Loss: 1.6446\n",
            "Epoch: 39/50... Step: 20240... Loss: 1.1571... Val Loss: 1.6411\n",
            "Epoch: 39/50... Step: 20250... Loss: 1.1943... Val Loss: 1.6465\n",
            "Epoch: 39/50... Step: 20260... Loss: 1.1551... Val Loss: 1.6396\n",
            "Epoch: 39/50... Step: 20270... Loss: 1.1003... Val Loss: 1.6369\n",
            "Epoch: 39/50... Step: 20280... Loss: 1.5627... Val Loss: 1.6407\n",
            "Epoch: 40/50... Step: 20290... Loss: 1.1617... Val Loss: 1.6427\n",
            "Epoch: 40/50... Step: 20300... Loss: 1.1439... Val Loss: 1.6445\n",
            "Epoch: 40/50... Step: 20310... Loss: 1.1412... Val Loss: 1.6523\n",
            "Epoch: 40/50... Step: 20320... Loss: 1.1337... Val Loss: 1.6572\n",
            "Epoch: 40/50... Step: 20330... Loss: 1.1452... Val Loss: 1.6552\n",
            "Epoch: 40/50... Step: 20340... Loss: 1.2059... Val Loss: 1.6500\n",
            "Epoch: 40/50... Step: 20350... Loss: 1.1607... Val Loss: 1.6526\n",
            "Epoch: 40/50... Step: 20360... Loss: 1.0920... Val Loss: 1.6522\n",
            "Epoch: 40/50... Step: 20370... Loss: 1.1429... Val Loss: 1.6476\n",
            "Epoch: 40/50... Step: 20380... Loss: 1.0954... Val Loss: 1.6433\n",
            "Epoch: 40/50... Step: 20390... Loss: 1.1934... Val Loss: 1.6401\n",
            "Epoch: 40/50... Step: 20400... Loss: 1.1567... Val Loss: 1.6409\n",
            "Epoch: 40/50... Step: 20410... Loss: 1.2082... Val Loss: 1.6418\n",
            "Epoch: 40/50... Step: 20420... Loss: 1.1768... Val Loss: 1.6429\n",
            "Epoch: 40/50... Step: 20430... Loss: 1.1816... Val Loss: 1.6504\n",
            "Epoch: 40/50... Step: 20440... Loss: 1.1819... Val Loss: 1.6490\n",
            "Epoch: 40/50... Step: 20450... Loss: 1.2004... Val Loss: 1.6401\n",
            "Epoch: 40/50... Step: 20460... Loss: 1.1508... Val Loss: 1.6429\n",
            "Epoch: 40/50... Step: 20470... Loss: 1.1234... Val Loss: 1.6465\n",
            "Epoch: 40/50... Step: 20480... Loss: 1.1134... Val Loss: 1.6429\n",
            "Epoch: 40/50... Step: 20490... Loss: 1.1843... Val Loss: 1.6397\n",
            "Epoch: 40/50... Step: 20500... Loss: 1.1126... Val Loss: 1.6383\n",
            "Epoch: 40/50... Step: 20510... Loss: 1.1948... Val Loss: 1.6462\n",
            "Epoch: 40/50... Step: 20520... Loss: 1.2124... Val Loss: 1.6466\n",
            "Epoch: 40/50... Step: 20530... Loss: 1.1461... Val Loss: 1.6377\n",
            "Epoch: 40/50... Step: 20540... Loss: 1.1615... Val Loss: 1.6407\n",
            "Epoch: 40/50... Step: 20550... Loss: 1.1806... Val Loss: 1.6446\n",
            "Epoch: 40/50... Step: 20560... Loss: 1.1785... Val Loss: 1.6399\n",
            "Epoch: 40/50... Step: 20570... Loss: 1.1282... Val Loss: 1.6413\n",
            "Epoch: 40/50... Step: 20580... Loss: 1.1270... Val Loss: 1.6398\n",
            "Epoch: 40/50... Step: 20590... Loss: 1.1868... Val Loss: 1.6399\n",
            "Epoch: 40/50... Step: 20600... Loss: 1.0748... Val Loss: 1.6364\n",
            "Epoch: 40/50... Step: 20610... Loss: 1.1662... Val Loss: 1.6402\n",
            "Epoch: 40/50... Step: 20620... Loss: 1.2378... Val Loss: 1.6470\n",
            "Epoch: 40/50... Step: 20630... Loss: 1.1022... Val Loss: 1.6439\n",
            "Epoch: 40/50... Step: 20640... Loss: 1.1410... Val Loss: 1.6405\n",
            "Epoch: 40/50... Step: 20650... Loss: 1.1640... Val Loss: 1.6482\n",
            "Epoch: 40/50... Step: 20660... Loss: 1.2232... Val Loss: 1.6418\n",
            "Epoch: 40/50... Step: 20670... Loss: 1.1081... Val Loss: 1.6391\n",
            "Epoch: 40/50... Step: 20680... Loss: 1.1943... Val Loss: 1.6446\n",
            "Epoch: 40/50... Step: 20690... Loss: 1.1286... Val Loss: 1.6514\n",
            "Epoch: 40/50... Step: 20700... Loss: 1.1854... Val Loss: 1.6473\n",
            "Epoch: 40/50... Step: 20710... Loss: 1.1722... Val Loss: 1.6414\n",
            "Epoch: 40/50... Step: 20720... Loss: 1.2202... Val Loss: 1.6396\n",
            "Epoch: 40/50... Step: 20730... Loss: 1.1466... Val Loss: 1.6468\n",
            "Epoch: 40/50... Step: 20740... Loss: 1.1480... Val Loss: 1.6452\n",
            "Epoch: 40/50... Step: 20750... Loss: 1.1236... Val Loss: 1.6430\n",
            "Epoch: 40/50... Step: 20760... Loss: 1.1693... Val Loss: 1.6413\n",
            "Epoch: 40/50... Step: 20770... Loss: 1.2102... Val Loss: 1.6436\n",
            "Epoch: 40/50... Step: 20780... Loss: 1.1952... Val Loss: 1.6411\n",
            "Epoch: 40/50... Step: 20790... Loss: 1.1185... Val Loss: 1.6425\n",
            "Epoch: 40/50... Step: 20800... Loss: 1.4999... Val Loss: 1.6427\n",
            "Epoch: 41/50... Step: 20810... Loss: 1.1700... Val Loss: 1.6449\n",
            "Epoch: 41/50... Step: 20820... Loss: 1.1239... Val Loss: 1.6408\n",
            "Epoch: 41/50... Step: 20830... Loss: 1.1462... Val Loss: 1.6486\n",
            "Epoch: 41/50... Step: 20840... Loss: 1.1717... Val Loss: 1.6534\n",
            "Epoch: 41/50... Step: 20850... Loss: 1.1338... Val Loss: 1.6530\n",
            "Epoch: 41/50... Step: 20860... Loss: 1.1743... Val Loss: 1.6545\n",
            "Epoch: 41/50... Step: 20870... Loss: 1.1756... Val Loss: 1.6568\n",
            "Epoch: 41/50... Step: 20880... Loss: 1.0678... Val Loss: 1.6491\n",
            "Epoch: 41/50... Step: 20890... Loss: 1.1380... Val Loss: 1.6503\n",
            "Epoch: 41/50... Step: 20900... Loss: 1.1173... Val Loss: 1.6499\n",
            "Epoch: 41/50... Step: 20910... Loss: 1.1988... Val Loss: 1.6458\n",
            "Epoch: 41/50... Step: 20920... Loss: 1.1640... Val Loss: 1.6472\n",
            "Epoch: 41/50... Step: 20930... Loss: 1.2046... Val Loss: 1.6529\n",
            "Epoch: 41/50... Step: 20940... Loss: 1.1593... Val Loss: 1.6500\n",
            "Epoch: 41/50... Step: 20950... Loss: 1.1760... Val Loss: 1.6480\n",
            "Epoch: 41/50... Step: 20960... Loss: 1.1915... Val Loss: 1.6558\n",
            "Epoch: 41/50... Step: 20970... Loss: 1.2071... Val Loss: 1.6507\n",
            "Epoch: 41/50... Step: 20980... Loss: 1.1609... Val Loss: 1.6473\n",
            "Epoch: 41/50... Step: 20990... Loss: 1.1646... Val Loss: 1.6486\n",
            "Epoch: 41/50... Step: 21000... Loss: 1.1058... Val Loss: 1.6466\n",
            "Epoch: 41/50... Step: 21010... Loss: 1.2242... Val Loss: 1.6418\n",
            "Epoch: 41/50... Step: 21020... Loss: 1.1395... Val Loss: 1.6368\n",
            "Epoch: 41/50... Step: 21030... Loss: 1.1654... Val Loss: 1.6445\n",
            "Epoch: 41/50... Step: 21040... Loss: 1.2347... Val Loss: 1.6502\n",
            "Epoch: 41/50... Step: 21050... Loss: 1.1248... Val Loss: 1.6367\n",
            "Epoch: 41/50... Step: 21060... Loss: 1.1436... Val Loss: 1.6390\n",
            "Epoch: 41/50... Step: 21070... Loss: 1.1896... Val Loss: 1.6447\n",
            "Epoch: 41/50... Step: 21080... Loss: 1.1803... Val Loss: 1.6440\n",
            "Epoch: 41/50... Step: 21090... Loss: 1.1612... Val Loss: 1.6437\n",
            "Epoch: 41/50... Step: 21100... Loss: 1.1165... Val Loss: 1.6435\n",
            "Epoch: 41/50... Step: 21110... Loss: 1.1633... Val Loss: 1.6473\n",
            "Epoch: 41/50... Step: 21120... Loss: 1.1054... Val Loss: 1.6495\n",
            "Epoch: 41/50... Step: 21130... Loss: 1.1991... Val Loss: 1.6439\n",
            "Epoch: 41/50... Step: 21140... Loss: 1.2397... Val Loss: 1.6473\n",
            "Epoch: 41/50... Step: 21150... Loss: 1.1110... Val Loss: 1.6439\n",
            "Epoch: 41/50... Step: 21160... Loss: 1.1180... Val Loss: 1.6391\n",
            "Epoch: 41/50... Step: 21170... Loss: 1.2044... Val Loss: 1.6447\n",
            "Epoch: 41/50... Step: 21180... Loss: 1.1831... Val Loss: 1.6448\n",
            "Epoch: 41/50... Step: 21190... Loss: 1.1131... Val Loss: 1.6435\n",
            "Epoch: 41/50... Step: 21200... Loss: 1.1664... Val Loss: 1.6447\n",
            "Epoch: 41/50... Step: 21210... Loss: 1.1043... Val Loss: 1.6501\n",
            "Epoch: 41/50... Step: 21220... Loss: 1.1756... Val Loss: 1.6507\n",
            "Epoch: 41/50... Step: 21230... Loss: 1.1670... Val Loss: 1.6426\n",
            "Epoch: 41/50... Step: 21240... Loss: 1.1811... Val Loss: 1.6418\n",
            "Epoch: 41/50... Step: 21250... Loss: 1.1468... Val Loss: 1.6483\n",
            "Epoch: 41/50... Step: 21260... Loss: 1.1280... Val Loss: 1.6457\n",
            "Epoch: 41/50... Step: 21270... Loss: 1.1500... Val Loss: 1.6407\n",
            "Epoch: 41/50... Step: 21280... Loss: 1.1524... Val Loss: 1.6409\n",
            "Epoch: 41/50... Step: 21290... Loss: 1.1589... Val Loss: 1.6455\n",
            "Epoch: 41/50... Step: 21300... Loss: 1.1789... Val Loss: 1.6423\n",
            "Epoch: 41/50... Step: 21310... Loss: 1.1371... Val Loss: 1.6423\n",
            "Epoch: 41/50... Step: 21320... Loss: 1.5378... Val Loss: 1.6461\n",
            "Epoch: 42/50... Step: 21330... Loss: 1.1730... Val Loss: 1.6432\n",
            "Epoch: 42/50... Step: 21340... Loss: 1.1633... Val Loss: 1.6402\n",
            "Epoch: 42/50... Step: 21350... Loss: 1.1580... Val Loss: 1.6470\n",
            "Epoch: 42/50... Step: 21360... Loss: 1.1713... Val Loss: 1.6573\n",
            "Epoch: 42/50... Step: 21370... Loss: 1.1518... Val Loss: 1.6599\n",
            "Epoch: 42/50... Step: 21380... Loss: 1.1787... Val Loss: 1.6588\n",
            "Epoch: 42/50... Step: 21390... Loss: 1.1877... Val Loss: 1.6597\n",
            "Epoch: 42/50... Step: 21400... Loss: 1.0955... Val Loss: 1.6542\n",
            "Epoch: 42/50... Step: 21410... Loss: 1.1178... Val Loss: 1.6554\n",
            "Epoch: 42/50... Step: 21420... Loss: 1.1119... Val Loss: 1.6509\n",
            "Epoch: 42/50... Step: 21430... Loss: 1.1563... Val Loss: 1.6466\n",
            "Epoch: 42/50... Step: 21440... Loss: 1.1925... Val Loss: 1.6474\n",
            "Epoch: 42/50... Step: 21450... Loss: 1.2200... Val Loss: 1.6466\n",
            "Epoch: 42/50... Step: 21460... Loss: 1.1811... Val Loss: 1.6493\n",
            "Epoch: 42/50... Step: 21470... Loss: 1.1820... Val Loss: 1.6487\n",
            "Epoch: 42/50... Step: 21480... Loss: 1.1904... Val Loss: 1.6475\n",
            "Epoch: 42/50... Step: 21490... Loss: 1.1834... Val Loss: 1.6496\n",
            "Epoch: 42/50... Step: 21500... Loss: 1.1497... Val Loss: 1.6500\n",
            "Epoch: 42/50... Step: 21510... Loss: 1.1358... Val Loss: 1.6518\n",
            "Epoch: 42/50... Step: 21520... Loss: 1.0928... Val Loss: 1.6492\n",
            "Epoch: 42/50... Step: 21530... Loss: 1.1935... Val Loss: 1.6492\n",
            "Epoch: 42/50... Step: 21540... Loss: 1.1055... Val Loss: 1.6449\n",
            "Epoch: 42/50... Step: 21550... Loss: 1.1472... Val Loss: 1.6464\n",
            "Epoch: 42/50... Step: 21560... Loss: 1.2514... Val Loss: 1.6478\n",
            "Epoch: 42/50... Step: 21570... Loss: 1.1334... Val Loss: 1.6489\n",
            "Epoch: 42/50... Step: 21580... Loss: 1.1456... Val Loss: 1.6480\n",
            "Epoch: 42/50... Step: 21590... Loss: 1.1558... Val Loss: 1.6488\n",
            "Epoch: 42/50... Step: 21600... Loss: 1.1575... Val Loss: 1.6499\n",
            "Epoch: 42/50... Step: 21610... Loss: 1.1283... Val Loss: 1.6538\n",
            "Epoch: 42/50... Step: 21620... Loss: 1.1489... Val Loss: 1.6583\n",
            "Epoch: 42/50... Step: 21630... Loss: 1.1850... Val Loss: 1.6528\n",
            "Epoch: 42/50... Step: 21640... Loss: 1.1342... Val Loss: 1.6470\n",
            "Epoch: 42/50... Step: 21650... Loss: 1.1772... Val Loss: 1.6486\n",
            "Epoch: 42/50... Step: 21660... Loss: 1.2488... Val Loss: 1.6505\n",
            "Epoch: 42/50... Step: 21670... Loss: 1.0935... Val Loss: 1.6488\n",
            "Epoch: 42/50... Step: 21680... Loss: 1.1475... Val Loss: 1.6435\n",
            "Epoch: 42/50... Step: 21690... Loss: 1.2055... Val Loss: 1.6514\n",
            "Epoch: 42/50... Step: 21700... Loss: 1.1920... Val Loss: 1.6478\n",
            "Epoch: 42/50... Step: 21710... Loss: 1.1147... Val Loss: 1.6450\n",
            "Epoch: 42/50... Step: 21720... Loss: 1.1853... Val Loss: 1.6444\n",
            "Epoch: 42/50... Step: 21730... Loss: 1.1257... Val Loss: 1.6536\n",
            "Epoch: 42/50... Step: 21740... Loss: 1.1863... Val Loss: 1.6565\n",
            "Epoch: 42/50... Step: 21750... Loss: 1.1520... Val Loss: 1.6527\n",
            "Epoch: 42/50... Step: 21760... Loss: 1.1950... Val Loss: 1.6428\n",
            "Epoch: 42/50... Step: 21770... Loss: 1.1654... Val Loss: 1.6466\n",
            "Epoch: 42/50... Step: 21780... Loss: 1.1325... Val Loss: 1.6471\n",
            "Epoch: 42/50... Step: 21790... Loss: 1.1327... Val Loss: 1.6430\n",
            "Epoch: 42/50... Step: 21800... Loss: 1.1415... Val Loss: 1.6398\n",
            "Epoch: 42/50... Step: 21810... Loss: 1.1607... Val Loss: 1.6432\n",
            "Epoch: 42/50... Step: 21820... Loss: 1.1918... Val Loss: 1.6450\n",
            "Epoch: 42/50... Step: 21830... Loss: 1.1450... Val Loss: 1.6472\n",
            "Epoch: 42/50... Step: 21840... Loss: 1.5292... Val Loss: 1.6467\n",
            "Epoch: 43/50... Step: 21850... Loss: 1.1771... Val Loss: 1.6475\n",
            "Epoch: 43/50... Step: 21860... Loss: 1.1331... Val Loss: 1.6486\n",
            "Epoch: 43/50... Step: 21870... Loss: 1.1328... Val Loss: 1.6522\n",
            "Epoch: 43/50... Step: 21880... Loss: 1.1924... Val Loss: 1.6554\n",
            "Epoch: 43/50... Step: 21890... Loss: 1.1219... Val Loss: 1.6593\n",
            "Epoch: 43/50... Step: 21900... Loss: 1.1774... Val Loss: 1.6586\n",
            "Epoch: 43/50... Step: 21910... Loss: 1.1602... Val Loss: 1.6590\n",
            "Epoch: 43/50... Step: 21920... Loss: 1.0762... Val Loss: 1.6573\n",
            "Epoch: 43/50... Step: 21930... Loss: 1.1473... Val Loss: 1.6584\n",
            "Epoch: 43/50... Step: 21940... Loss: 1.1223... Val Loss: 1.6589\n",
            "Epoch: 43/50... Step: 21950... Loss: 1.1994... Val Loss: 1.6501\n",
            "Epoch: 43/50... Step: 21960... Loss: 1.1747... Val Loss: 1.6487\n",
            "Epoch: 43/50... Step: 21970... Loss: 1.1822... Val Loss: 1.6548\n",
            "Epoch: 43/50... Step: 21980... Loss: 1.1210... Val Loss: 1.6475\n",
            "Epoch: 43/50... Step: 21990... Loss: 1.1810... Val Loss: 1.6483\n",
            "Epoch: 43/50... Step: 22000... Loss: 1.1854... Val Loss: 1.6527\n",
            "Epoch: 43/50... Step: 22010... Loss: 1.2127... Val Loss: 1.6521\n",
            "Epoch: 43/50... Step: 22020... Loss: 1.1694... Val Loss: 1.6496\n",
            "Epoch: 43/50... Step: 22030... Loss: 1.1493... Val Loss: 1.6489\n",
            "Epoch: 43/50... Step: 22040... Loss: 1.1185... Val Loss: 1.6488\n",
            "Epoch: 43/50... Step: 22050... Loss: 1.1942... Val Loss: 1.6433\n",
            "Epoch: 43/50... Step: 22060... Loss: 1.1102... Val Loss: 1.6420\n",
            "Epoch: 43/50... Step: 22070... Loss: 1.1884... Val Loss: 1.6462\n",
            "Epoch: 43/50... Step: 22080... Loss: 1.2530... Val Loss: 1.6486\n",
            "Epoch: 43/50... Step: 22090... Loss: 1.1345... Val Loss: 1.6453\n",
            "Epoch: 43/50... Step: 22100... Loss: 1.1668... Val Loss: 1.6411\n",
            "Epoch: 43/50... Step: 22110... Loss: 1.1740... Val Loss: 1.6432\n",
            "Epoch: 43/50... Step: 22120... Loss: 1.1699... Val Loss: 1.6434\n",
            "Epoch: 43/50... Step: 22130... Loss: 1.1421... Val Loss: 1.6426\n",
            "Epoch: 43/50... Step: 22140... Loss: 1.1236... Val Loss: 1.6488\n",
            "Epoch: 43/50... Step: 22150... Loss: 1.1583... Val Loss: 1.6493\n",
            "Epoch: 43/50... Step: 22160... Loss: 1.1427... Val Loss: 1.6441\n",
            "Epoch: 43/50... Step: 22170... Loss: 1.1602... Val Loss: 1.6472\n",
            "Epoch: 43/50... Step: 22180... Loss: 1.2539... Val Loss: 1.6483\n",
            "Epoch: 43/50... Step: 22190... Loss: 1.0826... Val Loss: 1.6477\n",
            "Epoch: 43/50... Step: 22200... Loss: 1.1361... Val Loss: 1.6487\n",
            "Epoch: 43/50... Step: 22210... Loss: 1.2492... Val Loss: 1.6541\n",
            "Epoch: 43/50... Step: 22220... Loss: 1.2169... Val Loss: 1.6534\n",
            "Epoch: 43/50... Step: 22230... Loss: 1.1191... Val Loss: 1.6506\n",
            "Epoch: 43/50... Step: 22240... Loss: 1.1830... Val Loss: 1.6523\n",
            "Epoch: 43/50... Step: 22250... Loss: 1.1498... Val Loss: 1.6530\n",
            "Epoch: 43/50... Step: 22260... Loss: 1.1455... Val Loss: 1.6593\n",
            "Epoch: 43/50... Step: 22270... Loss: 1.1513... Val Loss: 1.6574\n",
            "Epoch: 43/50... Step: 22280... Loss: 1.1974... Val Loss: 1.6521\n",
            "Epoch: 43/50... Step: 22290... Loss: 1.1327... Val Loss: 1.6477\n",
            "Epoch: 43/50... Step: 22300... Loss: 1.1392... Val Loss: 1.6463\n",
            "Epoch: 43/50... Step: 22310... Loss: 1.1272... Val Loss: 1.6510\n",
            "Epoch: 43/50... Step: 22320... Loss: 1.1426... Val Loss: 1.6529\n",
            "Epoch: 43/50... Step: 22330... Loss: 1.1965... Val Loss: 1.6493\n",
            "Epoch: 43/50... Step: 22340... Loss: 1.1933... Val Loss: 1.6406\n",
            "Epoch: 43/50... Step: 22350... Loss: 1.1437... Val Loss: 1.6409\n",
            "Epoch: 43/50... Step: 22360... Loss: 1.5074... Val Loss: 1.6532\n",
            "Epoch: 44/50... Step: 22370... Loss: 1.1411... Val Loss: 1.6497\n",
            "Epoch: 44/50... Step: 22380... Loss: 1.1337... Val Loss: 1.6496\n",
            "Epoch: 44/50... Step: 22390... Loss: 1.1526... Val Loss: 1.6530\n",
            "Epoch: 44/50... Step: 22400... Loss: 1.1674... Val Loss: 1.6562\n",
            "Epoch: 44/50... Step: 22410... Loss: 1.1144... Val Loss: 1.6580\n",
            "Epoch: 44/50... Step: 22420... Loss: 1.1512... Val Loss: 1.6593\n",
            "Epoch: 44/50... Step: 22430... Loss: 1.1660... Val Loss: 1.6553\n",
            "Epoch: 44/50... Step: 22440... Loss: 1.1015... Val Loss: 1.6528\n",
            "Epoch: 44/50... Step: 22450... Loss: 1.1439... Val Loss: 1.6507\n",
            "Epoch: 44/50... Step: 22460... Loss: 1.1047... Val Loss: 1.6481\n",
            "Epoch: 44/50... Step: 22470... Loss: 1.1867... Val Loss: 1.6457\n",
            "Epoch: 44/50... Step: 22480... Loss: 1.1582... Val Loss: 1.6474\n",
            "Epoch: 44/50... Step: 22490... Loss: 1.2062... Val Loss: 1.6514\n",
            "Epoch: 44/50... Step: 22500... Loss: 1.1804... Val Loss: 1.6510\n",
            "Epoch: 44/50... Step: 22510... Loss: 1.1623... Val Loss: 1.6530\n",
            "Epoch: 44/50... Step: 22520... Loss: 1.1653... Val Loss: 1.6554\n",
            "Epoch: 44/50... Step: 22530... Loss: 1.2060... Val Loss: 1.6471\n",
            "Epoch: 44/50... Step: 22540... Loss: 1.1515... Val Loss: 1.6453\n",
            "Epoch: 44/50... Step: 22550... Loss: 1.1485... Val Loss: 1.6546\n",
            "Epoch: 44/50... Step: 22560... Loss: 1.1116... Val Loss: 1.6488\n",
            "Epoch: 44/50... Step: 22570... Loss: 1.2263... Val Loss: 1.6485\n",
            "Epoch: 44/50... Step: 22580... Loss: 1.1052... Val Loss: 1.6431\n",
            "Epoch: 44/50... Step: 22590... Loss: 1.1678... Val Loss: 1.6509\n",
            "Epoch: 44/50... Step: 22600... Loss: 1.2211... Val Loss: 1.6561\n",
            "Epoch: 44/50... Step: 22610... Loss: 1.1616... Val Loss: 1.6425\n",
            "Epoch: 44/50... Step: 22620... Loss: 1.1700... Val Loss: 1.6460\n",
            "Epoch: 44/50... Step: 22630... Loss: 1.1847... Val Loss: 1.6516\n",
            "Epoch: 44/50... Step: 22640... Loss: 1.1754... Val Loss: 1.6459\n",
            "Epoch: 44/50... Step: 22650... Loss: 1.1431... Val Loss: 1.6459\n",
            "Epoch: 44/50... Step: 22660... Loss: 1.1221... Val Loss: 1.6539\n",
            "Epoch: 44/50... Step: 22670... Loss: 1.1721... Val Loss: 1.6556\n",
            "Epoch: 44/50... Step: 22680... Loss: 1.0841... Val Loss: 1.6495\n",
            "Epoch: 44/50... Step: 22690... Loss: 1.1293... Val Loss: 1.6536\n",
            "Epoch: 44/50... Step: 22700... Loss: 1.2413... Val Loss: 1.6526\n",
            "Epoch: 44/50... Step: 22710... Loss: 1.0790... Val Loss: 1.6488\n",
            "Epoch: 44/50... Step: 22720... Loss: 1.1261... Val Loss: 1.6486\n",
            "Epoch: 44/50... Step: 22730... Loss: 1.1998... Val Loss: 1.6608\n",
            "Epoch: 44/50... Step: 22740... Loss: 1.1724... Val Loss: 1.6558\n",
            "Epoch: 44/50... Step: 22750... Loss: 1.0980... Val Loss: 1.6516\n",
            "Epoch: 44/50... Step: 22760... Loss: 1.1831... Val Loss: 1.6544\n",
            "Epoch: 44/50... Step: 22770... Loss: 1.1150... Val Loss: 1.6579\n",
            "Epoch: 44/50... Step: 22780... Loss: 1.1664... Val Loss: 1.6540\n",
            "Epoch: 44/50... Step: 22790... Loss: 1.1260... Val Loss: 1.6527\n",
            "Epoch: 44/50... Step: 22800... Loss: 1.1695... Val Loss: 1.6486\n",
            "Epoch: 44/50... Step: 22810... Loss: 1.1268... Val Loss: 1.6521\n",
            "Epoch: 44/50... Step: 22820... Loss: 1.1303... Val Loss: 1.6525\n",
            "Epoch: 44/50... Step: 22830... Loss: 1.0989... Val Loss: 1.6492\n",
            "Epoch: 44/50... Step: 22840... Loss: 1.1444... Val Loss: 1.6478\n",
            "Epoch: 44/50... Step: 22850... Loss: 1.1815... Val Loss: 1.6533\n",
            "Epoch: 44/50... Step: 22860... Loss: 1.1908... Val Loss: 1.6503\n",
            "Epoch: 44/50... Step: 22870... Loss: 1.1443... Val Loss: 1.6553\n",
            "Epoch: 44/50... Step: 22880... Loss: 1.5041... Val Loss: 1.6572\n",
            "Epoch: 45/50... Step: 22890... Loss: 1.1490... Val Loss: 1.6542\n",
            "Epoch: 45/50... Step: 22900... Loss: 1.1558... Val Loss: 1.6490\n",
            "Epoch: 45/50... Step: 22910... Loss: 1.1524... Val Loss: 1.6583\n",
            "Epoch: 45/50... Step: 22920... Loss: 1.1887... Val Loss: 1.6625\n",
            "Epoch: 45/50... Step: 22930... Loss: 1.1300... Val Loss: 1.6595\n",
            "Epoch: 45/50... Step: 22940... Loss: 1.1620... Val Loss: 1.6613\n",
            "Epoch: 45/50... Step: 22950... Loss: 1.1635... Val Loss: 1.6641\n",
            "Epoch: 45/50... Step: 22960... Loss: 1.0830... Val Loss: 1.6552\n",
            "Epoch: 45/50... Step: 22970... Loss: 1.1496... Val Loss: 1.6503\n",
            "Epoch: 45/50... Step: 22980... Loss: 1.1075... Val Loss: 1.6512\n",
            "Epoch: 45/50... Step: 22990... Loss: 1.1722... Val Loss: 1.6477\n",
            "Epoch: 45/50... Step: 23000... Loss: 1.1601... Val Loss: 1.6491\n",
            "Epoch: 45/50... Step: 23010... Loss: 1.1964... Val Loss: 1.6528\n",
            "Epoch: 45/50... Step: 23020... Loss: 1.1737... Val Loss: 1.6509\n",
            "Epoch: 45/50... Step: 23030... Loss: 1.1862... Val Loss: 1.6513\n",
            "Epoch: 45/50... Step: 23040... Loss: 1.1763... Val Loss: 1.6514\n",
            "Epoch: 45/50... Step: 23050... Loss: 1.2159... Val Loss: 1.6476\n",
            "Epoch: 45/50... Step: 23060... Loss: 1.1657... Val Loss: 1.6480\n",
            "Epoch: 45/50... Step: 23070... Loss: 1.1454... Val Loss: 1.6459\n",
            "Epoch: 45/50... Step: 23080... Loss: 1.0905... Val Loss: 1.6449\n",
            "Epoch: 45/50... Step: 23090... Loss: 1.2173... Val Loss: 1.6497\n",
            "Epoch: 45/50... Step: 23100... Loss: 1.1231... Val Loss: 1.6497\n",
            "Epoch: 45/50... Step: 23110... Loss: 1.1507... Val Loss: 1.6527\n",
            "Epoch: 45/50... Step: 23120... Loss: 1.2016... Val Loss: 1.6580\n",
            "Epoch: 45/50... Step: 23130... Loss: 1.1473... Val Loss: 1.6523\n",
            "Epoch: 45/50... Step: 23140... Loss: 1.1297... Val Loss: 1.6529\n",
            "Epoch: 45/50... Step: 23150... Loss: 1.1844... Val Loss: 1.6550\n",
            "Epoch: 45/50... Step: 23160... Loss: 1.1576... Val Loss: 1.6519\n",
            "Epoch: 45/50... Step: 23170... Loss: 1.1494... Val Loss: 1.6517\n",
            "Epoch: 45/50... Step: 23180... Loss: 1.0873... Val Loss: 1.6567\n",
            "Epoch: 45/50... Step: 23190... Loss: 1.1386... Val Loss: 1.6538\n",
            "Epoch: 45/50... Step: 23200... Loss: 1.1242... Val Loss: 1.6524\n",
            "Epoch: 45/50... Step: 23210... Loss: 1.1391... Val Loss: 1.6594\n",
            "Epoch: 45/50... Step: 23220... Loss: 1.2733... Val Loss: 1.6602\n",
            "Epoch: 45/50... Step: 23230... Loss: 1.1103... Val Loss: 1.6534\n",
            "Epoch: 45/50... Step: 23240... Loss: 1.1458... Val Loss: 1.6561\n",
            "Epoch: 45/50... Step: 23250... Loss: 1.1519... Val Loss: 1.6637\n",
            "Epoch: 45/50... Step: 23260... Loss: 1.2008... Val Loss: 1.6573\n",
            "Epoch: 45/50... Step: 23270... Loss: 1.0804... Val Loss: 1.6514\n",
            "Epoch: 45/50... Step: 23280... Loss: 1.1396... Val Loss: 1.6558\n",
            "Epoch: 45/50... Step: 23290... Loss: 1.1329... Val Loss: 1.6573\n",
            "Epoch: 45/50... Step: 23300... Loss: 1.1550... Val Loss: 1.6587\n",
            "Epoch: 45/50... Step: 23310... Loss: 1.1532... Val Loss: 1.6583\n",
            "Epoch: 45/50... Step: 23320... Loss: 1.1972... Val Loss: 1.6553\n",
            "Epoch: 45/50... Step: 23330... Loss: 1.1320... Val Loss: 1.6554\n",
            "Epoch: 45/50... Step: 23340... Loss: 1.1388... Val Loss: 1.6573\n",
            "Epoch: 45/50... Step: 23350... Loss: 1.1522... Val Loss: 1.6542\n",
            "Epoch: 45/50... Step: 23360... Loss: 1.1324... Val Loss: 1.6530\n",
            "Epoch: 45/50... Step: 23370... Loss: 1.1794... Val Loss: 1.6532\n",
            "Epoch: 45/50... Step: 23380... Loss: 1.1913... Val Loss: 1.6460\n",
            "Epoch: 45/50... Step: 23390... Loss: 1.1261... Val Loss: 1.6504\n",
            "Epoch: 45/50... Step: 23400... Loss: 1.4930... Val Loss: 1.6557\n",
            "Epoch: 46/50... Step: 23410... Loss: 1.1453... Val Loss: 1.6564\n",
            "Epoch: 46/50... Step: 23420... Loss: 1.1704... Val Loss: 1.6518\n",
            "Epoch: 46/50... Step: 23430... Loss: 1.1556... Val Loss: 1.6560\n",
            "Epoch: 46/50... Step: 23440... Loss: 1.1364... Val Loss: 1.6627\n",
            "Epoch: 46/50... Step: 23450... Loss: 1.1169... Val Loss: 1.6659\n",
            "Epoch: 46/50... Step: 23460... Loss: 1.1721... Val Loss: 1.6643\n",
            "Epoch: 46/50... Step: 23470... Loss: 1.1468... Val Loss: 1.6587\n",
            "Epoch: 46/50... Step: 23480... Loss: 1.0410... Val Loss: 1.6572\n",
            "Epoch: 46/50... Step: 23490... Loss: 1.1237... Val Loss: 1.6561\n",
            "Epoch: 46/50... Step: 23500... Loss: 1.1275... Val Loss: 1.6532\n",
            "Epoch: 46/50... Step: 23510... Loss: 1.1849... Val Loss: 1.6502\n",
            "Epoch: 46/50... Step: 23520... Loss: 1.1855... Val Loss: 1.6490\n",
            "Epoch: 46/50... Step: 23530... Loss: 1.2050... Val Loss: 1.6534\n",
            "Epoch: 46/50... Step: 23540... Loss: 1.1652... Val Loss: 1.6541\n",
            "Epoch: 46/50... Step: 23550... Loss: 1.1649... Val Loss: 1.6554\n",
            "Epoch: 46/50... Step: 23560... Loss: 1.1896... Val Loss: 1.6573\n",
            "Epoch: 46/50... Step: 23570... Loss: 1.2057... Val Loss: 1.6539\n",
            "Epoch: 46/50... Step: 23580... Loss: 1.1678... Val Loss: 1.6505\n",
            "Epoch: 46/50... Step: 23590... Loss: 1.1515... Val Loss: 1.6499\n",
            "Epoch: 46/50... Step: 23600... Loss: 1.0959... Val Loss: 1.6498\n",
            "Epoch: 46/50... Step: 23610... Loss: 1.2121... Val Loss: 1.6477\n",
            "Epoch: 46/50... Step: 23620... Loss: 1.1142... Val Loss: 1.6506\n",
            "Epoch: 46/50... Step: 23630... Loss: 1.1855... Val Loss: 1.6524\n",
            "Epoch: 46/50... Step: 23640... Loss: 1.2146... Val Loss: 1.6505\n",
            "Epoch: 46/50... Step: 23650... Loss: 1.1014... Val Loss: 1.6500\n",
            "Epoch: 46/50... Step: 23660... Loss: 1.1461... Val Loss: 1.6484\n",
            "Epoch: 46/50... Step: 23670... Loss: 1.1550... Val Loss: 1.6501\n",
            "Epoch: 46/50... Step: 23680... Loss: 1.1469... Val Loss: 1.6540\n",
            "Epoch: 46/50... Step: 23690... Loss: 1.1308... Val Loss: 1.6593\n",
            "Epoch: 46/50... Step: 23700... Loss: 1.1234... Val Loss: 1.6558\n",
            "Epoch: 46/50... Step: 23710... Loss: 1.1773... Val Loss: 1.6523\n",
            "Epoch: 46/50... Step: 23720... Loss: 1.1116... Val Loss: 1.6537\n",
            "Epoch: 46/50... Step: 23730... Loss: 1.1725... Val Loss: 1.6565\n",
            "Epoch: 46/50... Step: 23740... Loss: 1.2219... Val Loss: 1.6561\n",
            "Epoch: 46/50... Step: 23750... Loss: 1.1127... Val Loss: 1.6568\n",
            "Epoch: 46/50... Step: 23760... Loss: 1.1482... Val Loss: 1.6560\n",
            "Epoch: 46/50... Step: 23770... Loss: 1.1789... Val Loss: 1.6596\n",
            "Epoch: 46/50... Step: 23780... Loss: 1.2281... Val Loss: 1.6514\n",
            "Epoch: 46/50... Step: 23790... Loss: 1.1147... Val Loss: 1.6496\n",
            "Epoch: 46/50... Step: 23800... Loss: 1.1747... Val Loss: 1.6499\n",
            "Epoch: 46/50... Step: 23810... Loss: 1.1347... Val Loss: 1.6556\n",
            "Epoch: 46/50... Step: 23820... Loss: 1.1846... Val Loss: 1.6626\n",
            "Epoch: 46/50... Step: 23830... Loss: 1.1486... Val Loss: 1.6612\n",
            "Epoch: 46/50... Step: 23840... Loss: 1.1953... Val Loss: 1.6556\n",
            "Epoch: 46/50... Step: 23850... Loss: 1.1293... Val Loss: 1.6573\n",
            "Epoch: 46/50... Step: 23860... Loss: 1.1346... Val Loss: 1.6570\n",
            "Epoch: 46/50... Step: 23870... Loss: 1.1026... Val Loss: 1.6524\n",
            "Epoch: 46/50... Step: 23880... Loss: 1.1405... Val Loss: 1.6554\n",
            "Epoch: 46/50... Step: 23890... Loss: 1.1582... Val Loss: 1.6589\n",
            "Epoch: 46/50... Step: 23900... Loss: 1.1992... Val Loss: 1.6535\n",
            "Epoch: 46/50... Step: 23910... Loss: 1.1286... Val Loss: 1.6533\n",
            "Epoch: 46/50... Step: 23920... Loss: 1.4772... Val Loss: 1.6598\n",
            "Epoch: 47/50... Step: 23930... Loss: 1.1515... Val Loss: 1.6672\n",
            "Epoch: 47/50... Step: 23940... Loss: 1.1212... Val Loss: 1.6571\n",
            "Epoch: 47/50... Step: 23950... Loss: 1.1588... Val Loss: 1.6566\n",
            "Epoch: 47/50... Step: 23960... Loss: 1.1369... Val Loss: 1.6617\n",
            "Epoch: 47/50... Step: 23970... Loss: 1.1215... Val Loss: 1.6690\n",
            "Epoch: 47/50... Step: 23980... Loss: 1.1609... Val Loss: 1.6683\n",
            "Epoch: 47/50... Step: 23990... Loss: 1.1641... Val Loss: 1.6688\n",
            "Epoch: 47/50... Step: 24000... Loss: 1.0800... Val Loss: 1.6679\n",
            "Epoch: 47/50... Step: 24010... Loss: 1.1947... Val Loss: 1.6610\n",
            "Epoch: 47/50... Step: 24020... Loss: 1.1065... Val Loss: 1.6574\n",
            "Epoch: 47/50... Step: 24030... Loss: 1.1942... Val Loss: 1.6564\n",
            "Epoch: 47/50... Step: 24040... Loss: 1.1708... Val Loss: 1.6558\n",
            "Epoch: 47/50... Step: 24050... Loss: 1.1851... Val Loss: 1.6589\n",
            "Epoch: 47/50... Step: 24060... Loss: 1.1560... Val Loss: 1.6601\n",
            "Epoch: 47/50... Step: 24070... Loss: 1.1274... Val Loss: 1.6574\n",
            "Epoch: 47/50... Step: 24080... Loss: 1.1531... Val Loss: 1.6617\n",
            "Epoch: 47/50... Step: 24090... Loss: 1.2331... Val Loss: 1.6612\n",
            "Epoch: 47/50... Step: 24100... Loss: 1.1442... Val Loss: 1.6612\n",
            "Epoch: 47/50... Step: 24110... Loss: 1.1499... Val Loss: 1.6552\n",
            "Epoch: 47/50... Step: 24120... Loss: 1.0887... Val Loss: 1.6567\n",
            "Epoch: 47/50... Step: 24130... Loss: 1.2097... Val Loss: 1.6584\n",
            "Epoch: 47/50... Step: 24140... Loss: 1.1522... Val Loss: 1.6589\n",
            "Epoch: 47/50... Step: 24150... Loss: 1.1400... Val Loss: 1.6579\n",
            "Epoch: 47/50... Step: 24160... Loss: 1.2176... Val Loss: 1.6603\n",
            "Epoch: 47/50... Step: 24170... Loss: 1.1306... Val Loss: 1.6586\n",
            "Epoch: 47/50... Step: 24180... Loss: 1.1526... Val Loss: 1.6599\n",
            "Epoch: 47/50... Step: 24190... Loss: 1.1535... Val Loss: 1.6605\n",
            "Epoch: 47/50... Step: 24200... Loss: 1.1753... Val Loss: 1.6620\n",
            "Epoch: 47/50... Step: 24210... Loss: 1.1412... Val Loss: 1.6604\n",
            "Epoch: 47/50... Step: 24220... Loss: 1.1272... Val Loss: 1.6559\n",
            "Epoch: 47/50... Step: 24230... Loss: 1.1318... Val Loss: 1.6594\n",
            "Epoch: 47/50... Step: 24240... Loss: 1.0950... Val Loss: 1.6638\n",
            "Epoch: 47/50... Step: 24250... Loss: 1.1689... Val Loss: 1.6572\n",
            "Epoch: 47/50... Step: 24260... Loss: 1.2327... Val Loss: 1.6557\n",
            "Epoch: 47/50... Step: 24270... Loss: 1.1234... Val Loss: 1.6588\n",
            "Epoch: 47/50... Step: 24280... Loss: 1.1302... Val Loss: 1.6617\n",
            "Epoch: 47/50... Step: 24290... Loss: 1.1998... Val Loss: 1.6671\n",
            "Epoch: 47/50... Step: 24300... Loss: 1.1883... Val Loss: 1.6620\n",
            "Epoch: 47/50... Step: 24310... Loss: 1.0801... Val Loss: 1.6607\n",
            "Epoch: 47/50... Step: 24320... Loss: 1.1639... Val Loss: 1.6586\n",
            "Epoch: 47/50... Step: 24330... Loss: 1.1142... Val Loss: 1.6571\n",
            "Epoch: 47/50... Step: 24340... Loss: 1.1400... Val Loss: 1.6562\n",
            "Epoch: 47/50... Step: 24350... Loss: 1.1640... Val Loss: 1.6620\n",
            "Epoch: 47/50... Step: 24360... Loss: 1.1898... Val Loss: 1.6584\n",
            "Epoch: 47/50... Step: 24370... Loss: 1.1164... Val Loss: 1.6575\n",
            "Epoch: 47/50... Step: 24380... Loss: 1.1220... Val Loss: 1.6585\n",
            "Epoch: 47/50... Step: 24390... Loss: 1.1175... Val Loss: 1.6613\n",
            "Epoch: 47/50... Step: 24400... Loss: 1.1888... Val Loss: 1.6534\n",
            "Epoch: 47/50... Step: 24410... Loss: 1.1830... Val Loss: 1.6558\n",
            "Epoch: 47/50... Step: 24420... Loss: 1.1312... Val Loss: 1.6645\n",
            "Epoch: 47/50... Step: 24430... Loss: 1.1088... Val Loss: 1.6656\n",
            "Epoch: 47/50... Step: 24440... Loss: 1.5190... Val Loss: 1.6668\n",
            "Epoch: 48/50... Step: 24450... Loss: 1.1714... Val Loss: 1.6639\n",
            "Epoch: 48/50... Step: 24460... Loss: 1.1095... Val Loss: 1.6646\n",
            "Epoch: 48/50... Step: 24470... Loss: 1.1369... Val Loss: 1.6664\n",
            "Epoch: 48/50... Step: 24480... Loss: 1.1406... Val Loss: 1.6666\n",
            "Epoch: 48/50... Step: 24490... Loss: 1.1080... Val Loss: 1.6742\n",
            "Epoch: 48/50... Step: 24500... Loss: 1.1477... Val Loss: 1.6672\n",
            "Epoch: 48/50... Step: 24510... Loss: 1.1688... Val Loss: 1.6706\n",
            "Epoch: 48/50... Step: 24520... Loss: 1.0802... Val Loss: 1.6717\n",
            "Epoch: 48/50... Step: 24530... Loss: 1.1441... Val Loss: 1.6685\n",
            "Epoch: 48/50... Step: 24540... Loss: 1.1122... Val Loss: 1.6646\n",
            "Epoch: 48/50... Step: 24550... Loss: 1.1674... Val Loss: 1.6623\n",
            "Epoch: 48/50... Step: 24560... Loss: 1.1342... Val Loss: 1.6597\n",
            "Epoch: 48/50... Step: 24570... Loss: 1.1624... Val Loss: 1.6608\n",
            "Epoch: 48/50... Step: 24580... Loss: 1.1815... Val Loss: 1.6647\n",
            "Epoch: 48/50... Step: 24590... Loss: 1.1380... Val Loss: 1.6674\n",
            "Epoch: 48/50... Step: 24600... Loss: 1.1568... Val Loss: 1.6701\n",
            "Epoch: 48/50... Step: 24610... Loss: 1.2037... Val Loss: 1.6648\n",
            "Epoch: 48/50... Step: 24620... Loss: 1.1657... Val Loss: 1.6614\n",
            "Epoch: 48/50... Step: 24630... Loss: 1.1338... Val Loss: 1.6538\n",
            "Epoch: 48/50... Step: 24640... Loss: 1.0893... Val Loss: 1.6565\n",
            "Epoch: 48/50... Step: 24650... Loss: 1.1795... Val Loss: 1.6593\n",
            "Epoch: 48/50... Step: 24660... Loss: 1.1486... Val Loss: 1.6576\n",
            "Epoch: 48/50... Step: 24670... Loss: 1.1653... Val Loss: 1.6614\n",
            "Epoch: 48/50... Step: 24680... Loss: 1.1917... Val Loss: 1.6610\n",
            "Epoch: 48/50... Step: 24690... Loss: 1.1388... Val Loss: 1.6564\n",
            "Epoch: 48/50... Step: 24700... Loss: 1.1269... Val Loss: 1.6542\n",
            "Epoch: 48/50... Step: 24710... Loss: 1.1673... Val Loss: 1.6570\n",
            "Epoch: 48/50... Step: 24720... Loss: 1.1545... Val Loss: 1.6586\n",
            "Epoch: 48/50... Step: 24730... Loss: 1.1148... Val Loss: 1.6555\n",
            "Epoch: 48/50... Step: 24740... Loss: 1.0864... Val Loss: 1.6563\n",
            "Epoch: 48/50... Step: 24750... Loss: 1.1478... Val Loss: 1.6586\n",
            "Epoch: 48/50... Step: 24760... Loss: 1.1108... Val Loss: 1.6569\n",
            "Epoch: 48/50... Step: 24770... Loss: 1.1881... Val Loss: 1.6583\n",
            "Epoch: 48/50... Step: 24780... Loss: 1.2063... Val Loss: 1.6603\n",
            "Epoch: 48/50... Step: 24790... Loss: 1.0902... Val Loss: 1.6587\n",
            "Epoch: 48/50... Step: 24800... Loss: 1.1355... Val Loss: 1.6625\n",
            "Epoch: 48/50... Step: 24810... Loss: 1.2034... Val Loss: 1.6664\n",
            "Epoch: 48/50... Step: 24820... Loss: 1.2137... Val Loss: 1.6556\n",
            "Epoch: 48/50... Step: 24830... Loss: 1.1153... Val Loss: 1.6516\n",
            "Epoch: 48/50... Step: 24840... Loss: 1.1645... Val Loss: 1.6546\n",
            "Epoch: 48/50... Step: 24850... Loss: 1.1117... Val Loss: 1.6617\n",
            "Epoch: 48/50... Step: 24860... Loss: 1.1497... Val Loss: 1.6684\n",
            "Epoch: 48/50... Step: 24870... Loss: 1.1531... Val Loss: 1.6627\n",
            "Epoch: 48/50... Step: 24880... Loss: 1.1856... Val Loss: 1.6552\n",
            "Epoch: 48/50... Step: 24890... Loss: 1.1089... Val Loss: 1.6545\n",
            "Epoch: 48/50... Step: 24900... Loss: 1.1115... Val Loss: 1.6586\n",
            "Epoch: 48/50... Step: 24910... Loss: 1.1158... Val Loss: 1.6618\n",
            "Epoch: 48/50... Step: 24920... Loss: 1.1291... Val Loss: 1.6552\n",
            "Epoch: 48/50... Step: 24930... Loss: 1.1778... Val Loss: 1.6522\n",
            "Epoch: 48/50... Step: 24940... Loss: 1.1997... Val Loss: 1.6555\n",
            "Epoch: 48/50... Step: 24950... Loss: 1.1192... Val Loss: 1.6604\n",
            "Epoch: 48/50... Step: 24960... Loss: 1.4834... Val Loss: 1.6664\n",
            "Epoch: 49/50... Step: 24970... Loss: 1.1400... Val Loss: 1.6627\n",
            "Epoch: 49/50... Step: 24980... Loss: 1.1247... Val Loss: 1.6534\n",
            "Epoch: 49/50... Step: 24990... Loss: 1.1140... Val Loss: 1.6612\n",
            "Epoch: 49/50... Step: 25000... Loss: 1.1643... Val Loss: 1.6702\n",
            "Epoch: 49/50... Step: 25010... Loss: 1.1046... Val Loss: 1.6716\n",
            "Epoch: 49/50... Step: 25020... Loss: 1.1478... Val Loss: 1.6710\n",
            "Epoch: 49/50... Step: 25030... Loss: 1.1567... Val Loss: 1.6709\n",
            "Epoch: 49/50... Step: 25040... Loss: 1.0847... Val Loss: 1.6602\n",
            "Epoch: 49/50... Step: 25050... Loss: 1.1385... Val Loss: 1.6530\n",
            "Epoch: 49/50... Step: 25060... Loss: 1.1293... Val Loss: 1.6566\n",
            "Epoch: 49/50... Step: 25070... Loss: 1.1845... Val Loss: 1.6538\n",
            "Epoch: 49/50... Step: 25080... Loss: 1.1506... Val Loss: 1.6508\n",
            "Epoch: 49/50... Step: 25090... Loss: 1.2520... Val Loss: 1.6602\n",
            "Epoch: 49/50... Step: 25100... Loss: 1.1700... Val Loss: 1.6585\n",
            "Epoch: 49/50... Step: 25110... Loss: 1.1725... Val Loss: 1.6575\n",
            "Epoch: 49/50... Step: 25120... Loss: 1.1421... Val Loss: 1.6616\n",
            "Epoch: 49/50... Step: 25130... Loss: 1.1941... Val Loss: 1.6587\n",
            "Epoch: 49/50... Step: 25140... Loss: 1.1375... Val Loss: 1.6555\n",
            "Epoch: 49/50... Step: 25150... Loss: 1.1338... Val Loss: 1.6575\n",
            "Epoch: 49/50... Step: 25160... Loss: 1.1193... Val Loss: 1.6573\n",
            "Epoch: 49/50... Step: 25170... Loss: 1.2006... Val Loss: 1.6570\n",
            "Epoch: 49/50... Step: 25180... Loss: 1.1619... Val Loss: 1.6501\n",
            "Epoch: 49/50... Step: 25190... Loss: 1.1335... Val Loss: 1.6561\n",
            "Epoch: 49/50... Step: 25200... Loss: 1.1901... Val Loss: 1.6549\n",
            "Epoch: 49/50... Step: 25210... Loss: 1.1130... Val Loss: 1.6579\n",
            "Epoch: 49/50... Step: 25220... Loss: 1.1512... Val Loss: 1.6547\n",
            "Epoch: 49/50... Step: 25230... Loss: 1.2126... Val Loss: 1.6539\n",
            "Epoch: 49/50... Step: 25240... Loss: 1.1924... Val Loss: 1.6605\n",
            "Epoch: 49/50... Step: 25250... Loss: 1.1244... Val Loss: 1.6638\n",
            "Epoch: 49/50... Step: 25260... Loss: 1.1099... Val Loss: 1.6643\n",
            "Epoch: 49/50... Step: 25270... Loss: 1.1717... Val Loss: 1.6650\n",
            "Epoch: 49/50... Step: 25280... Loss: 1.0865... Val Loss: 1.6602\n",
            "Epoch: 49/50... Step: 25290... Loss: 1.1573... Val Loss: 1.6561\n",
            "Epoch: 49/50... Step: 25300... Loss: 1.2243... Val Loss: 1.6630\n",
            "Epoch: 49/50... Step: 25310... Loss: 1.0769... Val Loss: 1.6554\n",
            "Epoch: 49/50... Step: 25320... Loss: 1.1473... Val Loss: 1.6575\n",
            "Epoch: 49/50... Step: 25330... Loss: 1.1536... Val Loss: 1.6661\n",
            "Epoch: 49/50... Step: 25340... Loss: 1.1903... Val Loss: 1.6549\n",
            "Epoch: 49/50... Step: 25350... Loss: 1.0917... Val Loss: 1.6508\n",
            "Epoch: 49/50... Step: 25360... Loss: 1.1660... Val Loss: 1.6552\n",
            "Epoch: 49/50... Step: 25370... Loss: 1.1224... Val Loss: 1.6611\n",
            "Epoch: 49/50... Step: 25380... Loss: 1.1379... Val Loss: 1.6620\n",
            "Epoch: 49/50... Step: 25390... Loss: 1.1411... Val Loss: 1.6622\n",
            "Epoch: 49/50... Step: 25400... Loss: 1.1868... Val Loss: 1.6621\n",
            "Epoch: 49/50... Step: 25410... Loss: 1.1453... Val Loss: 1.6607\n",
            "Epoch: 49/50... Step: 25420... Loss: 1.1104... Val Loss: 1.6592\n",
            "Epoch: 49/50... Step: 25430... Loss: 1.1323... Val Loss: 1.6639\n",
            "Epoch: 49/50... Step: 25440... Loss: 1.1562... Val Loss: 1.6628\n",
            "Epoch: 49/50... Step: 25450... Loss: 1.1763... Val Loss: 1.6564\n",
            "Epoch: 49/50... Step: 25460... Loss: 1.1396... Val Loss: 1.6581\n",
            "Epoch: 49/50... Step: 25470... Loss: 1.1123... Val Loss: 1.6638\n",
            "Epoch: 49/50... Step: 25480... Loss: 1.4872... Val Loss: 1.6671\n",
            "Epoch: 50/50... Step: 25490... Loss: 1.1532... Val Loss: 1.6642\n",
            "Epoch: 50/50... Step: 25500... Loss: 1.1601... Val Loss: 1.6656\n",
            "Epoch: 50/50... Step: 25510... Loss: 1.1206... Val Loss: 1.6669\n",
            "Epoch: 50/50... Step: 25520... Loss: 1.1592... Val Loss: 1.6692\n",
            "Epoch: 50/50... Step: 25530... Loss: 1.1107... Val Loss: 1.6747\n",
            "Epoch: 50/50... Step: 25540... Loss: 1.1709... Val Loss: 1.6736\n",
            "Epoch: 50/50... Step: 25550... Loss: 1.1513... Val Loss: 1.6769\n",
            "Epoch: 50/50... Step: 25560... Loss: 1.0560... Val Loss: 1.6722\n",
            "Epoch: 50/50... Step: 25570... Loss: 1.1339... Val Loss: 1.6710\n",
            "Epoch: 50/50... Step: 25580... Loss: 1.1026... Val Loss: 1.6664\n",
            "Epoch: 50/50... Step: 25590... Loss: 1.1721... Val Loss: 1.6586\n",
            "Epoch: 50/50... Step: 25600... Loss: 1.1777... Val Loss: 1.6553\n",
            "Epoch: 50/50... Step: 25610... Loss: 1.1871... Val Loss: 1.6596\n",
            "Epoch: 50/50... Step: 25620... Loss: 1.1628... Val Loss: 1.6641\n",
            "Epoch: 50/50... Step: 25630... Loss: 1.1563... Val Loss: 1.6665\n",
            "Epoch: 50/50... Step: 25640... Loss: 1.1861... Val Loss: 1.6654\n",
            "Epoch: 50/50... Step: 25650... Loss: 1.1854... Val Loss: 1.6574\n",
            "Epoch: 50/50... Step: 25660... Loss: 1.1311... Val Loss: 1.6567\n",
            "Epoch: 50/50... Step: 25670... Loss: 1.1175... Val Loss: 1.6601\n",
            "Epoch: 50/50... Step: 25680... Loss: 1.0893... Val Loss: 1.6568\n",
            "Epoch: 50/50... Step: 25690... Loss: 1.1643... Val Loss: 1.6503\n",
            "Epoch: 50/50... Step: 25700... Loss: 1.1187... Val Loss: 1.6515\n",
            "Epoch: 50/50... Step: 25710... Loss: 1.0924... Val Loss: 1.6598\n",
            "Epoch: 50/50... Step: 25720... Loss: 1.2043... Val Loss: 1.6601\n",
            "Epoch: 50/50... Step: 25730... Loss: 1.1354... Val Loss: 1.6562\n",
            "Epoch: 50/50... Step: 25740... Loss: 1.1417... Val Loss: 1.6548\n",
            "Epoch: 50/50... Step: 25750... Loss: 1.1686... Val Loss: 1.6581\n",
            "Epoch: 50/50... Step: 25760... Loss: 1.1567... Val Loss: 1.6650\n",
            "Epoch: 50/50... Step: 25770... Loss: 1.0933... Val Loss: 1.6691\n",
            "Epoch: 50/50... Step: 25780... Loss: 1.0944... Val Loss: 1.6679\n",
            "Epoch: 50/50... Step: 25790... Loss: 1.1706... Val Loss: 1.6669\n",
            "Epoch: 50/50... Step: 25800... Loss: 1.1100... Val Loss: 1.6623\n",
            "Epoch: 50/50... Step: 25810... Loss: 1.2127... Val Loss: 1.6635\n",
            "Epoch: 50/50... Step: 25820... Loss: 1.2153... Val Loss: 1.6632\n",
            "Epoch: 50/50... Step: 25830... Loss: 1.0806... Val Loss: 1.6640\n",
            "Epoch: 50/50... Step: 25840... Loss: 1.0917... Val Loss: 1.6603\n",
            "Epoch: 50/50... Step: 25850... Loss: 1.1890... Val Loss: 1.6590\n",
            "Epoch: 50/50... Step: 25860... Loss: 1.1929... Val Loss: 1.6580\n",
            "Epoch: 50/50... Step: 25870... Loss: 1.0931... Val Loss: 1.6567\n",
            "Epoch: 50/50... Step: 25880... Loss: 1.1855... Val Loss: 1.6522\n",
            "Epoch: 50/50... Step: 25890... Loss: 1.1261... Val Loss: 1.6592\n",
            "Epoch: 50/50... Step: 25900... Loss: 1.1370... Val Loss: 1.6592\n",
            "Epoch: 50/50... Step: 25910... Loss: 1.1296... Val Loss: 1.6571\n",
            "Epoch: 50/50... Step: 25920... Loss: 1.1820... Val Loss: 1.6544\n",
            "Epoch: 50/50... Step: 25930... Loss: 1.1515... Val Loss: 1.6609\n",
            "Epoch: 50/50... Step: 25940... Loss: 1.1144... Val Loss: 1.6594\n",
            "Epoch: 50/50... Step: 25950... Loss: 1.1182... Val Loss: 1.6563\n",
            "Epoch: 50/50... Step: 25960... Loss: 1.1430... Val Loss: 1.6568\n",
            "Epoch: 50/50... Step: 25970... Loss: 1.1945... Val Loss: 1.6633\n",
            "Epoch: 50/50... Step: 25980... Loss: 1.1578... Val Loss: 1.6618\n",
            "Epoch: 50/50... Step: 25990... Loss: 1.1170... Val Loss: 1.6631\n",
            "Epoch: 50/50... Step: 26000... Loss: 1.5170... Val Loss: 1.6653\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMaRk7e0P950"
      },
      "source": [
        "## Getting the best model\n",
        "\n",
        "Set your hyperparameters to get the best performance, and watch the training and validation losses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3aH8UF3P951"
      },
      "source": [
        "## Checkpoint\n",
        "\n",
        "After training, save the model to load it again later if we need too. Here I'm saving the parameters needed to create the same architecture, the hidden layer hyperparameters and the text characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "884oUk09P952"
      },
      "source": [
        "# change the name, for saving multiple files\n",
        "model_name = 'rnn_x_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9o-o_8g5P956"
      },
      "source": [
        "---\n",
        "## Making Predictions\n",
        "\n",
        "Now that the model is trained, I'll want to sample from it and make predictions about next characters! To sample, pass in a character and have the network predict the next character. Then take that character, pass it back in, and get another predicted character. Just keep doing this and generate a bunch of text!\n",
        "\n",
        "### A note on the `predict`  function\n",
        "\n",
        "The output of our RNN is from a fully-connected layer and it outputs a **distribution of next-character scores**.\n",
        "\n",
        "> To actually get the next character, we apply a softmax function, which gives us a *probability* distribution that we can then sample to predict the next character.\n",
        "\n",
        "### Top K sampling\n",
        "\n",
        "The predictions come from a categorical probability distribution over all the possible characters. The sample text can be made more reasonable to handle (with less variables) by only considering some $K$ most probable characters. This will prevent the network from giving completely absurd characters while allowing it to introduce some noise and randomness into the sampled text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVfWFYgpP957"
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgrW2ZbBP95-"
      },
      "source": [
        "### Priming and generating the plot\n",
        "\n",
        "Typically I'll want to prime the network to build up a hidden state. Otherwise the network will start out generating characters at random. In general the first bunch of characters will be a little rough since it hasn't built up a long history of characters to predict from."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0YOTdKgP95_"
      },
      "source": [
        "def PlotGenerate(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw8DDZaZl32M"
      },
      "source": [
        "Plot Generation by the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIeoYiCKP96E",
        "outputId": "c2559ec7-0c93-47c1-ba80-c7ab2843a5c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(PlotGenerate(net, 15000, prime='scene', top_k=5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scenes to stand them they have been a mother that, but she should take him to the tribe to heal both mare, and this is being restoning their apartment and he has no interview on thinking that the season two sent the screen as he could step for too clate to the car. she says they are standing on tom and tells him that he's an american for the tournament, and says she has no interesting the police decision in a metaphor.is talking as his wife soon of the same time. the scortion thinks suspicious. the second to gathing her featured, but tarek says that he had a marriage, still hugh alerand (a fearing of closer and starts to talk about how to get a marine and still thinks that this is bringing target in the team about leging the prisoners. they start to get help off at his still, as the two of them drives off the courter that is freelan from the first manish that he has burned her for the county and son it is being striggering her time he sees her way for a child. a man named seems to bang the bottle. solomon is completely explaining to say that his behavior it is brought away where the strogoi would be the trunk. they give up back, but he is coming to the fourth season and so she will be talking to the same entire children, well helps the bomb on the first could his his house with supervisor. the cop tarred her fire standing a prop-6 and he speaks with the phone. after teddy and that she will be reminded of the battle sintered by the father and a previous church, which he and collapses the something; after the protecting castle, but they have been human. she asks where he was blood in the time and starts about her from behind the scene that is soon that they can't send her and a continue to get on his, and he is crushing the biters every car with the corleone patients behind the spirit magic to the science car the tournament counter and starts to be the first and the streets of the back of the front gathering and hopes of the telephone to the back of the side, the sexual activist is able to tells her to be chasing the seaton and, the two of them alice. he takes the girl was to the same time. john only a performance is stranged in tarning.tom also all the tournament.talking about the comic book by string into his control. as the character warns to help his, she doesn't know why the same time later had a peacekeepers, she starts to bases a train and how to share his campaign story at the building and he staleek is attracted by salameh and a marine is aware that she will be seen in their, looking along a critain and another flats and says that as the those and their decision that the scent is stopped by a milk a small station and hit the partnership. the cullens says the scortes his chopper is comes as an also talk to secure the cullens and the prisoners are crichton the people of the circumstance, but at the present; he can get a brief story. the there and his secret realizes it will be completely a bomb to the prisoners. she has sacet its told by the pain of the battlefield and seemed moves tries to see that sometimes it's all become a small descent tho winch back to the chicches, but he cut hit him and his previous through the series, and a party is carried having to be able to company. he says that he had a friend, sawa, and he walks away from her. and so they are shown as them and in the confrict of the time.the sexual activist still sees he will be seen an imputs with the problem to buy something that he will have sex with her world war ii and her home with support. he tells him that someone talks to the time of a then technique to the story, as he sees the moment and told him she doesn't become his own scent, and the men is closing the cullens have a commanding his car from her brutality and says, \"and she takes a moment of the target that shows malcolm that she has a spy who can be an alarm and his points if she doesn't know when he's shitched in a surrounding, she will be. seligman realizes that she is stopped by and he's surprised, at the bank figures of a pointing of the series, a stone for a stolen communication as well as the confrontation, she slips her through a sharo so that he is a good point of the chair, but the senset to get to him. he tells him about how to straightening the cullens and his story tournament cross through the process of sex and his frataries, and she has breek a short battle, setting the priest that the monologue. tells the temple as they go to the floor. this is shown that he human roables. she also adds that they are the story in the battlefield on him, but they should see if she doesn't believe hes that they completely eret as he is in the crowd is intercup tracking and raises the protest that another draws her body straded of the constilution that he is any going towards. he starts to climb the street into the program how actian attending to find her show or that his plantation to attack their days at the bathroom before the character search for simon to be able to thinks: the commander says that the stranger think in that means they are carrying a protecting and a sexual events of tong po's contents. malcolm is three secretary sexual in shocking steve clear that anna to be coming against the scene. he tells the card is in the car, as a character informs him what this meeting. joe survives and terrifying him a stage of her family about the series' at all of them, but he is, telling the tracking device and has the corleone computer and the same railway talks if they help him. she tells him he would like to be a friends watch. the correcting crossword that she hadn't want to get her to the scene of the fact that scar and that scorpius was to be capturing susan's head and is that they alert him back into the passon. and anna is stabbed. she's been at she asked her to see it off and an elderly marriage is a progony of an ilanted arm, she shoots her with a small bottle of the police station. mia's last car at the contents to him and says that they wrent himself for her to the first person. she says that there was some conflicting for the beginning of the first person. at this time that he can go to a parking room. seth takes a great series are chosing her, they have to leave a might and seem a second time, all adostimately to the corleone second and shoots him to be a matter of the top sam to the tribe and he sent tony, so they want, the statested from tournament would be. malcolm tells supervisor and takes him into bella. and she says he has a man are able to rulling himself in sexual and says; the means that senies, and that milk has seen in the castro to take off her the opening docks. india and the same street is satisfied when she is in the city's could higher on the front door. all, in ambert of the crossward characters; hand and her castle. he that she wasn't about to be a restaurant that shaw second life when she is an assumption once of the secret she will never return about a mindrapersional story and the second room fox someone else that he still has to be as a surgical strength, and he calls his hostage that assembers an ally to the police. holmes sees the time against pooh-bear and the town was mentally being help to the street, and selving shot on a booby and still, solomon and the captured his from the family well down. at that moment was a point of the book as he rishes from the chirace breaks. bella is and his probably a far who she would leave.\" telling him where they atear the truth, and she comes home, we see that she has solved him in the best. he sees a muscular door, and the cop sees the street for the police. he has the bed he cut order a meat in. shaw is shot and that the compromision at the techies to go to the forehol and assumes he is some start in the front of him, but it survives his highest computer who are the best first, a meat of season two are somewhat and solomon start at a series of confrontation, and he will have to steal to start on the shadows and attempts to call him that the tracker is shot in order to speak to them. the tracking dimprish start to break them worried to the car for the car. after a police. john solving off a motivated but the camera search signature, he sees that he's a place before and say why hes allowed the party's release and sees his sight and stops and she had been talking. after her feat, she sees that some time is dranked to the fight shot and talked on the ground. the street is surpressing that some of the same this mini-seation is ablusted in the castro.milk's considers and suggests, however, they can become the train out, so he can remove an unarrunation. the two men shaves they start to go to how teddy's best. he comes home to him. she shows him his corsciouster tracking dot and the police shie chases her to throw, and much his family at the corse, and as the scene spirs a strength, because it's now held both and that his father, helled him with a stranger station, but he had been stating about the first time, they continue terrifying him at all and her sentences are the ordining story to a surviving about her being about a brother bringing her same things that the contestants would like a computer change but its consequented the but tells him to shoot on the room; she says that shes building to the street.the scene still had a call a bloody of the same side of the corleone concentration, and a prisoners look out. she then allows malcolm how that there was only the chemical remaining stage and his one that he work. they get a moss stathouse, tony to take on her crap of the film. he tells her that the most day, with a bar wighing have been traced in one work that the same time was the couple and tells the guards to the scene of the films inside, and shortly then shots the bomb to the school, and makes his machine is decided from the soldiers in the time singer that she's been seen talking about what she would be clean that he stands away. she tells her that the top, he tries to stay and sees his corpsite, and tony asks him what to trape him before leaving the people and tells him the brother was telling the same room before and the men to get at the street their confrontations. she sees a film plays in on the bottom of the second as well. he approaches. he has alro knows that the corleone said is the only private to try and has harder, and this is being help of the boy where they say now something with someone come in the terrorist. malcolm and she adds that a small party is also allowing the tational mind, but she's been conversation, and they are all be about to have television. she's too more than that she would have a plan on the stage to the car. jerme asks how after her. the two spots his friend who seems on the teachers and all with her at him in the trust. johnson has taken the car. there will grab accorsing them. a man is awred to sandwii again and starts to state on, and he has beated his chase to see if he will be happy to the tournament. as a second and says she said. he is the one who could have no set one time. the concert was somewhat and hurt a boobs of sexual actions and the three dm's that he was dated by the fireplowed and says that hes have been head to his father, allowing the bottom on the street, taken into his first stage of the first cylon district. at the soldier, so milk's soon, she would be stayed in of a couple of tournament contestants all of their friends in the back, before she starts to break together. she received the protegning after a back of the body and starts a few supervisor' because she who stopped him and tells him that the couch were assassinated with aside, who have been seen attacked by her first.in a flashback, too comes to his character was a shot and she alice sought that there's a small drawn.tony causes the street at the some of the car at the stop carraining to the trees, he tells him. he's numbers and it had a motel concest of a shot off. she tries to reach the problem that they're comen back to the stage and the support holds a river arm still broke to survive. as the track is seen any months. after solomon, and that milk trained him with a based all battle-a cell in have found security assassination with the still and they are not all alan for their shadow with an extent trace. the scientisted would let. then meets her before he tells them that he is a memoryal strong that would be a male think and seems to help her assistance and their business says, \"taking him off on the fire. a commander that all as the meeting we see them is seen held and they're start to hear an intercourters and so secure of the truck, the cullen crime to get a metal doctors to cannot support, but something to say that his body tortured by the battlefield. a farther season trained, says that she would have seen them out of the campaign station; a monitors wants to be the series' party the bus is training his son's death, but after his for her. he says her senses. she then then resules he doesn't think he will be the same exactly is stopped by the carriage father macavoy to a factory to help his son. the modiscrowlarlus astended the battle that he's the south footage of several team, and starts to be a major and collection with the convenience to be forced to recaute this is a club.as a sudden convicts want to go to the table.the temperation armed is droming the county and they see his wife and daughter are told that they want to go but and miles says that the machine is telling her to stay their feutce, and she sees he's to to talk to be a sex at thought of her story to try to say anything in the field, he shering at the car and starts to brother since he hoters cant committed by the terrified, and they are trief to control the foandary. the second man talks that he has all a past the bus. she shows his high station, while how managed to take off the pathetic display after his hair by crichton shooting their trip were named he will cause other grand crash and some one of the train, tony appears only a place in the face and him. the three dm's are a series of attacks, when john shoots the beginning of the story and to the three dm's. after say, he is seen a flashback of a far handly said and she had been said at a things who then goes out of the truth; it was done wouldn't take an incompoter. he also talks that he was the one who suggests she can get a police wonderland and the computer tracker is told that alan has always seen a fartary through the tournament. he sees it to be deal. the conversation to the stairs, holmes stops the speech. he came out of his peece. she says that if he does not know why he has the two monitors the beard and he wants. he then realizes that he wants to believe the second respect for the production as a sherisapeap, since tony's face, but she is introductibled, but she was the one from the time of having been to the present man to help see his complete, steve the series, states that they want to stay for his children in the fourth season, to the campsite at a teen\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}